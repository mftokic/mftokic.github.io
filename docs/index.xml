<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Thoughts on Things</title>
<link>https://mftokic.github.io/</link>
<atom:link href="https://mftokic.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>A collection of thoughts on things from the mind of Mike Tokic</description>
<generator>quarto-1.5.45</generator>
<lastBuildDate>Tue, 25 Jun 2024 07:00:00 GMT</lastBuildDate>
<item>
  <title>Microsoft Finance ML Forecasting Journey: Part Two</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/image.png" class="img-fluid"></p>
<p>This is a multipart series: - <a href="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/">Part One</a> - <a href="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/">Part Two</a></p>
<p>The success of Microsoft finance‚Äôs first machine learning (ML) forecast spread like wildfire throughout finance. The ML forecast was shared with all finance leaders. So naturally knowledge of ML‚Äôs potential trickled down to more people across the organization. Eventually the news came to a team in central finance (CFT). Think of this team as Microsoft‚Äôs core FP&amp;A team across the entire company. After seeing the accuracy at a worldwide level, this team knew it could help in the biggest forecast process at Microsoft. Something called the commercial field forecast. This forecast is created by finance members who sit in the ‚Äúfield‚Äù. The field is just a cool way to say regional offices all around the world. These field finance teams support the sales teams who also sit in the field. How could they take a worldwide forecast by product and break it down into specific countries all across the world? Well buckle up gang, it‚Äôs time to find out! This is how a tool called ‚ÄúCommercial Predict‚Äù was born in 2017.</p>
<section id="how-things-used-to-work" class="level3">
<h3 class="anchored" data-anchor-id="how-things-used-to-work">How Things Used to Work</h3>
<p>Before we dive into all the ML goodness, we have to understand how the old way used to work. I know, it‚Äôs kind of like eating your vegetables. But we just have to do it real fast then we can get to the fun parts.</p>
<p>In the past, each finance team in the field was responsible for their own forecast each quarter. These forecasts would happen in ‚ÄúCFO forecast cycles‚Äù. With cycles happening in October, January, and April. Microsoft‚Äôs fiscal year runs from July - June so these forecast cycles happen at the start of Q2, Q3, and Q4. The forecast at the start of Q1 is budget (that‚Äôs a story for a different day). Each cycle, a forecast would be created for the remainder of the fiscal year.</p>
<p>Microsoft sells products in over 100 countries. Most of those countries have a sales team that tries to sell products to companies in that geographical region. If there‚Äôs a sales team, then there is a finance team who supports them. This means there are dozens of sales finance teams creating quarterly forecasts for the rest of the fiscal year each CFO forecast cycle. Each team had their own secret recipe of how the forecasting was done. Often a custom excel model that would create the forecast. This model needed to be handled with care. Since each quarter it would have to be rolled over and prepared for the next forecast cycle. Anyone who has ever created and owned a financial model in excel knows the anxiety faced with trying to build and maintain one. These models were complex, and you said a little prayer every time you opened the file. Hoping it wouldn‚Äôt crash your machine because it was so large.</p>
<p>Once each team in the field had their forecast for their geography, it would get sent up the food chain. Forecasts from each country would be combined to form higher level aggregations in Microsoft‚Äôs sales territories. Each aggregation added more countries and continents together. This continued until you got the total worldwide number for the entire commercial business. Each time the forecasts got combined together at a higher level, senior finance leaders had the opportunity to make adjustments to that forecast. Based on their domain knowledge of the business. Eventually the final forecast the CFO, Amy Hood, saw was something completely different than what was initially created by each sales finance team for their specific geography.</p>
<p>Layers upon layers of bias were added to the forecast. Some was good bias that could improve forecast accuracy, but often it was too many cooks in the forecast kitchen. Too many people touching a forecast that didn‚Äôt need to be touched. Resulting in worse accuracy and more confusion once the books were closed at quarter end. This process would take upwards of a month every quarter. From the initial forecast created by field team all the way up the food chain to the CFO. In the spirit of every good infomercial, ‚Äúthere just has to be a better way!‚Äù.</p>
<p>Now you know why finance had to do something different. Drastically different.</p>
</section>
<section id="excel-prototype-built-in-a-redmond-garage" class="level3">
<h3 class="anchored" data-anchor-id="excel-prototype-built-in-a-redmond-garage">Excel Prototype Built in a Redmond Garage</h3>
<p>All good things start from humble beginnings. The team in CFT wanted to centralize the field forecast process for the commercial business. Create a single way that everyone in the field would follow to create a forecast. To make this a reality, they started with the swiss army knife of every finance professional. That‚Äôs right. You guessed it. They started with excel. Like any innovative project, it quickly became their baby. And all babies need a name. The named it Commercial Predict.</p>
<p>The team created an excel prototype of a single model that every field finance team could use. It was a combination of the old and the new. First was old but reliable PxQ forecasts. Where you take what‚Äôs in the sales pipeline for a quarter and multiply it by how many deals on average have closed in similar historical quarters. Second was classic CAGR and year over year percentage growths, which acutally still work quite well. These traditional methods were combined with more statistical rigor. Something more along the lines of machine learning. They built by hand, formula by formula, exponential smoothing statistical models. Which is a common model in time series forecasting. It‚Äôs more stats than machine learning, but still performed really well. Today exponential smoothing is a simple function call in excel, but this team built it from scratch. I tip my cap to them, because that was hard to do.</p>
<p>Now there were multiple forecasting methods in this mega excel model. The beauty of the idea is that someone could come into the model and choose what methods they wanted to use to forecast a specific geography, customer segments, and products. Users could even combine multiple methods together to get a more accurate forecast. This was powerful because someone could use the PxQ sales pipeline method for products that depended on big customer deals landing, and use the other methods for things that had more stable trends and seasonality.</p>
<p>It was genious prototype. The team was able to take this to their leadership team and show how one single approach to do forecasting in the field could save thousands of days of combined human effort across the field every year. One mega model to rule them all. It had the promise to cut forecasting down by 50%. Would it work though? To test it out, the team ran this excel model alongside the traditional bottoms up forecast process from each field team. They could then compare the results and even track accuracy across the old and new ways. The results were good. The new prototype was the same or even better than the existing process, but was 50% faster.</p>
<p>The new approach was fast, and it was accurate. The final roadblock before adopting the new approach was to get everyone in the field to agree on what level they should forecast at. This historically was a difficult subject to discuss, with everyone having differing opinions. Thankfully this was solved by getting the buy in from the top senior finance leaders in central finance and the field. Once that happened everyone was able to get on board.</p>
<p>Ok, so the team had a cool protoype that knew worked well. But if they just used that excel model, then they are still maintaining a model that is messy and requires constant upkeep. It would be hard to scale. They needed something more robust. A real tool that was built by engineers. Thankfully there was a team who could do just that.</p>
</section>
<section id="building-the-tool" class="level3">
<h3 class="anchored" data-anchor-id="building-the-tool">Building the Tool</h3>
<p>The vendor team who was created to take over the <a href="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/">initial ML forecasts</a> was the up to the task. They had the data science knowledge, but the data engineering and software engineering needed to build a software tool to scale out the excel prototype was missing. So the team got other vendors to fill in those gaps. Now there was a team of engineers all capable of making the tool a reality.</p>
<p>The first version of the production level Commercial Predict tool had to be built fast, before an upcoming CFO forecast cycle. V1 was built into excel within six months as an add-in field users could download and connect to. It needed to combine new machine learning methods with traditional PxQ and CAGR/YoY run rate methods.</p>
<p>Here‚Äôs how it worked.</p>
<ol type="1">
<li>Data engineers would pull historical revenue and sales pipeline data. All the forecasts methods were precomputed and saved in a database that was turned into a cube. This would be the starting point for all field members. Instead of calculating these forecasts by hand in their old excel models, it would be precomputed for them. At scale.</li>
<li>The forecasts in the cube were then served to users in a custom excel file. Each field team could come into the tool and select the geography, products, and segments they were responsible for forecasting. After making these selections, all the forecast methods would populate in the excel.</li>
<li>Users could then see each forecast method and see which ones pass their smell test of what they expect to happen in the business based on their domain knowledge. They could choose a specific forecast method to use, or combine multiple methods together to get a more robust forecast. Finally there might be things these forecast methods don‚Äôt know about. Like upcoming tax changes or product strategy changes. Field users could ultimately make manually adjustments to get a final forecast.</li>
<li>Once the final forecast was created, they could save it back to the cube. This allowed finance leaders to see the forecast creation in real time. Also it would prevent the classic ‚Äúexcel crash without saving‚Äù headache we‚Äôve all been through in the past.</li>
<li>Once the forecast was complete for each field team, a static output file was created at the touch of the button. Teams could take this output and load it into the final planning system.</li>
</ol>
<p>Before official launch, training sessions were held to make sure everyone knew how to use it. It was also a good opportunity to fix any bugs in the tool. This resulted in some late nights and even weekend shifts, but the job got done. The tool was launched on time and the rest is history.</p>
<p>We were able to go from a forecast process of 21 business days each quarter, down to just 10. It was a revolution. This saved Microsoft millions of dollars each year of human capital. Finance teams in the field could now forecast faster, with less headaches, and prevent the layering of bias that was a staple of the previous way.</p>
</section>
<section id="evolutions" class="level3">
<h3 class="anchored" data-anchor-id="evolutions">Evolutions</h3>
<p>After this officially launched in 2018, the Commercial Predict tool has gone through a lot of iterations. What started in excel then moved into a web based tool. Then back to excel. With each iteration, we got better at the machine learning methods. Better at adding more features to give users more control over the final forecast.</p>
<p>Eventually Commercial Predict evolved into a much broader solution called ‚ÄúFusion‚Äù in 2022. Think of it as a tool that could still do the commercial field forecast process but now also take on other forecasts within Microsoft finance. A true one stop shop for all things planning. Fusion is an excel add-in with a built in UI on the side of excel. Kind of like how excel copilot opens on the side of your excel tab, Fusion does the same. A user could select what forecast they want to do, select the parts they‚Äôre responsible for forecasting, and Fusion would populate the blank excel file with all the information they need to finalize their forecast. Methods like PxQ and machine learning are still ran ahead of time. The UI was truly dynamic. You could take any excel file and open the Fusion app inside to get going on the forecast. Fusion allowed finance to scale the learnings of Commercial Predict to so many other forecast processes. Improving the impact ML and centralization can have on forecasting.</p>
<p>Planning tools like Fusion will most definitely change in the future. As the business evolves, so should our way of forecasting it. What doesn‚Äôt change is how ML has become a central part of the forecast process.</p>
</section>
<section id="lessons-learned" class="level3">
<h3 class="anchored" data-anchor-id="lessons-learned">Lessons Learned</h3>
<section id="iterate-iterate-iterate" class="level4">
<h4 class="anchored" data-anchor-id="iterate-iterate-iterate">Iterate Iterate Iterate</h4>
<p>Rome wasn‚Äôt built in a day. Instead of building this complex centralized forecasting tool from the start, we started small. Built a prototype. Got senior leadership buy in. And continued to make it better every 6-12 months. Even as I write this we are in the process of improving the ML accuracy of the commercial field forecast. If you‚Äôre coasting, you‚Äôre going downhill. You need to continue to iterate.</p>
</section>
<section id="combine-the-old-with-the-new" class="level4">
<h4 class="anchored" data-anchor-id="combine-the-old-with-the-new">Combine the Old with the New</h4>
<p>ML didn‚Äôt outright replace every part of the commercial field forecast. Instead we combined ML techniques with older methods like PxQ sales pipeline methods. This allowed us to use the strengths of each approach based on what product was being forecasted. Some products are sensitive to large customer deals closing, so PxQ works best. Others have stable trends and seasonality, that‚Äôs where ML shines. Using both gives us the best of both worlds.</p>
</section>
<section id="senior-leadership-buy-in" class="level4">
<h4 class="anchored" data-anchor-id="senior-leadership-buy-in">Senior Leadership Buy In</h4>
<p>A forecast process is like a ship. The bigger the ship, the harder it is to change course. So the bigger the forecast process, the higher the buy in needed from a senior leader. Getting a GM or CVP level support allowed us to supercharge the change management. It‚Äôs easy to get bogged down in arguing with senior finance managers about how a forecast process should be done. Once a CVP (someone who reports to the CFO) comes in and says this is how we‚Äôre going to do it. Then everyone gets on board and starts turning the wheel of the ship togethger to change direction. The commercial field forecast had to get support from GM and CVP level leaders or else it would have taken years to change it instead of months.</p>
</section>
<section id="final-thoughts" class="level4">
<h4 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h4>
<p>The hardest part of any ML project comes down to people. Training models is easy, convincing people to use them is hard. It takes time. It takes senior leader buy in. It takes an open mind to rethink how your job can be done. It might be hard, but in the end it‚Äôs worth it.</p>
<p>Often we get asked by finance teams outside the company if they can take our ‚ÄúCommercial Predict‚Äù or ‚ÄúFusion‚Äù tool off the shelf and start using it at their own company for forecasting. Sadly you cannot. We build a lot of these custom tools because we don‚Äôt have a choice. Microsoft‚Äôs business is complex. Often we need custom solutions that are hard to standardize in external products. Thankfully the machine learning methods we use are available for free as an <a href="https://microsoft.github.io/finnts/index.html">open-source R package</a>. Check it out if you‚Äôd like to learn more.</p>


</section>
</section>

 ]]></description>
  <category>finance</category>
  <category>machine-learning</category>
  <category>forecasting</category>
  <guid>https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/</guid>
  <pubDate>Tue, 25 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Microsoft Finance ML Forecasting Journey: Part One</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/image.png" class="img-fluid"></p>
<p>Ever wonder how Microsoft Finance got started with machine learning? It didn‚Äôt just happen overnight. It started small and grew from calculated steps. In this post and a few others I want to tell the journey of how we got started. Gather round children! It‚Äôs story time.</p>
<section id="paradigm-shift" class="level3">
<h3 class="anchored" data-anchor-id="paradigm-shift">Paradigm Shift</h3>
<p>In the summer of 2015 AI and machine learning (ML) weren‚Äôt terms you‚Äôd hear every day. Maybe you‚Äôd hear the word ‚Äúbig data‚Äù being thrown around business circles but no one had a clue what it meant. There was a lot of data being captured about our world. Somehow we could ‚Äúmine‚Äù the data to get some value out of it. No one really knew.</p>
<p>Earlier that year, something interesting happened at Microsoft. A new product called <a href="https://techcrunch.com/2015/02/18/microsoft-officially-launches-azure-machine-learning-big-data-platform/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuYmluZy5jb20v&amp;guce_referrer_sig=AQAAAE95NY6ZC1q7S3u9eB-VRujlcx7hEFnu35ya9daztiyij3JgOLZWH9VqOfmNl6FuEi2KTbr67hu2aoyrIMUYcUGPoDhzEDsXRJ01LUAF0c-VUU_lHdUhGzLcW5FQYKZwXQJ93c3qzwocA3_WIi-5Z4VXcKJGnaD-1E40xMI6LRnB">Azure Machine Learning</a> was officially released. The service allowed anyone to start mining their data up in the cloud. You could train models and serve them through APIs. It was basically magic. Unfortunately in finance, those words meant nothing. To a Microsoft finance worker the term ‚Äútrain a model‚Äù meant training the new employee on building excel models. Everything was done by hand and with care. Especially forecasting our financial statements. The CFO of Microsoft, Amy Hood, thought differently. What if we could use the new product to improve some of the manual work we did in finance? Could we have these models be trained to forecast our business? It was a tough question. No one in finance at the time was really qualified to answer it. She had to go ask the expert.</p>
</section>
<section id="getting-the-ball-rolling" class="level3">
<h3 class="anchored" data-anchor-id="getting-the-ball-rolling">Getting The Ball Rolling</h3>
<p>Amy went to the legend himself. The head of Microsoft‚Äôs cloud, Scott Guthrie. King of the cloud and wearing red polo shirts. She wanted to see if Scott‚Äôs engineering team could help finance build machine learning models. Allowing finance to forecast the business. Thankfully Scott said yes and lent a few data scientists to help the finance org get off the ground with ML.</p>
<p>The big ticket item was forecasting revenue. Instead of starting small with one specific area we started very high level. Amy wanted a quarterly global revenue forecast by each of Microsoft‚Äôs major products. This forecast could be used internally to compare against the manual forecasts. Which are created by sales finance and product finance teams. The ML forecast could either confirm or contradict these bottoms up forecasts made by humans. Allowing finance to either adjust their forecasts. Or make sure they know why they are different than ML.</p>
<p>The results were strong. The ML forecast was around 1%-2% off on average, compared to the manual human forecast error of 2%-4%.</p>
</section>
<section id="keeping-the-ball-rolling" class="level3">
<h3 class="anchored" data-anchor-id="keeping-the-ball-rolling">Keeping The Ball Rolling</h3>
<p>The game officially changed. The finance team could now just rely 100% on ML going forward right? Not so fast! Who would keep training these models? What if we wanted to forecast at a more granular level? Scott‚Äôs data scientists couldn‚Äôt help forever. To fix this Amy had to hire some data science talent. People who knew what they were doing. Like the engineers on Scott‚Äôs team.</p>
<p>Hiring your first data scientist is a hard thing to do. Creating a career path for them in a non-technical team like finance makes it harder. As a first step, a team of vendor data scientists were hired. This was enough help to take the work done by Scott‚Äôs team and keep it going. Even expand it to other areas. The hope was to eventually turn a vendor data scientist team into a team of full time employees.</p>
</section>
<section id="lessons-learned" class="level3">
<h3 class="anchored" data-anchor-id="lessons-learned">Lessons Learned</h3>
<p>Going from zero ML work to your first forecast solution takes hard work and perseverance. Here are a few lessons Microsoft finance learned when starting out.</p>
<section id="borrow---rent---buy" class="level4">
<h4 class="anchored" data-anchor-id="borrow---rent---buy">Borrow -&gt; Rent -&gt; Buy</h4>
<p>Initially data scientists were borrowed from other teams at the company. Then they were rented from outside companies as vendors. Then finally once a strong data science practice was established after a few years, full time employees were hired. Many were vendors who turned into full time employees. This process was slow, but allowed finance the time to make sure a data science practice and career path could be built.</p>
</section>
<section id="whats-the-biggest-opportunity" class="level4">
<h4 class="anchored" data-anchor-id="whats-the-biggest-opportunity">What‚Äôs the biggest opportunity?</h4>
<p>The biggest opportunity to forecast with ML was revenue. We could have spread ourselves thin and tried to do the entire income statement. But we knew revenue was the hardest to forecast. So that‚Äôs where we started first.</p>
</section>
<section id="start-at-the-top-work-your-way-down" class="level4">
<h4 class="anchored" data-anchor-id="start-at-the-top-work-your-way-down">Start at the top, work your way down</h4>
<p>Starting first with worldwide revenue allowed finance to get good results without getting too deep into the weeds first. If we wanted to get an accurate daily forecast down to the sku level, that would have taken forever. Instead we started big and then eventually worked our way down. This process may not initially replace the manual forecast work being done. But it starts to get others in finance comfortable using ML in the decision making process. After finance leaders got used to seeing these ML forecasts, we could then start working on more granular forecasts that could replace more manual work.</p>
</section>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Ok now you know how the ML ball got rolling in Microsoft finance. Before reading this article you might have thought we had this amazing ML kick-off with millions invested in the space. We definitely did not. Instead we started small in areas that had the highest ROI and worked our way from there. If your company is just starting out on your ML journey, I suggest you do the same. Small, incremental change can compound into enormous impact over the long run. That‚Äôs the kind of change that lasts.</p>


</section>

 ]]></description>
  <category>finance</category>
  <category>machine-learning</category>
  <category>forecasting</category>
  <guid>https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/</guid>
  <pubDate>Wed, 12 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>How To Master Storytelling</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/image.png" class="img-fluid"></p>
<p>Our dumb caveman brains can‚Äôt remember everything. There is too much information passing through our heads each day. The only thing that sticks are stories. Either ones we tell ourselves or ones we hear from others. Don‚Äôt believe me? Let‚Äôs play a game. Tell me what you had for lunch last week on Tuesday. Now tell me the plot to the first Star War‚Äôs movie. Ha gotcha. Stories stick. More than anything else in this world. Whoever can tell the best stories has the power to do great things in our world.</p>
<p>Shaan Puri is an entrepreneur and content creator. Known for his work in the tech industry and his popular podcast ‚ÄúMy First Million.‚Äù A few months ago he was on the popular <a href="https://www.youtube.com/watch?v=Z2BnqYArwaw&amp;list=PLrxUIdme2aVnB6z5BwgIJx0rJjKrl7Qlc&amp;index=12">‚ÄúHow I Write‚Äù podcast</a>. While on the show he blew my mind with tons of great ideas around storytelling. I learned a lot and wanted to share the best ideas here.</p>
<section id="aaron-sorkins-30-second-masterclass" class="level3">
<h3 class="anchored" data-anchor-id="aaron-sorkins-30-second-masterclass">Aaron Sorkin‚Äôs 30 Second Masterclass</h3>
<blockquote class="blockquote">
<p>‚ÄúI worship at the alter of intention and obstacle.‚Äù</p>
<p>‚Äî Aaron Sorkin</p>
</blockquote>
<p>Aaron Sorkin is a famous screenwriter. Creating hits like the TV show ‚ÄúThe West Wing‚Äù or movies like ‚ÄúThe Social Network‚Äù. His says that every story needs to have a clear intention and obstacle.</p>
<p>In any story, the main character has a have a clear intention. What do they want? Why do they want it? After that you need an obstacle. Who or what is trying to stop them from getting their intention? It‚Äôs fundamental to every story, but doesn‚Äôt have to be life and death. Every movie you have ever seen has this. If it didn‚Äôt have a strong intention or obstacle in the first 10 minutes, you probably hated the movie. For example, all Harry Potter wants is to live a normal life with a loving family and close friends. Since the day he was a born a dark wizard is trying to kill him. Intention (living), and obstacle (trying to kill him).</p>
</section>
<section id="hooks-vs-frames" class="level3">
<h3 class="anchored" data-anchor-id="hooks-vs-frames">Hooks vs Frames</h3>
<p>Everyone has seen Twitter/X threads that start with ‚ÄúThe unbelievable story of XYZ person doing XYZ thing‚Äù. It‚Äôs clickbait and makes me cringe every time I see it. These are tactics that try to hook a reader into continuing to engage in the content. Instead of creating hooks, Shaan recommends creating the right frame for a story. Hooks are about the words you‚Äôre going to write. Frames are about the idea. And how you‚Äôre going to connect many ideas together to make it relevant to an audience.</p>
<p>Check out the following tweets about the popular audio app Clubhouse.</p>
<p><img src="https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/image2.png" class="img-fluid"></p>
<p>This first tweet is by someone who founded a similar audio app before Clubhouse. He should be very knowledgeable on the subject. The tweet gained little traction because he told no story. It was dry and full of technical industry facts and jargon.</p>
<p><img src="https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/image3.png" class="img-fluid"></p>
<p>Now take a similar tweet by Shaan. In his tweet he told a story. He put in the frame of ‚Äúevery one thinks X, but I think Y, and here‚Äôs how I think it‚Äôs going to go down‚Äù. It had millions of views. The thread is a mini screenplay and masterclass on how the right frame can make an idea turn into a powerful story.</p>
<p>Another powerful example is Dave Chappelle trying to get the rights back to his famous sketch show. He didn‚Äôt complain to Viacom (owners of Comedy Central which initially hosted the show). Instead he told a story with a powerful framing. It wasn‚Äôt a funny story. But one that made his fans start boycotting the show on streaming platforms. Hoping to get paid for his original hard work. This was all due to a powerful story with the right framing. Check <a href="https://youtu.be/5N4UFl9G00A?si=a9AwDMvbQ8IrLqSU">out this video</a> to hear the full story.</p>
</section>
<section id="tell-100-stories" class="level3">
<h3 class="anchored" data-anchor-id="tell-100-stories">Tell 100 Stories</h3>
<p>If you don‚Äôt know who Mr.&nbsp;Beast is than you live under a rock with no internet connection. People always ask how they can be like him. His advice is simple. Make 100 videos. Each time do one thing better than the last video. He says it‚Äôs the perfect advice. Because either no one actually goes through with making 100 videos. Or the people that do never reach out to him again because once they make 100 videos they figured out how to make them successful.</p>
<p>To tell great stories you need to tell hundreds of stories, maybe thousands. You need to get intelligent reps in. With each rep you get a little better. These gains can compound a powerful skill over time.</p>
</section>
<section id="make-a-story-go-viral" class="level3">
<h3 class="anchored" data-anchor-id="make-a-story-go-viral">Make A Story Go Viral</h3>
<p>There are companies out in the world whose sole purpose is to make content go viral. Now that‚Äôs a wild job. They know the only way something goes viral is how many people share the content. People will normally share content if it creates an emotional reaction. Shaan calls these emoji reactions. Things like ü§£üòçüòéü§îüòÆü§¨. When the company is creating a piece of content, they start with the emotion they want the content to create. Then they work back from that emotion to create the content. The north star is always the emotion the want the audience to feel. If their first draft doesn‚Äôt create that emotion, they will adjust until they get it right.</p>
</section>
<section id="storyworthy-book" class="level3">
<h3 class="anchored" data-anchor-id="storyworthy-book">Storyworthy Book</h3>
<p>Matthew Dicks wrote the book on storytelling. No he actually did. He wrote a book called <a href="https://www.amazon.com/Storyworthy-Engage-Persuade-through-Storytelling-ebook/dp/B07CV2PFYJ/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;dib_tag=se&amp;dib=eyJ2IjoiMSJ9.2Tt6q12dGtz0elc2vBXMeyV883OkegYe9BTrbUg1DoV1313B1TaDUMm32fkWouW4yp32_9uzvUNcJg8SWX1bGlXVs0rAmqsbMtWi7m90beyC1bkyuNxQ_w7cFfM4foQXbGN4ZxWU-E0I-RSOKZeQQgyvwjJsGFhW_O6QjXUle0AU4TCHQgNX3mzmorrmy14UX1OJ8Ez7pjoHN-DywVpZf8yD2CjeErvvkPZOVB0m3HVbHH9OaXsgLacUEK-QYz65c8GAERMLkmOsBjiMxrBCmMaMePTvVA3bQu5wYEkgSmw.d0fzZ1oE--2p9Td8xF85u2X5bHNOCPuYjR81yxxZnM4&amp;qid=1717428290&amp;sr=8-1">‚ÄúStoryworthy‚Äù</a>. I‚Äôm currently reading it and love the book, future post coming soon. Shaan calls out a few kew ideas from the book in the podcast.</p>
<p>First is every story needs stakes. You need to make clear what‚Äôs at stake if the main character doesn‚Äôt get what they want (their intention). These stakes don‚Äôt have to be life or death though. Stakes come from the emotion. Something closer to regular life has the biggest impact because other people can see themselves in the story. Telling a heartwarming story about learning life lessions from your kids at the dinner table can be more powerful than a shark attack story.</p>
<p>No one cares about your vacation or crazy party story. If you have something that‚Äôs generally interesting about your vacation, then only keep the parts that are relevant to the story. If you got pickpocketed at the Vatican, but were able to chase down the bad guy and get your wallet back, then just tell that part of the story. It may not matter that you were on the Amolfi coast the previous day or you were even at the Vatican. No one cares. Just keep your story as long as it is interesting. Not a second longer.</p>
<p>At its core, a story is a five second moment of change. Everything comes up to one moment where a thing or person is transformed. If the story doesn‚Äôt have change, then it‚Äôs just an anecdote. A sequence of events. Not a real story. Stories start with the world one way, and end with it another way. Shaan says a good example is every romantic comedy you‚Äôve ever seen. Whatever the main character is, they are 100% going to be the opposite at the end of the movie. Is the woman a fast paced lawyer who never made time for love? By the end of the movie she will have a romantic partner and will take more time away from work. Is the guy a ladies man who will never settle down? By the end of the movie he will fall for someone who will make him rethink everything and start hearing wedding bells. It‚Äôs always about change.</p>
</section>
<section id="adding-humor-in-storytelling" class="level3">
<h3 class="anchored" data-anchor-id="adding-humor-in-storytelling">Adding Humor in Storytelling</h3>
<p>Humor is the sauce but not the meal in most stories. All humor is just surprise. If you see the punch line coming it‚Äôs not very funny. Try to add humor into your storytelling. Just don‚Äôt make it the whole story. Leave that up to the comedians.</p>
</section>
<section id="binge-bank" class="level3">
<h3 class="anchored" data-anchor-id="binge-bank">Binge Bank</h3>
<p>If someone wanted to learn more about you, what would they do? Maybe they‚Äôre a recruiter trying to offer you the perfect job. Maybe they‚Äôre an entrepreneur trying to find their next co-founder. Or maybe they‚Äôre a cute girl you‚Äôre about to have dinner with. Chances are the only things they can find on you is your social media presence, and hopefully not any mugshots.</p>
<p>Shaan recommends creating a binge bank for yourself. Think of it as a bank of content that someone can go down the rabbit hole on you. Blogs, videos, newsclippings. Hopefully a collection of stories. Whatever content that gives someone all access pass to how your brain works and who you are as a person. By the end a person‚Äôs opinion of you should drastically change. This binge bank can become more powerful than a resume, because it shows your true self. While also showcasing how you communicate.</p>
<p>After hearing this I started to curate my own binge bank. I added a <a href="https://mftokic.github.io/start_here.html">Start Here</a> section to my personal site. On this page I have links to posts that best describe me and my capabilities. Right now it‚Äôs small but one day I plan to grow it to the binge banks of people like <a href="https://ryanholiday.net/best-articles/">Ryan Holiday</a>, <a href="https://tim.blog/new-start-here/">Tim Ferriss</a>, or <a href="https://markmanson.net/articles">Mark Manson</a>.</p>
</section>
<section id="growing-an-audience" class="level3">
<h3 class="anchored" data-anchor-id="growing-an-audience">Growing An Audience</h3>
<p>Shaan has a friend with a cool rule about being interesting. If you tell someone something interesting, they will say ‚Äúwow that‚Äôs interesting‚Äù. If you tell someone two interesting things, the will say ‚Äúthose are interesting‚Äù. If you tell someone three interesting things, they will say ‚Äúok, now you are interesting‚Äù. To build a following on the internet or at your job, you need to demonstrate your insight constantly. Instead of doing this three times you may have to do this 100 times to get people to come back and pay attention.</p>
<p>Another powerful point that Shaan learned from a branding expert is people will follow you to the ends of the earth if you can give them a feeling more consistently than anyone else. Why do people listen to podcasts from comedians? It‚Äôs not because they have groundbreaking insight about current events or they perform their standup routine. It‚Äôs because when people listen they feel like ‚Äúone of the guys‚Äù who like to hang out with their friends and crack jokes at one another. How people feel after consuming your story is the end goal.</p>
</section>
<section id="closing-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h3>
<p>Storytelling is more art than science. It takes reps to get it right. If you can tell good stories, you can do almost anything in this world. Get out there and start telling better stories today. Just not ones about your vacation.</p>


</section>

 ]]></description>
  <category>podcast</category>
  <category>learning</category>
  <guid>https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/</guid>
  <pubDate>Tue, 04 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/image.png" medium="image" type="image/png" height="78" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Deep Learning Last</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the tenth and final principle of a good time series forecast, deep learning last. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/"><strong>Deep Learning Last</strong></a></li>
</ol>
</section>
<section id="the-shiny-new-thing" class="level3">
<h3 class="anchored" data-anchor-id="the-shiny-new-thing">The Shiny New Thing</h3>
<p>Deep learning is the latest frontier in the field of machine learning. It‚Äôs a subset of machine learning that uses neural networks with many layers (hence ‚Äúdeep‚Äù) to model complex patterns in data. These neural networks are built to resemble how human brains work. There are a lot of different types of deep learning models. Even the latest large language models from OpenAI are using deep learning techniques.</p>
<p>Since deep learning is getting all the hype nowadays, it can be tempting to go straight to training deep learning models when starting a new forecasting project. This is a bad idea. While deep learning can be very effective, there are many reasons I‚Äôll call out in this post that make deep learning hard to use for forecasting projects. You can still use a deep learning model in your forecast, but I recommend exhausting all other avenues before trying deep learning. Let‚Äôs dive into why deep learning should be tried last.</p>
</section>
<section id="reasons-to-use-deep-learning-last" class="level3">
<h3 class="anchored" data-anchor-id="reasons-to-use-deep-learning-last">Reasons To Use Deep Learning Last</h3>
<section id="lack-of-quality-data" class="level4">
<h4 class="anchored" data-anchor-id="lack-of-quality-data"><strong>Lack of Quality Data</strong></h4>
<p>Deep learning can work well if you have thousands, or better yet millions, of observations in your historical data. In my job we might be trying to forecast a monthly time series for a single product, but only have the last three years of historical data. That‚Äôs 36 data points. This lack of data is a common problem at my company, where new products are released constantly (meaning they have limited data) and our business shifts so often that even historical years from six years ago may not be relevant to where our business is headed. If you don‚Äôt have tons of historical data, it becomes very hard to train an accurate deep learning model.</p>
</section>
<section id="expensive-hardware" class="level4">
<h4 class="anchored" data-anchor-id="expensive-hardware"><strong>Expensive Hardware</strong></h4>
<p>Deep learning requires millions of matrix algebra calculations. Think of it as multiplying two sets of tables together. Regular computers have CPUs (central processing unit), which are designed for sequential processing. Even if you have 10+ CPUs on a computer, it will take a while to crank through the millions of matrix operations needed to train a deep learning model. GPUs on the other hand, are specialized to have thousands of cores and parallelize matrix operations effectively. They were initially built for video game graphics, hence the name graphical processing unit, but in recent years have stumbled across a new use case in training deep learning models. This is why Nvidia is the third most valuable company at the time of this writing, since they are the leading manufacturer of GPUs. These GPUs are hard to build, making them expensive to buy or rent from a cloud provider. Because they are expensive to use, they make it harder for anyone to start using them. With non-deep learning models you can start training them on your local computer, but to train a deep learning model you either have to camp out at Best Buy to purchase a Nvidia H100 or jump through a lot of hoops with a cloud provided to rent one by the minute. The juice may not be worth the squeeze.</p>
</section>
<section id="bigger-black-box" class="level4">
<h4 class="anchored" data-anchor-id="bigger-black-box"><strong>Bigger Black Box</strong></h4>
<p>Deep learning models are often harder to interpret than other machine learning models. This is due to them having up to billions of parameters (model inputs) that are abstracted between multiple layers. This means one layer of parameters feeds into another layer of parameters. There are ways to interpret the inner workings of these models, but they are often just an educated guess. Can anyone explain how a deep learning model like GPT-4 came up with its answer? Not likely.</p>
</section>
</section>
<section id="what-to-use-instead" class="level3">
<h3 class="anchored" data-anchor-id="what-to-use-instead">What to Use Instead</h3>
<p>Maybe I‚Äôve convinced you to not chase after the shiny thing and try something non-deep learning first. What model should I use instead? Here is what to try first. Once you have tried these models and evaluated their performance, you can then see if the juice is worth the squeeze with deep learning.</p>
<section id="univariate-models" class="level4">
<h4 class="anchored" data-anchor-id="univariate-models"><strong>Univariate Models</strong></h4>
<p>These are models that only need one variable, historical values of what you‚Äôre trying to forecast. Univariate models are more statistics than machine learning, and are custom built for time series. They train very fast and are tuned for each specific time series in your data. One weakness is some of these models cannot take in outside data in the form of features. With that said they are a terrific starting point for any new forecasting project. Often they can get you the required accuracy needed, and if they don‚Äôt they can serve as the benchmark to beat with other models. Here are a few common univariate models to try first.</p>
<ul>
<li><strong>ARIMA</strong>: An ARIMA (AutoRegressive Integrated Moving Average) model predicts future values in a time series by combining differencing (modeling the difference between periods), autoregression (using past values), and moving averages (using past forecast errors).</li>
<li><strong>Exponential Smoothing</strong>: Forecasts future values in a time series by applying decreasingly weighted averages of past observations, giving more importance to recent data points to capture trends and seasonal patterns.</li>
<li><strong>Seasonal Naive</strong>: Predicts future values by simply repeating the observations from the same season of the previous period, assuming that future patterns will mimic past seasonal cycles.</li>
</ul>
</section>
<section id="traditional-ml-models" class="level4">
<h4 class="anchored" data-anchor-id="traditional-ml-models"><strong>Traditional ML Models</strong></h4>
<p>After trying univariate models, it‚Äôs time to try more traditional machine learning models. These are models built specifically for tabular data, or data that can live in a SQL table or excel spreadsheet. These models are multivariate, which allow them to incorporate outside variables as features to improve their forecast accuracy. They require more handling than a model like ARIMA, since they need <a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/index.html">feature engineering</a> and proper <a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/index.html">time series cross-validation</a>. Multivariate models can also learn across multiple time series at the same time, instead of being trained on just a single time series like a univarite model. Here are a few common multivariate models.</p>
<ul>
<li><strong>Linear Regression</strong>: Predicts future values by fitting a line to the historical data, where the line represents the relationship between the dependent variable and one or more independent variables.</li>
<li><strong>XGBoost</strong>: Predicts future values using an ensemble of decision trees, boosting their performance by iteratively correcting errors from previous trees, resulting in a highly accurate and robust prediction model.</li>
<li><strong>Cubist</strong>: Predicts future values by combining decision trees with linear regression models, creating rule-based predictions that incorporate linear relationships within each segment of the data for greater accuracy.</li>
</ul>
</section>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>Do mega retail corporations like Amazon or Walmart only use ARIMA or linear regression models when trying to forecast the millions of product skus in their universe? Probably not. When the stakes are that high, and they can hire hundreds of data scientists to forecast, then they most likely build their own custom deep learning approaches that can learn from billions of data points to produce robust forecasts. With limitless resources and data, deep learning becomes easy. Assume you are not them.</p>
<p>Exciting startups like Nixtla have been doing great work on deep learning transformer models. These are the types of models that power products like GPT-4 from OpenAI. They built something called <a href="https://docs.nixtla.io/">TimeGPT-1</a>, which is a generative model for time series. They trained this model on billions of publicly available time series, creating the first GPT model tailored to time series. What required special hardware and tons of data to train can now be a simple API call in any programming language. This is a potential game changer and can completely change how forecasting is done, turning it more into a software engineering problem than a data science problem. Keep a close eye on this space as innovations like this can move at light speed.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>While deep learning holds great promise and can offer high accuracy in certain scenarios, it is often not the best starting point for most forecasting projects. The need for extensive data, expensive hardware, and the complexity of interpreting deep learning models make it a less practical choice compared to more traditional methods. Starting with simpler, well-established models like ARIMA, exponential smoothing, or traditional machine learning models often provides sufficient accuracy with lower costs and greater interpretability. As innovations continue to emerge, especially with models like TimeGPT-1, the landscape of time series forecasting may shift, making deep learning more accessible and practical. However, for now, prioritize simpler, more transparent models and reserve deep learning as a last resort when simpler methods fall short.</p>
</section>
<section id="series-wrap-up" class="level3">
<h3 class="anchored" data-anchor-id="series-wrap-up">Series Wrap Up</h3>
<p>That‚Äôs a wrap on our <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">First Principles in Time Series Forecasting</a> series! My goal was to walk through core concepts of creating a strong time series forecast. Instead of diving deep into code and super technical concepts, I wanted to give timeless knowledge that will serve anyone who builds or consumes time series forecasts. Hopefully you enjoyed the series and learned a lot ü§û.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/</guid>
  <pubDate>Fri, 31 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/image.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Model Combinations Are King</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the ninth principle of a good time series forecast, model combinations are king. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/"><strong>Model Combinations Are King</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="wisdom-of-the-crowds" class="level3">
<h3 class="anchored" data-anchor-id="wisdom-of-the-crowds">Wisdom of the Crowds</h3>
<p>In 1906, famed statistician Francis Galton went to a county fair for some fun. While there he came upon a competition to guess the weight of an ox. Eight hundred people entered the competition but the guesses were all over the place, some too high, some too low. Francis was a big numbers guy, so he took all of the guesses home with him and crunched the data. He found out that the average of all the guesses was only one pound away from the actual weight of the ox, which weighed 1,198 pounds. That‚Äôs an error of less than 0.08%. What he stumbled upon that day is now know as the wisdom of the crowds.</p>
<p>The concept of wisdom of the crowds states that the collective wisdom of a group of individuals is usually more accurate than that of a single expert. When guessing the weight of the ox, the overestimates and underestimates of regular people cancelled each other out. Creating an average prediction that was more accurate and any single person‚Äôs estimate.</p>
<p>This principle is important in machine learning forecasting. Usually it‚Äôs not one single model that performs the best, but instead a combination of multiple models. Let‚Äôs take a look at how we can combine models into more accurate forecasts.</p>
</section>
<section id="types-of-model-combinations" class="level3">
<h3 class="anchored" data-anchor-id="types-of-model-combinations">Types of Model Combinations</h3>
<p>There are many different ways individual model forecasts can be combined to create more accurate forecasts. For today we‚Äôll cover the most common approaches. If you‚Äôd like to dive deeper I recommend this <a href="https://robjhyndman.com/publications/combinations/index.html">amazing paper</a> by our forecasting Godfather Rob Hyndman.</p>
<ol type="1">
<li><strong>Simple Average</strong>: As simple as it sounds. Just take the forecasts from individual models and average them together.</li>
<li><strong>Ensemble Models</strong>: Feed the individual model forecasts as features into a machine learning model, and have the model come up with the correct weighted combination. This is also known as ‚Äúmodel stacking‚Äù.</li>
<li><strong>Hierarchical Reconciliation</strong>: This involves forecasting at different aggregations of the data set based on its inherent hierarchies, then reconciling the down to the lowest level (bottoms up) using a statistical process. For example forecasting by city, country, continent, and global level then reconciling each forecast down to the city level. This reconciliation can be thought as combining different forecasts together to create something more accurate. This approach has more nuances, and will be covered in another post.</li>
</ol>
</section>
<section id="model-combination-example" class="level3">
<h3 class="anchored" data-anchor-id="model-combination-example">Model Combination Example</h3>
<p>Let‚Äôs walk through a simple example around how combining the predictions of more than one model can outperform any single model. Below is an example monthly time series. We will try to back test the last 12 months of the historical data.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/chart1.png" class="img-fluid"></p>
<p>To keep things simple we can just run a few models to get the back testing results for the last year of the data. We‚Äôll use various univariate time series models. Ignore the types of models used. Instead, let‚Äôs just see how each model did on it‚Äôs own. Learn more about accuracy metrics in a <a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">previous post</a>.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/chart2.png" class="img-fluid"></p>
<table class="caption-top table">
<caption>Accuracy by Single Model</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>MAPE</th>
<th>MAE</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>arima</td>
<td>1.97</td>
<td>3.76</td>
<td>4.68</td>
</tr>
<tr class="even">
<td>croston</td>
<td>10.18</td>
<td>19.54</td>
<td>20.01</td>
</tr>
<tr class="odd">
<td>nnetar</td>
<td>9.77</td>
<td>18.37</td>
<td>26.00</td>
</tr>
<tr class="even">
<td>stlm-ets</td>
<td>1.92</td>
<td>3.68</td>
<td>4.59</td>
</tr>
<tr class="odd">
<td>tbats</td>
<td>1.86</td>
<td>3.51</td>
<td>4.05</td>
</tr>
<tr class="even">
<td>theta</td>
<td>2.46</td>
<td>4.71</td>
<td>5.52</td>
</tr>
</tbody>
</table>
<p>It looks like the tbats model performs the best across the board with stlm-ets and arima not far behind. What if we averaged the three of them together? Let‚Äôs see how the results change.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/chart3.png" class="img-fluid"></p>
<table class="caption-top table">
<caption>Accuracy for Average Model</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>MAPE</th>
<th>MAE</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>arima_stlm-ets_tbats</td>
<td>1.84</td>
<td>3.51</td>
<td>3.99</td>
</tr>
</tbody>
</table>
<p>Even better results! See how creating simple model averages can improve the results? Averaging the results can help smooth out any under or over forecasts, creating more accurate models.</p>
<p>Simple model averages are often the quickest way to improved forecast accuracy. Another way is to create an ensemble model that can create the weights on its own. Let‚Äôs feed the predictions from each model into a linear regression model and have it determine the optimal weights.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/chart4.png" class="img-fluid"></p>
<table class="caption-top table">
<caption>Accuracy for Ensemble Model</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>MAPE</th>
<th>MAE</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ensemble</td>
<td>1.81</td>
<td>3.40</td>
<td>3.65</td>
</tr>
</tbody>
</table>
<p>Alight more accurate results! By feeding each individual model forecast into a final ensemble model, we were able to get a more accurate forecast.</p>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>When trying to combine models, there is always a risk of overfitting. Meaning the combination approach (like simple average or ensemble) could have great accuracy on the back test data but not generalize well to new unseen data in our future forecast. To prevent that we can make sure to back test on enough historical data to prove our combination approach works well for more than just a period or two. We can also have separate validation and test splits in the back testing to see how combinations made on one data set can generalize well when tested on the other.</p>
<p><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/#future-uncertainty">Prediction intervals</a> are harder to create. Simply combining the 80% and 95% prediction intervals of multiple models together is not going to fully capture the uncertainty of forecasts created by the new model combination. So we would need to re-create the intervals based on the results of the new combined model.</p>
<p>Similar to prediction intervals, combining models can also make it harder to interpret them. Instead of just understanding one model and its predictions, we now have to understand how multiple models work and are combined to get the final forecast.</p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>Model combinations can be hard to do effectively. Thankfully my forecasting package, <a href="https://microsoft.github.io/finnts/index.html">finnts</a>, is here to help! It automatically handles every kind of model combination method listed in this post. Check out the package and see just how easy forecasting can be.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Just like the county fair crowd nailed the ox‚Äôs weight, combining multiple models in time series forecasting yields more accurate predictions by balancing out individual errors. When you‚Äôre forecasting, remember to embrace the collective wisdom of models for better results!</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/</guid>
  <pubDate>Tue, 28 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Multistep Horizon Forecasting With finnts</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-13-finnts-multistep-horizon/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-13-finnts-multistep-horizon/image.png" class="img-fluid"></p>
<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TL;DR</h3>
<p>I‚Äôm excited to announce that we just released a new feature in our machine learning forecast package, <a href="https://microsoft.github.io/finnts">finnts</a>, centered around multistep horizon forecasting. It‚Äôs a mouthful to say but at a high level it helps improve forecast accuracy by optimizing models to be accurate at each period of a forecast horizon. For example, a 3 month forecast would then be optimized for the forecast in each month (period) of the future forecast.</p>
<p>Let‚Äôs dive in to how multivariate modeling used to work in the package and how multistep horizon forecasting can help.</p>
</section>
<section id="how-it-used-to-work" class="level3">
<h3 class="anchored" data-anchor-id="how-it-used-to-work">How It Used to Work</h3>
<p>Let‚Äôs use an example of a monthly revenue forecast for our business‚Äôs main product. In practice we would want more than a year of data but let‚Äôs just keep it simple today.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Date</th>
<th>Revenue</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2023-01-01</td>
<td>120</td>
</tr>
<tr class="even">
<td>2023-02-01</td>
<td>135</td>
</tr>
<tr class="odd">
<td>2023-03-01</td>
<td>140</td>
</tr>
<tr class="even">
<td>2023-04-01</td>
<td>145</td>
</tr>
<tr class="odd">
<td>2023-05-01</td>
<td>150</td>
</tr>
<tr class="even">
<td>2023-06-01</td>
<td>155</td>
</tr>
<tr class="odd">
<td>2023-07-01</td>
<td>160</td>
</tr>
<tr class="even">
<td>2023-08-01</td>
<td>165</td>
</tr>
<tr class="odd">
<td>2023-09-01</td>
<td>170</td>
</tr>
<tr class="even">
<td>2023-10-01</td>
<td>175</td>
</tr>
<tr class="odd">
<td>2023-11-01</td>
<td>180</td>
</tr>
<tr class="even">
<td>2023-12-01</td>
<td>185</td>
</tr>
</tbody>
</table>
<p>If we wanted to forecast the next 3 months of revenue using multivariate machine learning models we would have to do some <a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">feature engineering</a> to get our data in good shape. This involves creating lags on our <span style="text-decoration: underline; cursor: help;" title="What we want to forecast, in this case Revenue.">target variable</span>. Let‚Äôs try create some lags on this data.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Date</th>
<th>Revenue</th>
<th>Revenue_Lag1</th>
<th>Revenue_Lag2</th>
<th>Revenue_Lag3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2023-01-01</td>
<td>120</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>2023-02-01</td>
<td>135</td>
<td>120</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2023-03-01</td>
<td>140</td>
<td>135</td>
<td>120</td>
<td></td>
</tr>
<tr class="even">
<td>2023-04-01</td>
<td>145</td>
<td>140</td>
<td>135</td>
<td>120</td>
</tr>
<tr class="odd">
<td>2023-05-01</td>
<td>150</td>
<td>145</td>
<td>140</td>
<td>135</td>
</tr>
<tr class="even">
<td>2023-06-01</td>
<td>155</td>
<td>150</td>
<td>145</td>
<td>140</td>
</tr>
<tr class="odd">
<td>2023-07-01</td>
<td>160</td>
<td>155</td>
<td>150</td>
<td>145</td>
</tr>
<tr class="even">
<td>2023-08-01</td>
<td>165</td>
<td>160</td>
<td>155</td>
<td>150</td>
</tr>
<tr class="odd">
<td>2023-09-01</td>
<td>170</td>
<td>165</td>
<td>160</td>
<td>155</td>
</tr>
<tr class="even">
<td>2023-10-01</td>
<td>175</td>
<td>170</td>
<td>165</td>
<td>160</td>
</tr>
<tr class="odd">
<td>2023-11-01</td>
<td>180</td>
<td>175</td>
<td>170</td>
<td>165</td>
</tr>
<tr class="even">
<td>2023-12-01</td>
<td>185</td>
<td>180</td>
<td>175</td>
<td>170</td>
</tr>
<tr class="odd">
<td>2024-01-01</td>
<td>???</td>
<td>185</td>
<td>180</td>
<td>175</td>
</tr>
<tr class="even">
<td>2024-02-01</td>
<td>???</td>
<td>???</td>
<td>185</td>
<td>180</td>
</tr>
<tr class="odd">
<td>2024-03-01</td>
<td>???</td>
<td>???</td>
<td>???</td>
<td>185</td>
</tr>
</tbody>
</table>
<p>We added some rows onto the bottom of the data, to allow us to forecast out the next 3 months after our historical data ends. That‚Äôs what we have question marks ‚Äú???‚Äù for those values. We also added lags for a 1 month, 2 month, and 3 month lag. But hey, looks like we have a problem. We have more question marks for a few future months for lag 1 and lag 2. If we wanted to forecast the next three months we wouldn‚Äôt be able to use those lags, since once we get out further in the <span style="text-decoration: underline; cursor: help;" title="How far out we want to forecast, in this case we have a forecast horizon of 3.">forecast horizon</span> we start to have missing lag data.</p>
<p>This means that the smallest lag we could use would always be equal to or greater than the forecast horizon. Since our forecast horizon is 3 than the smallest lag we could use to train a model on would be lag 3. This approach can yield good results, but it removes a lot of potential signal in the data. Revenue next month is most likely impacted by how revenue grew in the current month, but if our forecast horizon is long a lot of this insight has to get thrown away before we can train models. Imagine a forecast horizon of 12. For a monthly forecast this limits our lags to 12 months or more, which is a really bummer since our business can drastically change within 3-6 months, and not using that information in our model can hurt forecast accuracy.</p>
</section>
<section id="how-multistep-horizon-forecasting-works" class="level3">
<h3 class="anchored" data-anchor-id="how-multistep-horizon-forecasting-works">How Multistep Horizon Forecasting Works</h3>
<p>Multistep horizon helps fix this issue that allows us to use smaller lags while still being able to have long forecast horizons. In our 3 month forecast horizon example, we can keep the lag 1 and lag 2 features, but how the model gets trained will be different.</p>
<p>In the non-multistep horizon approach, a specific model is trained once on the data using lags that are equal or greater than the forecast horizon. When we run a multistep horizon approach, we can actually train multiple sub models under the hood of a specific model. In our 3 month forecast horizon, here‚Äôs how one model like linear regression will be trained.</p>
<ul>
<li>For the first month in the forecast horizon, we can use all available lags. Lag 1, lag 2, and lag 3 of revenue will all be used to predict the first month.</li>
<li>In the second month of the forecast horizon, we will use lag 2 and 3 to predict the second month.</li>
<li>In the third month of the forecast horizon, we will use lag 3 to predict the third month.</li>
</ul>
<p>Are you starting to get the hang of it? With multistep horizon forecasting we can still have one model that under the hood has multiple sub models that are each optimized on forecasting out a specific part of our forecast horizon. This allows us to have greater accuracy in the first few periods of our forecast horizon. In a non-mulitstep horizon approach, we are always optimizing for the last period in a forecast horizon. If the forecast horizon is 12 months, the way we do the feature engineering and train models is optimized for forecasting out the 12th month. When running a multistep horizon approach, we instead optimize for every period of the forecast horizon.</p>
<p>This kind of approach is so crucial to forecasters in the corporate finance space. Often these financial analysts are tasked with always forecasting out the rest of the entire fiscal year, even though they might only care about the next 3 months, since they are most likely going to be re-creating a new forecast in the following quarter. Multistep horizon forecasting allows these analysts to still forecast out long forecast horizons like 9 or 12 months, while still being able to optimize for the next 1-3 months. How cool is that!</p>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>If each specific model can have 2-5 sub models under the hood, the amount of time needed to train these models can multiply by the same amount. Make sure to keep that in mind if run time is a big factor in your forecasting process.</p>
<p>A multistep horizon forecast may not result in a more accurate forecast for smaller forecast horizons. Some time series may have a strong relationship with a 12 month lag, but less with a 1 month or 2 month lag. This means there is strong yearly seasonality in the data. If there is not a strong relationship with 1 month or 2 month lag, then having multiple sub models optimize for each future month in a multistep horizon approach may not result in more accurate forecasts. Consider doing some <span style="text-decoration: underline; cursor: help;" title="Data analysis that helps us better understand our historical data before we start feature engineering and training models.">exploratory data analysis</span> to see what kind of relationships there are with historical lags of your target variable and other features.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>The new multistep horizon forecasting approach in finnts allows users to create even more accurate forecasts, regardless of their forecast horizon length. If you‚Äôd like to learn more check out the <a href="https://microsoft.github.io/finnts/articles/models-used-in-finnts.html#multistep-horizon-models">official finnts documentation</a> to see how you can use the newest multistep horizon feature!</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <category>finnts</category>
  <guid>https://mftokic.github.io/posts/2024-05-13-finnts-multistep-horizon/</guid>
  <pubDate>Mon, 13 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-13-finnts-multistep-horizon/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Capture Uncertainty</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the eighth principle of a good time series forecast, capture uncertainty. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/"><strong>Capture Uncertainty</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="building-trust" class="level3">
<h3 class="anchored" data-anchor-id="building-trust">Building Trust</h3>
<p>Would you give your retirement savings to a hedge fund manager because they asked nicely? Probably not. Instead, you would like to do your research about them. Ask them how well they performed in the market historically, and also see how they expect the future markets to unravel in the near term. If their answer to those questions are, ‚ÄúI don‚Äôt have a historical track record‚Äù and ‚ÄúI have no clue what the future holds‚Äù then you are probably not going to give them one penny of your hard earned money. The same holds true for using a time series forecast created by machine learning (ML) models. In order to build trust with the end user of the forecast, you need to show them how a similar forecast would have performed historically and also quantify some aspect about the future. Let‚Äôs dive into each one.</p>
</section>
<section id="past-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="past-uncertainty">Past Uncertainty</h3>
<p>Before a ML model can be used to forecast the future, we need to see how it has handled the past. This is called back testing, where we see how a model performed historically. This can give us a good proxy around how it could perform in the future.</p>
<p>Back testing at its core is all about training a model on a portion of your historical data set (training data), then using the trained model on another portion of the historical data (testing data). This can be as simple as using the first 80% of your historical data to train a model, and use the last 20% for testing. Check out a <a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">previous post</a> to learn more about why the order of that train/test split is important.</p>
<p>There are also more advanced methods of doing this, like time series cross-validation. This involves many rounds of training a model and then creating a prediction on the testing data. Time series cross-validation can be used to tune model hyperparameters (inputs a model cannot learn from but must be given by a human) but is especially useful for model back testing. Check out the chart below that shows how we can effectively back test using a time series cross-validation approach. Each pass has its own train and test split, and the testing splits can overlap from one pass to another.</p>
<dl>
<dt><img src="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/chart1.png" class="img-fluid"></dt>
<dd>
<p>Source: Uber Engineering</p>
</dd>
</dl>
<p>In order to capture how accurate the back testing is, we need to calculate a metric that summarizes the model‚Äôs performance on the testing data splits. There are countless metrics we can use, each with their own pros and cons. That kind of discussion is out of scope for this post but let‚Äôs highlight a few common ones you could use in determining how accurate a model is during back testing.</p>
<ol type="1">
<li><strong>Mean Absolute Error (MAE)</strong>
<ul>
<li><strong>Description</strong>: MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It‚Äôs calculated as the average of the absolute differences between forecasts and actual observations.</li>
<li><strong>Strengths</strong>: MAE is straightforward and easy to interpret as it directly represents average error magnitude.</li>
<li><strong>Weaknesses</strong>: MAE treats all errors with the same weight, thus large errors have the same influence as small ones, which might not be optimal for all applications.</li>
</ul></li>
<li><strong>Root Mean Squared Error (RMSE)</strong>
<ul>
<li><strong>Description</strong>: RMSE is the square root of the mean of the squared errors. It measures the average magnitude of the error, with the squaring giving higher weight to larger errors.</li>
<li><strong>Strengths</strong>: RMSE is sensitive to outliers and provides a measure of how large errors are when they occur, which can be crucial for many practical applications.</li>
<li><strong>Weaknesses</strong>: Like MSE, RMSE can be heavily influenced by outliers and large errors, possibly leading to overestimations of the typical error if the error distribution is skewed.</li>
</ul></li>
<li><strong>Mean Absolute Percentage Error (MAPE)</strong>
<ul>
<li><strong>Description</strong>: MAPE expresses accuracy as a percentage, and it measures the size of the error in percentage terms. It is calculated as the average of the absolute errors divided by the actual values, expressed as a percentage.</li>
<li><strong>Strengths</strong>: MAPE is scale-independent and provides a clear interpretation in terms of percentage errors, making it easy to communicate.</li>
<li><strong>Weaknesses</strong>: MAPE can be highly skewed when dealing with values close to zero, and it disproportionately penalizes underestimations compared to overestimations.</li>
</ul></li>
</ol>
</section>
<section id="future-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="future-uncertainty">Future Uncertainty</h3>
<p>Now that we‚Äôve quantified how well our model works historically, we can just give the future forecast to our end user right? Not so fast. Our model might say that next month our company‚Äôs product will make $100, but if that‚Äôs all the info we provide to the end user of that forecast that‚Äôs not a good way to build trust. Instead we need to show how confident we are in that $100 forecast. How likely are we to hit that number? That‚Äôs where prediction intervals come in.</p>
<p>Prediction intervals help quantify the future uncertainty in our model‚Äôs forecast. They are statistical ranges, typically based on the forecast error, used to indicate the likelihood that the future value of a time series will fall within a specified range at a certain confidence level. Common ranges for a prediction interval are 80% and 95%. For example, the future forecast may be $100 but have a 95% prediction interval of $75 and $125. This means that there is a 95% likelihood that the future value will fall between $75 and $125. The tighter the range, the less uncertainty there is in the forecast. Below is an example forecast with 80% and 95% prediction intervals.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/chart2.png" class="img-fluid"></p>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>The back testing process can only ever be a proxy of what kind of results to expect on the future forecast. It follows the assumption that <a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">the future will be similar to the past</a>. Sometimes this is not the case, and future results may be worse than historical back testing performance.</p>
<p>While prediction intervals help quantify uncertainty, they also do not do a perfect job. There may be times where the future forecast will fall outside of the ranges. It‚Äôs not the end of the world when it does, but instead shows that the future is often different than what happened in the past. This is where strong domain knowledge comes in to understand what‚Äôs truly an outlier and what‚Äôs a new fundamental factor in your business going forward. For example, a new product launch in the future is hard to quantify with a prediction interval, but once it happens we can learn from that information and try to capture it the next time we train our model.</p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>Back testing and prediction intervals is tough work. Thankfully my forecasting package, <a href="https://microsoft.github.io/finnts/index.html">finnts</a>, takes care of both of these for you. You can even customize the back testing process to fit your needs. Check out the package and see just how easy forecasting can be.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Capturing uncertainty in time series forecasting is essential for creating robust forecasts that stakeholders can rely on. Utilizing back testing and prediction intervals not only strengthens the credibility of forecasts but also provides users with a clearer perspective on potential risks and variations. In the end these approaches help build trust with the forecast end user. The more trust we can build, the more likely the ML forecast will be used.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/</guid>
  <pubDate>Tue, 07 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Simple Models Are Better Models</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the seventh principle of a good time series forecast, simple models are better models. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/"><strong>Simple Models Are Better Models</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="occams-ml-model-razor" class="level3">
<h3 class="anchored" data-anchor-id="occams-ml-model-razor">Occam‚Äôs ML Model Razor</h3>
<p>William of Ockham was a 14th-century English Franciscan friar, philosopher, and theologian. In his work he preached that for most things in life the simplest explanation is the correct one. I‚Äôve learned this inadvertently in my life many times. For example, when I was studying for the ACT in high school a teacher told me that on the english questions it‚Äôs usually the shortest answer that is often correct. You could get a decent score just by following this one rule, even if you couldn‚Äôt read or speak english. This one tip saved my ass more than I‚Äôd like to admit, and I could read and speak english. Or so I thought.</p>
<p>Often in life, just like the ACT english section, it‚Äôs usually the simplest approaches that provide the best results. You can hire a fitness coach and buy all the supplements in the world but you‚Äôll probably get similar results following a handful of simple exercise and eating tips. The same applies in the world of machine learning. The more complexity you add to your data and models, the less likely they are going to be useful in the end. Let‚Äôs walk through how simplicity helps in all aspects of machine learning, from the data you use all the way down to models you train.</p>
</section>
<section id="more-features-more-problems" class="level3">
<h3 class="anchored" data-anchor-id="more-features-more-problems">More Features, More Problems</h3>
<p>In the world of time series forecasting, there are so many ways we can do feature engineering. Learn more about feature engineering in a <a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">previous post</a>. A dataset containing two columns, a date and value column, can be transformed into 100+ new features. This can easily get out of hand once we add external regressors (outside variables like consumer sentiment or inflation data) and create new features from them.</p>
<p>Each feature you add to a dataset hurts your model in multiple ways.</p>
<ol type="1">
<li>Train Time: It can slow down model training, meaning it will take longer to train the model. This may not seem like a big deal with small datasets but once you start having tens of thousands of rows in a dataset, adding a new feature can really slow things down.</li>
<li>Overfitting: Adding more features can lead to overfitting, meaning your model might be very accurate on the data it was trained on but cannot generalize well to unseen data in the future. Your model will learn from the noise in the data instead of the signal.</li>
<li>Interpretability: Adding more features makes it harder to explain the model‚Äôs predictions. If you can‚Äôt explain your forecast to non-technical business partners, then the forecast may not be used by anyone. I‚Äôve seen this countless times in my work. An accurate model doesn‚Äôt help anyone if the end user ultimately wants to know how the prediction was created. More on that in <a href="https://mftokic.github.io/posts/2023-02-11-three-levels-of-ml-adoption/">this post</a>.</li>
</ol>
</section>
<section id="feature-selection" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection">Feature Selection</h3>
<p>One way to simplify your data before model fitting is to implement a feature selection process. It‚Äôs called selection but it‚Äôs more like removal, where we drop any features that do not contribute to a model that can generalize well to new data. Here are a few techniques for feature selection.</p>
<ol type="1">
<li>Domain Expertise: Remove features that don‚Äôt make sense to you as a human. For example, the annual rain fall in Iceland might be perfectly correlated to Coca Cola sales in South America, but it doesn‚Äôt pass our smell test of being a factor that impacts the business. When in doubt take it out.</li>
<li>Correlation: If a feature has a strong correlation to the target variable (what we want to forecast) then we keep it in, but only after it passes our domain knowledge smell test.</li>
<li>Model Specific: Some models, like certain flavors of linear regression, have built in feature selection or feature importance. We can use that info to remove features and can then retrain on any kind of ML model.</li>
</ol>
<p>There are many other methods for feature selection, but are out of the scope of this post. The ones called out above are a good starting point.</p>
</section>
<section id="simple-models" class="level3">
<h3 class="anchored" data-anchor-id="simple-models">Simple Models</h3>
<p>Simplifying our data is helpful, but sometimes simplifying our models is even better. When starting a new forecast project, you might feel tempted to go out and build an advanced deep learning model, using all of the latest bells and whistles. That model may show promising results, but often a simpler model like linear regression can get the same or even better results. Models like linear regression are faster to train and have better model interpretability.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/image2.png" class="img-fluid"></p>
<p>We can even go one level deeper and not use any features at all. Univariate statistical models like ARIMA or exponential smoothing are classic time series forecasting models that only need one column of data, the historical values of your target variable. That‚Äôs what makes them univariate (one variable). They have built in feature engineering under the hood that allows them to learn from historical trends and seasonality in the data, so no additional work is needed to create features. Often in time series forecasting competitions a large team of deep learning researchers can just barely beat a single person team who uses simple models like ARIMA or random forest models. More on that in a future post.</p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>My forecasting package, <a href="https://microsoft.github.io/finnts/index.html">finnts</a>, has built in feature selection and other techniques to ensure simple models are built in ways that produce accurate forecasts. Check out the package and see for yourself.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Ultimately, the goal of any forecasting model is to provide clear, accurate, and quick results. Simpler models often meet these criteria better than complex ones because they‚Äôre easier to understand, faster to run, and just as accurate. By focusing on simplicity and minimizing inputs, we ensure that our forecasts are not only effective but also user-friendly. This approach doesn‚Äôt just save time; it makes the insights gained from the data accessible to everyone involved in the decision-making process. Simplicity, therefore, isn‚Äôt just a principle; it‚Äôs a practical strategy for better forecasting.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/</guid>
  <pubDate>Fri, 03 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: The Magic Is In The Feature Engineering</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-01-time-series-features/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the sixth principle of a good time series forecast, the magic is in the feature engineering. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/"><strong>The Magic Is In The Feature Engineering</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="turning-data-into-insight" class="level3">
<h3 class="anchored" data-anchor-id="turning-data-into-insight">Turning Data Into Insight</h3>
<p>A machine learning (ML) model is only as good as the data it‚Äôs fed. The process of transforming data, to make it easier for a model to learn from that data, is called feature engineering. It‚Äôs a technical term that is actually very simple in nature, really just data transformations. In the world of time series forecasting, feature engineering can make or break a good forecast.</p>
<p>Creating high quality features is a combination of strong domain expertise and data transformation skills. We have already covered how domain expertise impacts a forecast in a <a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">previous post</a>, so this post will cover how simple data transformations can drastically improve the accuracy of a machine learning forecast. Check out each category of time series feature engineering below to learn more.</p>
</section>
<section id="date-features" class="level3">
<h3 class="anchored" data-anchor-id="date-features">Date Features</h3>
<p>The most common type of feature engineering for time series is around dates. Date features allow us to capture seasonality patterns in our data. Think of seasonality as repeating peaks and valleys in our data. For example, our business might make most of its revenue in Q4 every year, with a subsequent dip in sales in Q1.</p>
<p>Let‚Äôs use the example time series below to illustrate each type of feature engineering.</p>
<table class="caption-top table">
<caption>Fake Time Series Data</caption>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Consumer Sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>100,000</td>
<td>68</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>110,000</td>
<td>67</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>120,000</td>
<td>65</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>115,000</td>
<td>70</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>130,000</td>
<td>72</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>125,000</td>
<td>73</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>135,000</td>
<td>74</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>140,000</td>
<td>75</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>130,000</td>
<td>70</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>145,000</td>
<td>72</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>150,000</td>
<td>71</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>160,000</td>
<td>75</td>
</tr>
</tbody>
</table>
<p>In this time series we would like to forecast monthly sales. We also have information about consumer sentiment that we can use to help forecast sales. A multivariate machine learning model cannot easily use the date column as is, so we have to do some data transformations (aka feature engineering) to make it easier for a model to understand how date information can help predict sales. Let‚Äôs go through a few examples of new features we can create from the date column. It‚Äôs important to note that after we create these new features it‚Äôs a good idea to remove the original date column before training a ML model.</p>
<p>Since the data is monthly there are a lot of simple features we can use. We can pull out the specific month, quarter, and even year into their own columns to use as features. If our data was at a daily level, we can even go deeper and get features related to day of the week, day of year, week of month, etc.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Date</th>
<th>Month</th>
<th>Quarter</th>
<th>Year</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>January</td>
<td>Q1</td>
<td>2023</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>February</td>
<td>Q1</td>
<td>2023</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>March</td>
<td>Q1</td>
<td>2023</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>April</td>
<td>Q2</td>
<td>2023</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>May</td>
<td>Q2</td>
<td>2023</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>June</td>
<td>Q2</td>
<td>2023</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>July</td>
<td>Q3</td>
<td>2023</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>August</td>
<td>Q3</td>
<td>2023</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>September</td>
<td>Q3</td>
<td>2023</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>October</td>
<td>Q4</td>
<td>2023</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>November</td>
<td>Q4</td>
<td>2023</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>December</td>
<td>Q4</td>
<td>2023</td>
</tr>
</tbody>
</table>
<p>That seems pretty straight forward right? Let‚Äôs keep squeezing our date fruit for more juice and see what other kinds of features we can create. Since this is a time series, adding some order of time can be helpful. This can be something as simple as an index starting at 1 (or even convert your date to a seconds format). This helps establish the proper order of our data and makes is easier for a model to pick up growing or declining trends over time. There is also slight differences in how many days there are from month to month, so we can add that too. If you don‚Äôt think that‚Äôs important then you have never been stung by the harsh mistress that is leap year. There have been multiple times where finance exec‚Äôs have dismissed forecasts for the quarter that includes February, where in the end we didn‚Äôt account for the fact that it was a leap year or we are one year removed from one. You can even take this one step further and add the number of business days for each month.</p>
<table class="caption-top table">
<caption>Adding a time index and other day related features</caption>
<thead>
<tr class="header">
<th>Date</th>
<th>Index</th>
<th>Days in Month</th>
<th>Business Days</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>1</td>
<td>31</td>
<td>22</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>2</td>
<td>28</td>
<td>20</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>3</td>
<td>31</td>
<td>23</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>4</td>
<td>30</td>
<td>20</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>5</td>
<td>31</td>
<td>23</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>6</td>
<td>30</td>
<td>22</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>7</td>
<td>31</td>
<td>21</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>8</td>
<td>31</td>
<td>23</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>9</td>
<td>30</td>
<td>21</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>10</td>
<td>31</td>
<td>22</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>11</td>
<td>30</td>
<td>22</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>12</td>
<td>31</td>
<td>21</td>
</tr>
</tbody>
</table>
<p>To get the final drop of juice out of the date column, we can also add Fourier series features. A Fourier series feature in time series forecasting is a component that captures seasonal patterns using sine and cosine functions to model periodic cycles in the data. In a nutshell they are just recurring peaks and valleys that can occur at various date grains like monthly or daily. These features can help capture more complex seasonality in your data. The chart below shows some standard Fourier series at the monthly and quarterly grain.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/chart1.png" class="img-fluid"></p>
</section>
<section id="lag-features" class="level3">
<h3 class="anchored" data-anchor-id="lag-features">Lag Features</h3>
<p>Time series forecasting is all about learning from the past to forecast the future. In order to learn about the past we have to create lags on our data. Often what we‚Äôre trying to forecast today is correlated to what happened in the past. This is a concept known as autocorrelation. For our monthly forecast example, a 3 month lag may be highly correlated to sales with a 0 month lag (or sales today). Consumer sentiment can also be correlated with sales, but this time a lag of 6 might have higher correlation, since there is most likely a long delay between customer purchase patters and how it affects our company‚Äôs product. Lags can be created for any amount, depending on your domain knowledge of the business and results from more exploratory data analysis (deep dive for a different day).</p>
<table class="caption-top table">
<caption>Adding lag features</caption>
<colgroup>
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 22%">
<col style="width: 21%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Consumer Sentiment</th>
<th>Sales 3-Month Lag</th>
<th>Sentiment 6-Month Lag</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>100,000</td>
<td>68</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>110,000</td>
<td>67</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>120,000</td>
<td>65</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>115,000</td>
<td>70</td>
<td>100,000</td>
<td></td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>130,000</td>
<td>72</td>
<td>110,000</td>
<td></td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>125,000</td>
<td>73</td>
<td>120,000</td>
<td></td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>135,000</td>
<td>74</td>
<td>115,000</td>
<td>68</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>140,000</td>
<td>75</td>
<td>130,000</td>
<td>67</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>130,000</td>
<td>70</td>
<td>125,000</td>
<td>65</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>145,000</td>
<td>72</td>
<td>135,000</td>
<td>70</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>150,000</td>
<td>71</td>
<td>140,000</td>
<td>72</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>160,000</td>
<td>75</td>
<td>130,000</td>
<td>73</td>
</tr>
</tbody>
</table>
<p>Last thing I‚Äôll say here is that you can also create leading features, especially for features that you know with 100% certainty ahead of time. For example, customers knowing of a new product launch in the future will definitely change how they purchase similar products you sell for the periods leading up to the launch. Someone may hold off on buying a new iPhone until the latest one gets released in a few months. Same goes for cars and many other products.</p>
</section>
<section id="rolling-window-features" class="level3">
<h3 class="anchored" data-anchor-id="rolling-window-features">Rolling Window Features</h3>
<p>Often using pure historical lags is not enough. The historical data of our target variable (what we want to forecast) can be very noisy, making it hard for a model to learn the proper trends and seasonality. One way to handle this is through rolling window transformations.</p>
<p>Rolling window features in time series forecasting help smooth out data, reduce noise, and capture essential trends and cycles by averaging or computing other statistics over a specified period. For a monthly forecast we can create rolling window features of averages, min/max, and other statistical calculations.</p>
<dl>
<dt><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/chart2.png" class="img-fluid"></dt>
<dd>
<p>Rolling Window Averages aka Moving Average</p>
</dd>
</dl>
<p>It‚Äôs best to calculate rolling window features based on your existing lag features. That way there is no <a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/#data-leakage">data leakage</a> during initial model training. See below for example of creating a 3 month rolling window average of the 3 month sales lag.</p>
<table class="caption-top table">
<caption>Rolling 3 month average applied to the 3 month sales lag</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 16%">
<col style="width: 27%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Sales 3-Month Lag</th>
<th>3-Month Rolling Avg</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>100,000</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>110,000</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>120,000</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>115,000</td>
<td>100,000</td>
<td></td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>130,000</td>
<td>110,000</td>
<td></td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>125,000</td>
<td>120,000</td>
<td>110,000</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>135,000</td>
<td>115,000</td>
<td>115,000</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>140,000</td>
<td>130,000</td>
<td>121,667</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>130,000</td>
<td>125,000</td>
<td>123,333</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>145,000</td>
<td>135,000</td>
<td>130,000</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>150,000</td>
<td>140,000</td>
<td>133,333</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>160,000</td>
<td>130,000</td>
<td>135,000</td>
</tr>
</tbody>
</table>
</section>
<section id="polynomial-features" class="level3">
<h3 class="anchored" data-anchor-id="polynomial-features">Polynomial Features</h3>
<p>The final type of feature engineering I‚Äôd like to discuss are polynomial transformations. Sometimes there is a non-linear relationship between your initial feature and the target variable. Some models, like ones that use decision trees, can handle this kind of relationship while others like linear regression cannot. To fix this we can transform the data via polynomials like squaring, cubing, and even taking the log of the initial feature.</p>
<p>Let‚Äôs take our example monthly sales data and add some spice to it. This time creating an exponential relationship between consumer sentiment and sales.</p>
<table class="caption-top table">
<caption>Updated sales data with an exponential relationship with consumer sentiment</caption>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Consumer Sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>1,309,000</td>
<td>68</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>1,204,000</td>
<td>67</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>1,000,000</td>
<td>65</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>1,525,000</td>
<td>70</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>1,849,000</td>
<td>72</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>1,964,000</td>
<td>73</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>2,121,000</td>
<td>74</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>2,500,000</td>
<td>75</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>1,525,000</td>
<td>70</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>1,849,000</td>
<td>72</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>1,764,000</td>
<td>71</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>2,500,000</td>
<td>75</td>
</tr>
<tr class="odd">
<td>January 2024</td>
<td>2,890,000</td>
<td>76</td>
</tr>
<tr class="even">
<td>February 2024</td>
<td>3,361,000</td>
<td>78</td>
</tr>
<tr class="odd">
<td>March 2024</td>
<td>3,844,000</td>
<td>79</td>
</tr>
<tr class="even">
<td>April 2024</td>
<td>4,641,000</td>
<td>81</td>
</tr>
</tbody>
</table>
<p>When graphing the data, see how the increase in consumer sentiment has an exponential effect on sales?</p>
<p><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/chart3.png" class="img-fluid"></p>
<p>To account for this, we can square the values of consumer sentiment and create a new feature to use. This new feature will make it easier for models like linear regression to capture these kinds of non-linear relationships.</p>
<table class="caption-top table">
<caption>New polynomial feature added</caption>
<colgroup>
<col style="width: 21%">
<col style="width: 15%">
<col style="width: 25%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Consumer Sentiment</th>
<th>Consumer Sentiment Squared</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>1,309,000</td>
<td>68</td>
<td>4,624</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>1,204,000</td>
<td>67</td>
<td>4,489</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>1,000,000</td>
<td>65</td>
<td>4,225</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>1,525,000</td>
<td>70</td>
<td>4,900</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>1,849,000</td>
<td>72</td>
<td>5,184</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>1,964,000</td>
<td>73</td>
<td>5,329</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>2,121,000</td>
<td>74</td>
<td>5,476</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>2,500,000</td>
<td>75</td>
<td>5,625</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>1,525,000</td>
<td>70</td>
<td>4,900</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>1,849,000</td>
<td>72</td>
<td>5,184</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>1,764,000</td>
<td>71</td>
<td>5,041</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>2,500,000</td>
<td>75</td>
<td>5,625</td>
</tr>
<tr class="odd">
<td>January 2024</td>
<td>2,890,000</td>
<td>76</td>
<td>5,776</td>
</tr>
<tr class="even">
<td>February 2024</td>
<td>3,361,000</td>
<td>78</td>
<td>6,084</td>
</tr>
<tr class="odd">
<td>March 2024</td>
<td>3,844,000</td>
<td>79</td>
<td>6,241</td>
</tr>
<tr class="even">
<td>April 2024</td>
<td>4,641,000</td>
<td>81</td>
<td>6,561</td>
</tr>
</tbody>
</table>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>Sometimes too much of a good thing can be a bad thing. Adding a lot of new features can increase the chance that a model overfits. Overfitting in machine learning occurs when a model learns to capture noise or random fluctuations in the training data, leading to poor generalization and high performance on training data but low performance on unseen data. The best way to prevent this kind of overfitting is to limit the number of features used to train a model. This will be discussed in greater detail in another post in this series.</p>
<p>Did you notice that when creating lags and rolling window features we had a lot of missing data at the start of the time series for those new features? This can be a problem. Some ML models do not like missing data, so we need to deal with those missing values. An easy way is to just drop the initial rows in the time series that have blank values for the new lags and rolling window features. This can work well if you have a lot of historical data. Dropping data can hurt model performance though, and if you don‚Äôt have a lot of data to start with it becomes a less favorable option. You could also replace the missing values, either by using a simple model to impute the value or just use the closest available value in the time series to ‚Äúfill in‚Äù the missing values. Both of these missing value replacement approaches have their own pros and cons but could be a better strategy then just simply dropping rows with missing values.</p>
<table class="caption-top table">
<caption>Filling in missing values with their closest available value</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 16%">
<col style="width: 27%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Sales 3-Month Lag</th>
<th>3-Month Rolling Avg</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>100,000</td>
<td>100,000</td>
<td>110,000</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>110,000</td>
<td>100,000</td>
<td>110,000</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>120,000</td>
<td>100,000</td>
<td>110,000</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>115,000</td>
<td>100,000</td>
<td>110,000</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>130,000</td>
<td>110,000</td>
<td>110,000</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>125,000</td>
<td>120,000</td>
<td>110,000</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>135,000</td>
<td>115,000</td>
<td>115,000</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>140,000</td>
<td>130,000</td>
<td>121,667</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>130,000</td>
<td>125,000</td>
<td>123,333</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>145,000</td>
<td>135,000</td>
<td>130,000</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>150,000</td>
<td>140,000</td>
<td>133,333</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>160,000</td>
<td>130,000</td>
<td>135,000</td>
</tr>
</tbody>
</table>
</section>
<section id="other-pre-processing" class="level3">
<h3 class="anchored" data-anchor-id="other-pre-processing">Other Pre-Processing</h3>
<p>One thing I wanted to add that technically isn‚Äôt considered feature engineering are other data pre-processing methods. These are things you apply before you start your feature engineering process. They are specific to time series forecasting and can greatly improve forecast accuracy. Here are two pre-processing methods you should know about.</p>
<p>First is making your data stationary. This is a time series technical term that pretty much means removing the trend component of your data, where the time series has a constant mean and standard deviation. We can make a time series stationary by the process of differencing. This involves taking the difference between each date observation and using that as the new time series to train models with. Check out the example below. See how the upward trend gets removed when we simply use the difference between months instead of the original monthly values? Some machine learning models, like ones that rely on decision trees, cannot extrapolate trends. So differencing the data removes any trend pattern, making it a lot easier for these models to produce high quality forecasts.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/chart4.png" class="img-fluid"></p>
<p>Another pre-processing technique is a box-cox transformation. This helps remove any exponentially increasing trends by applying various types of power transformations. For example, taking the log of your time series. Removing non-linear trends can make it a lot easier for a model to create accurate forecasts. See the example below of a time series with a non-linear trend. We can then apply a box-cox transformation and then difference the data. See how nice the final time series looks? It will be way easier for a ML model to learn the patterns in the final transformed time series.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/chart5.png" class="img-fluid"></p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>There‚Äôs a lot to unpack on feature engineering for time series forecasting. Thankfully my package, <a href="https://microsoft.github.io/finnts/index.html">finnts</a>, can automatically handle all of the feature engineering for you. It does everything I called out in this post plus more. Check it out and see just how easy ML forecasting can be.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Feature engineering is the backbone of successful time series forecasting, allowing models to uncover hidden patterns and relationships within the data, ultimately leading to more accurate predictions. By transforming raw data into meaningful features like date-related attributes, lag features, rolling window statistics, and polynomial transformations, we equip machine learning models with the necessary insights to make informed forecasts. However, it‚Äôs crucial to strike a balance between adding informative features and avoiding overfitting, as too many features can lead to poor generalization on unseen data. With careful consideration and the right techniques, feature engineering becomes a powerful tool in the arsenal of any data scientist or analyst aiming to unlock the predictive potential of time series data.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-05-01-time-series-features/</guid>
  <pubDate>Wed, 01 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-01-time-series-features/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Order Is Important</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-23-time-series-order/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-23-time-series-order/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the fifth principle of a good time series forecast, order is important. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/"><strong>Order Is Important</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="baking-cakes-over-making-smoothies" class="level3">
<h3 class="anchored" data-anchor-id="baking-cakes-over-making-smoothies">Baking Cakes Over Making Smoothies</h3>
<p>Machine learning (ML) is a lot like cooking. You have various ingredients and can combine them together in clever ways to make for a tasty dish. Most machine learning approaches like classification (predicting an outcome) and regression (predicting a number) can follow a similar process to making a smoothie. We can take some data (fruits and veggies) and blend it all together inside of our model blender.</p>
<p>Time series forecasting is a whole other beast. It still technically falls under the regression family tree but has to be handled very differently. Forecasting is more like baking a cake, where the order in which you do things is very important. For example, you cannot switch when you add the eggs and when you add the frosting. If you do you will certainly not be invited back to your nephew‚Äôs birthday party next year. In order to bake something tasty please follow the below guidance.</p>
</section>
<section id="time-series-training" class="level3">
<h3 class="anchored" data-anchor-id="time-series-training">Time Series Training</h3>
<p>Training any sort of machine learning model often requires two separate historical data sets. One that is used to train the initial model, then another that is set aside to create predictions based on the initial model. We can then see how accurate the predictions were on the test data set. This ensures that our new ML model can generalize well to new and unseen data, making sure our model doesn‚Äôt overfit to the training data.</p>
<p>Common ML approaches like classification and regression don‚Äôt need a lot of sophistication when splitting up the historical data between a training set and a testing set. Often it will be split randomly. This is similar to making a smoothie. You can randomly throw in bananas, apples, spinach, and blueberries. All without having to think about the order of when you do it.</p>
<p>Take the below housing data. This is a traditional regression problem. Let‚Äôs use the total square feet and number of bedrooms to predict how much the house will cost. We can randomly split 80% of the data to train the model, then hold out 20% of the data to test how accurate the model is. Randomly splitting the data ensures we get a healthy mix of different data in each split.</p>
<table class="caption-top table">
<caption>Example fake housing data for a regression model</caption>
<thead>
<tr class="header">
<th>Square_Feet</th>
<th>Bedrooms</th>
<th>Total_Cost</th>
<th>Split</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3774</td>
<td>2</td>
<td>822732</td>
<td>Training</td>
</tr>
<tr class="even">
<td>1460</td>
<td>1</td>
<td>245280</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>1894</td>
<td>4</td>
<td>602292</td>
<td>Training</td>
</tr>
<tr class="even">
<td>1730</td>
<td>4</td>
<td>550140</td>
<td>Training</td>
</tr>
<tr class="odd">
<td><strong>1695</strong></td>
<td><strong>4</strong></td>
<td><strong>539010</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="even">
<td>3692</td>
<td>5</td>
<td>1358656</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>2238</td>
<td>1</td>
<td>375984</td>
<td>Training</td>
</tr>
<tr class="even">
<td>2769</td>
<td>5</td>
<td>1018992</td>
<td>Training</td>
</tr>
<tr class="odd">
<td><strong>1066</strong></td>
<td><strong>5</strong></td>
<td><strong>392288</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="even">
<td>1838</td>
<td>1</td>
<td>308784</td>
<td>Training</td>
</tr>
</tbody>
</table>
<p>A time series has a built in order to it. It‚Äôs said right there in the name, time. Ignoring the order based on time can have disastrous consequences, resulting in your final future forecast not being accurate. Just like baking a cake, we need to make sure how we train a model is done in the right order. When splitting a historical time series into a training set and a testing set, splitting not at random but based on time is the proper way to go. Using the oldest data as the training set and the newest data as the testing set makes sure we respect the order of our data based on time. The example table below is a made up time series of the price of one specific house. In reality we would need a lot more data to train a good time series model but just be cool for a minute and go with me on this one. The split column now has the test data set at the very end instead of randomly split across time. We can now use <a href="https://developers.google.com/machine-learning/crash-course/framing/ml-terminology">features</a> like interest rates and gdp growth to help us forecast the price of this house over time. The first 9 months of data will train the model, and the final 3 months will be used to test the model‚Äôs accuracy.</p>
<table class="caption-top table">
<caption>Example fake time series for the price of a specific house</caption>
<colgroup>
<col style="width: 24%">
<col style="width: 23%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Interest_Rate</th>
<th>GDP_Growth</th>
<th>Total_Cost</th>
<th>Split</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2002</td>
<td>3.43635</td>
<td>1.58111</td>
<td>315052</td>
<td>Training</td>
</tr>
<tr class="even">
<td>February 2002</td>
<td>4.87679</td>
<td>0.03085</td>
<td>314723</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>March 2002</td>
<td>4.32998</td>
<td>-0.04544</td>
<td>312854</td>
<td>Training</td>
</tr>
<tr class="even">
<td>April 2002</td>
<td>3.99665</td>
<td>-0.04149</td>
<td>311865</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>May 2002</td>
<td>2.89005</td>
<td>0.26061</td>
<td>309452</td>
<td>Training</td>
</tr>
<tr class="even">
<td>June 2002</td>
<td>2.88999</td>
<td>0.81189</td>
<td>311106</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>July 2002</td>
<td>2.64521</td>
<td>0.57986</td>
<td>309675</td>
<td>Training</td>
</tr>
<tr class="even">
<td>August 2002</td>
<td>4.66544</td>
<td>0.22807</td>
<td>314681</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>September 2002</td>
<td>4.00279</td>
<td>1.02963</td>
<td>315097</td>
<td>Training</td>
</tr>
<tr class="even">
<td><strong>October 2002</strong></td>
<td><strong>4.27018</strong></td>
<td><strong>-0.15127</strong></td>
<td><strong>312357</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="odd">
<td><strong>November 2002</strong></td>
<td><strong>2.55146</strong></td>
<td><strong>0.23036</strong></td>
<td><strong>308345</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="even">
<td><strong>December 2002</strong></td>
<td><strong>4.92477</strong></td>
<td><strong>0.41591</strong></td>
<td><strong>316022</strong></td>
<td><strong>Testing</strong></td>
</tr>
</tbody>
</table>
</section>
<section id="data-leakage" class="level3">
<h3 class="anchored" data-anchor-id="data-leakage">Data Leakage</h3>
<p>Whenever time is involved in machine learning, the probability of shooting yourself in the foot rises. This has to do with the concept of <a href="https://machinelearningmastery.com/data-leakage-machine-learning/">data leakage</a>. Data leakage occurs when information from outside the training dataset is used to create the model, leading it to make overly optimistic predictions. It can also happen when we train with data that may not be available in the future when we need to create new predictions.</p>
<p>In time series forecasting we have already discussed one component of data leakage, related to splitting the data correctly based on time. Take the below table, instead of splitting properly by time the data is now split randomly. Our model can now ‚Äúsee ahead in time‚Äù when training, and in effect cheat when being evaluated on the testing splits. For example, for the test observation in July 2002 the model can learn from data on either side of that month. Figuring out previous and future trends and seasonality. This makes it easy to predict what the housing cost in July should be, since it has information before and after that month. With this approach our test accuracy will be a lot better than in the previous example where the splits are based on time. Future forecast performance will suffer though, since we have now trained and chosen a model that may only be good at figuring out how to extrapolate between two points, instead of trying to create predictions on unseen data in the future.</p>
<table class="caption-top table">
<caption>Incorrect train and test splits</caption>
<colgroup>
<col style="width: 24%">
<col style="width: 23%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Interest_Rate</th>
<th>GDP_Growth</th>
<th>Total_Cost</th>
<th>Split</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2002</td>
<td>3.43635</td>
<td>1.58111</td>
<td>315052</td>
<td>Training</td>
</tr>
<tr class="even">
<td><strong>February 2002</strong></td>
<td><strong>4.87679</strong></td>
<td><strong>0.03085</strong></td>
<td><strong>314723</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="odd">
<td>March 2002</td>
<td>4.32998</td>
<td>-0.04544</td>
<td>312854</td>
<td>Training</td>
</tr>
<tr class="even">
<td>April 2002</td>
<td>3.99665</td>
<td>-0.04149</td>
<td>311865</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>May 2002</td>
<td>2.89005</td>
<td>0.26061</td>
<td>309452</td>
<td>Training</td>
</tr>
<tr class="even">
<td>June 2002</td>
<td>2.88999</td>
<td>0.81189</td>
<td>311106</td>
<td>Training</td>
</tr>
<tr class="odd">
<td><strong>July 2002</strong></td>
<td><strong>2.64521</strong></td>
<td><strong>0.57986</strong></td>
<td><strong>309675</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="even">
<td>August 2002</td>
<td>4.66544</td>
<td>0.22807</td>
<td>314681</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>September 2002</td>
<td>4.00279</td>
<td>1.02963</td>
<td>315097</td>
<td>Training</td>
</tr>
<tr class="even">
<td>October 2002</td>
<td>4.27018</td>
<td>-0.15127</td>
<td>312357</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>November 2002</td>
<td>2.55146</td>
<td>0.23036</td>
<td>308345</td>
<td>Training</td>
</tr>
<tr class="even">
<td><strong>December 2002</strong></td>
<td><strong>4.92477</strong></td>
<td><strong>0.41591</strong></td>
<td><strong>316022</strong></td>
<td><strong>Testing</strong></td>
</tr>
</tbody>
</table>
<p>Ok, so we know not to split data randomly when training. Another thing to watch out for is how features are used in the model. In our time series housing example, we can use date information (month, quarter, etc) along with our macro features like interest rates and GDP growth. Let‚Äôs say we follow the right approach, split the data based on time, and see that we get good results on the test data. We can now take our model into production and try to create a forecast for the future. But wait, what do we do with the future feature values of interest rate and GDP growth? This is another potential data leakage issue, where data used to train the model is not available to create new predictions in the future. You might be thinking, no problem we can just create a forecast of future interest rates and gdp growth right? Wrong. If you can produce accurate interest rate and GDP growth forecasts, then you shouldn‚Äôt be reading this post. You should instead be sitting on your own private island, watching the return on your flagship hedge fund skyrocket. See where I‚Äôm going here? If you cannot perfectly predict future values of the feature you want to use, then you should consider not using that feature. You might be able to know with 100% certainty when a holiday or special event will take place, but not even expert economists can perfectly predict interest rate fluctuations. Instead you can look at using feature lags. Where instead of using real time interest rates or GDP growth you can instead use lags of them. For example, using a 3 or 12 month lag of each feature. Using lags in this example can help prevent <a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/#reversal">compounding errors</a> in your forecast.</p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>Looking for a way to to never worry about the order of your time series data again? Have no fear because <a href="https://microsoft.github.io/finnts/index.html">finnts</a> is here! Ok enough with the used car salesman talk. The finnts package is something myself and other outstanding team members have built to automate all of the tedious aspects related to time series forecasting. The package can automatically handle the proper splits of your data and has build in data leakage prevention. Check out the package to learn for yourself how easy forecasting can be.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Remember, order is important in forecasting. Make sure you don‚Äôt mix up your data when training models, and keep a look out for data leakage. Do this right and you just might get invited back to your nephew‚Äôs birthday party next year.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-04-23-time-series-order/</guid>
  <pubDate>Tue, 23 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-23-time-series-order/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Weekend Reads (4/19/24)</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-19-weekend-reads/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-19-weekend-reads/image.png" class="img-fluid"></p>
<section id="articles" class="level2">
<h2 class="anchored" data-anchor-id="articles">Articles</h2>
<ul>
<li><a href="https://hbr.org/2023/12/use-strategic-thinking-to-create-the-life-you-want">Use Strategic Thinking to Create the Life You Want</a></li>
<li><a href="https://www.maximiliankiener.com/digitalprojects/time/">How Time Flies</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Time Series First Principles: Higher Grain, Higher Accuracy</a></li>
</ul>
</section>
<section id="videos" class="level2">
<h2 class="anchored" data-anchor-id="videos">Videos</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=5N4UFl9G00A">Dave Chappelle - Unforgiven</a></li>
<li><a href="https://www.youtube.com/watch?v=-9ROlCeB5FQ">Balaji on AI Gods</a></li>
<li><a href="https://www.youtube.com/watch?v=SEnuWRLMI88">Building Laser Focus</a></li>
<li><a href="https://www.youtube.com/watch?v=5qGItht6U0E">Overrated and Underrated Habits</a></li>
<li><a href="https://www.youtube.com/watch?v=FPFqB1P9BIo">Lessons Learned on Writing</a></li>
</ul>
</section>
<section id="sites" class="level2">
<h2 class="anchored" data-anchor-id="sites">Sites</h2>
<ul>
<li><a href="https://suno.com/">Make Music with AI</a></li>
</ul>
</section>
<section id="tweets" class="level2">
<h2 class="anchored" data-anchor-id="tweets">Tweets</h2>
<ul>
<li><a href="https://twitter.com/trungtphan/status/1780794237426323918?s=46&amp;t=8Xa2BngQ9d359SJzFrCFMA">Building an Engineering Dream Team</a></li>
<li><a href="https://twitter.com/tunguz/status/1779532581274570991?s=46&amp;t=8Xa2BngQ9d359SJzFrCFMA">Experiences &gt; Things</a></li>
</ul>


</section>

 ]]></description>
  <category>weekend-reads</category>
  <guid>https://mftokic.github.io/posts/2024-04-19-weekend-reads/</guid>
  <pubDate>Fri, 19 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-19-weekend-reads/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Higher Grain Higher Accuracy</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-18-time-series-grain/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the fourth principle of a good time series forecast, the higher the grain the higher the accuracy. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/"><strong>Higher Grain Higher Accuracy</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="clear-skies" class="level3">
<h3 class="anchored" data-anchor-id="clear-skies">Clear Skies</h3>
<p>When planes take off from the ground they climb high into the sky. During that 5-10 minute period passengers have to stay seated with their seatbelt fastened. It‚Äôs only after the plane reaches 10,000 feet people can start to get up and move around the plane. Eventually the plane can reach an altitude of 40,000 feet. To compare, the peak of Mount Everest is 29,000 feet off the ground. Planes go up that high because it‚Äôs easier to fly the plane and more efficient. If planes flew a few feet off the ground it would be a lot bumpier ride, having to deal with changing weather and turbulence.</p>
<p>Forecasting is similar to flying a plane. Training a machine learning (ML) model at a higher grain of data is akin to a plane climbing in altitude. There is less turbulence (noise) in the data and your forecast has a better chance of being more accurate. You can either climb in altitude at the individual time series grain, or by the date grain. Let‚Äôs discuss each of them along with other methods.</p>
</section>
<section id="time-series-grain" class="level3">
<h3 class="anchored" data-anchor-id="time-series-grain">Time Series Grain</h3>
<p>Think of a higher time series grain as an aggregation of your original data. For example you might have product sales across a bunch of cities, where each city is a time series. Individual cities might have hard to model trend and seasonality, but when combined at a total country-level, it can be easier to model. Take the example charts below. Each city might be noisy but climbing in altitude up to the county level makes it easier to spot trends and seasonality.</p>
<p><img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/chart2.png" class="img-fluid"> <img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/chart1.png" class="img-fluid"></p>
</section>
<section id="date-grain" class="level3">
<h3 class="anchored" data-anchor-id="date-grain">Date Grain</h3>
<p>In a perfect world we would be able to forecast our businesses down to the day, or even minute, across the next 10 years. This is sadly not the case. The more granular you try to forecast at the date grain, the noisier the data is going to be, and the harder it will be to create accurate forecasts. Unless there is an absolute need to forecast at a certain level, I almost always recommend a higher date grain. Take the example charts below. See how aggregating daily data to a monthly date grain level makes it easier to spot trends and seasonality.</p>
<p><img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/chart3.png" class="img-fluid"> <img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/chart4.png" class="img-fluid"></p>
<p>In the finance org, the most common date grain to forecast at is month. This gives a healthy balance of being able to forecast long periods of time while also being able to update your forecast with new historical data every few weeks.</p>
<p>Here is another important point to call out. The longer your forecast horizon, the higher the date grain you should forecast at. Here are some recommendations based on your forecast horizon (how many periods you want to forecast). For example, if you‚Äôre trying to forecast out the next 6 months at the daily grain, you might get better results if you aggregate up to the monthly grain and forecast by month instead.</p>
<ul>
<li>Daily Grain: 1-90 day forecast horizon</li>
<li>Weekly Grain: 1-12 week forecast horizon</li>
<li>Monthly Grain: 1-18 month forecast horizon</li>
<li>Quarterly Grain: 1-8 quarter forecast horizon</li>
<li>Yearly Grain: more than 1 year forecast horizon</li>
</ul>
</section>
<section id="hierarchical-forecasting" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-forecasting">Hierarchical Forecasting</h3>
<p>A potential ‚Äúbest of both worlds‚Äù solution to the data grain issue is to use a hierarchical forecast. This is where you can train models at different grains of the data, then use a statistical process to reconcile each forecast together so they are in sync. Our forecasting godfather, Rob Hyndman, has done a lot of great work in this space. Here is a <a href="https://otexts.com/fpp3/hierarchical.html">chapter from his book</a> on hierarchical forecasting.</p>
<p>Let‚Äôs go back to our time series grain example. Using a hierarchical forecast you could train models and create forecasts for each city, then do the same at the total country-level, then finally do the same at a total world wide level across all countries. This is a standard hierarchical approach shown in the chart below. This hierarchical process blends a ‚Äúbottoms up‚Äù forecast of creating predictions at the lowest level by city, with a ‚Äútops down‚Äù forecast of creating predictions at the highest global level. A statistical process is then used to make the ‚Äútops down‚Äù forecast equal the ‚Äúbottoms up‚Äù forecast, optimizing for accuracy at all levels of the hierarchy.</p>
<p><img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/chart5.png" class="img-fluid"></p>
<p>The same idea can be applied at the date grain too. Where you can forecast at the daily level, weekly level, and monthly level. Then use a reconciliation process to get the final forecast at the daily level that is also accurate when summing up by month. This can work well if a monthly forecast is more accurate, but the final forecast needs to be at a daily level.</p>
</section>
<section id="allocations" class="level3">
<h3 class="anchored" data-anchor-id="allocations">Allocations</h3>
<p>Another option is to take a forecast at a higher grain and allocate it down to a lower grain using simple allocation logic. This process can replace the more complicated hierarchical forecasting discussed earlier. Simple allocations can be done in two ways.</p>
<p>The first is to take historical values and create a percent split to apply to the final forecast. For example we can create a forecast at the country-level, then split that out by city. The split percent by city (allocation percent) can be calculated based on how much each city was the percent of total country over the last few years. This can be broken down by period. So you can get a specific percent split for each month on average in the past. This approach helps maintain historical seasonality across each time series (each city). See the charts below for an example of using two historical years of monthly data to create the final allocation percentages.</p>
<table class="caption-top table">
<caption>Historical splits by city</caption>
<thead>
<tr class="header">
<th>Month</th>
<th>City A %</th>
<th>City B %</th>
<th>City C %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jan 2021</td>
<td>30.87%</td>
<td>30.08%</td>
<td>39.05%</td>
</tr>
<tr class="even">
<td>Feb 2021</td>
<td>28.11%</td>
<td>23.90%</td>
<td>47.99%</td>
</tr>
<tr class="odd">
<td>Mar 2021</td>
<td>23.77%</td>
<td>47.43%</td>
<td>28.81%</td>
</tr>
<tr class="even">
<td>Apr 2021</td>
<td>31.26%</td>
<td>18.42%</td>
<td>50.32%</td>
</tr>
<tr class="odd">
<td>May 2021</td>
<td>39.50%</td>
<td>34.08%</td>
<td>26.42%</td>
</tr>
<tr class="even">
<td>Jun 2021</td>
<td>50.58%</td>
<td>30.98%</td>
<td>18.44%</td>
</tr>
<tr class="odd">
<td>Jan 2022</td>
<td>36.33%</td>
<td>31.79%</td>
<td>31.88%</td>
</tr>
<tr class="even">
<td>Feb 2022</td>
<td>19.79%</td>
<td>40.74%</td>
<td>39.47%</td>
</tr>
<tr class="odd">
<td>Mar 2022</td>
<td>19.95%</td>
<td>28.99%</td>
<td>51.06%</td>
</tr>
<tr class="even">
<td>Apr 2022</td>
<td>20.37%</td>
<td>26.76%</td>
<td>52.87%</td>
</tr>
<tr class="odd">
<td>May 2022</td>
<td>41.37%</td>
<td>41.53%</td>
<td>17.10%</td>
</tr>
<tr class="even">
<td>Jun 2022</td>
<td>43.86%</td>
<td>41.10%</td>
<td>15.04%</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<caption>Average of city split by month</caption>
<thead>
<tr class="header">
<th>Month</th>
<th>City A %</th>
<th>City B %</th>
<th>City C %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jan</td>
<td>33.60%</td>
<td>30.93%</td>
<td>35.46%</td>
</tr>
<tr class="even">
<td>Feb</td>
<td>23.95%</td>
<td>32.32%</td>
<td>43.73%</td>
</tr>
<tr class="odd">
<td>Mar</td>
<td>21.86%</td>
<td>38.21%</td>
<td>39.93%</td>
</tr>
<tr class="even">
<td>Apr</td>
<td>25.82%</td>
<td>22.59%</td>
<td>51.60%</td>
</tr>
<tr class="odd">
<td>May</td>
<td>40.43%</td>
<td>37.81%</td>
<td>21.76%</td>
</tr>
<tr class="even">
<td>Jun</td>
<td>47.22%</td>
<td>36.04%</td>
<td>16.74%</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<caption>Final forecast using the city splits</caption>
<colgroup>
<col style="width: 8%">
<col style="width: 16%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Month</th>
<th style="text-align: right;">Country Forecast</th>
<th style="text-align: right;">City A %</th>
<th style="text-align: right;">City B %</th>
<th style="text-align: right;">City C %</th>
<th style="text-align: right;">City A Forecast</th>
<th style="text-align: right;">City B Forecast</th>
<th style="text-align: right;">City C Forecast</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Jan 2023</td>
<td style="text-align: right;">15000</td>
<td style="text-align: right;">33.60</td>
<td style="text-align: right;">30.93</td>
<td style="text-align: right;">35.46</td>
<td style="text-align: right;">5040</td>
<td style="text-align: right;">4639.5</td>
<td style="text-align: right;">5319</td>
</tr>
<tr class="even">
<td style="text-align: left;">Feb 2023</td>
<td style="text-align: right;">15200</td>
<td style="text-align: right;">23.95</td>
<td style="text-align: right;">32.32</td>
<td style="text-align: right;">43.73</td>
<td style="text-align: right;">3640.4</td>
<td style="text-align: right;">4912.64</td>
<td style="text-align: right;">6646.96</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mar 2023</td>
<td style="text-align: right;">15400</td>
<td style="text-align: right;">21.86</td>
<td style="text-align: right;">38.21</td>
<td style="text-align: right;">39.93</td>
<td style="text-align: right;">3366.44</td>
<td style="text-align: right;">5884.34</td>
<td style="text-align: right;">6149.22</td>
</tr>
<tr class="even">
<td style="text-align: left;">Apr 2023</td>
<td style="text-align: right;">15600</td>
<td style="text-align: right;">25.82</td>
<td style="text-align: right;">22.59</td>
<td style="text-align: right;">51.60</td>
<td style="text-align: right;">4027.92</td>
<td style="text-align: right;">3524.04</td>
<td style="text-align: right;">8049.6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">May 2023</td>
<td style="text-align: right;">15800</td>
<td style="text-align: right;">40.43</td>
<td style="text-align: right;">37.81</td>
<td style="text-align: right;">21.76</td>
<td style="text-align: right;">6387.94</td>
<td style="text-align: right;">5973.98</td>
<td style="text-align: right;">3438.08</td>
</tr>
<tr class="even">
<td style="text-align: left;">Jun 2023</td>
<td style="text-align: right;">16000</td>
<td style="text-align: right;">47.22</td>
<td style="text-align: right;">36.04</td>
<td style="text-align: right;">16.74</td>
<td style="text-align: right;">7555.2</td>
<td style="text-align: right;">5766.4</td>
<td style="text-align: right;">2678.4</td>
</tr>
</tbody>
</table>
<p>The second approach is to use a future forecast to create the allocation splits. For example we can create future forecasts at the country-level and also at the city-level. Then we can create the split percent for each city by taking the city forecast and summing it up to the country-level, then taking the percent split for each city. These splits can then be applied to the final country-level forecast to get the final forecast by city. This approach uses the more robust country-level forecast, while still trying to capture future changing trends and seasonality by city.</p>
<table class="caption-top table">
<caption>Initial forecast, where country and each city are forecasted separately</caption>
<colgroup>
<col style="width: 12%">
<col style="width: 22%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Month</th>
<th>Country Forecast</th>
<th>City A Forecast</th>
<th>City B Forecast</th>
<th>City C Forecast</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jan 2023</td>
<td>10,000</td>
<td>4,000</td>
<td>3,500</td>
<td>2,000</td>
</tr>
<tr class="even">
<td>Feb 2023</td>
<td>10,500</td>
<td>4,200</td>
<td>3,000</td>
<td>3,300</td>
</tr>
<tr class="odd">
<td>Mar 2023</td>
<td>11,000</td>
<td>4,500</td>
<td>3,200</td>
<td>3,300</td>
</tr>
<tr class="even">
<td>Apr 2023</td>
<td>11,500</td>
<td>4,800</td>
<td>3,400</td>
<td>3,300</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>12,000</td>
<td>5,000</td>
<td>3,500</td>
<td>3,500</td>
</tr>
<tr class="even">
<td>Jun 2023</td>
<td>12,500</td>
<td>5,200</td>
<td>3,800</td>
<td>3,500</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<caption>Calculating the percent splits by city</caption>
<thead>
<tr class="header">
<th>Month</th>
<th>City A %</th>
<th>City B %</th>
<th>City C %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jan 2023</td>
<td>42.11%</td>
<td>36.84%</td>
<td>21.05%</td>
</tr>
<tr class="even">
<td>Feb 2023</td>
<td>40.00%</td>
<td>28.57%</td>
<td>31.43%</td>
</tr>
<tr class="odd">
<td>Mar 2023</td>
<td>40.91%</td>
<td>29.09%</td>
<td>30.00%</td>
</tr>
<tr class="even">
<td>Apr 2023</td>
<td>41.74%</td>
<td>29.57%</td>
<td>28.70%</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>41.67%</td>
<td>29.17%</td>
<td>29.17%</td>
</tr>
<tr class="even">
<td>Jun 2023</td>
<td>41.60%</td>
<td>30.40%</td>
<td>28.00%</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<caption>Final forecast after applying the city splits to the country-level forecast</caption>
<colgroup>
<col style="width: 12%">
<col style="width: 29%">
<col style="width: 29%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Month</th>
<th>City A Final Forecast</th>
<th>City B Final Forecast</th>
<th>City C Final Forecast</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jan 2023</td>
<td>4,211</td>
<td>3,684</td>
<td>2,105</td>
</tr>
<tr class="even">
<td>Feb 2023</td>
<td>4,200</td>
<td>3,000</td>
<td>3,300</td>
</tr>
<tr class="odd">
<td>Mar 2023</td>
<td>4,500</td>
<td>3,200</td>
<td>3,300</td>
</tr>
<tr class="even">
<td>Apr 2023</td>
<td>4,800</td>
<td>3,400</td>
<td>3,300</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>5,000</td>
<td>3,500</td>
<td>3,500</td>
</tr>
<tr class="even">
<td>Jun 2023</td>
<td>5,200</td>
<td>3,800</td>
<td>3,500</td>
</tr>
</tbody>
</table>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>A more granular forecast can sometimes be more accurate, especially if the more detailed grain uncovers more stable trends and seasonality that can be modeled. Take for example a product whose sales are impacted by Chinese New Year. That holiday doesn‚Äôt happen on the same day every year, and it can even happen in different months. Sometimes in January, and sometimes in February. Since it happens over multiple days the split between the two months can change drastically from year to year. Creating a forecast at the daily level, adding information around when Chinese New Year is happening, could result in a more accurate forecast. You could also take the approach of a monthly forecast, and have a numeric feature that lists how many days of Chinese New Year falls within each month.</p>
<p>If your initial data at the higher grain is noisy or has low forecast accuracy, consider asking the domain expert if there could be more insightful trends and seasonality at a lower grain.</p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>Hierarchical forecasting is a tricky business, thankfully my open-source package <a href="https://microsoft.github.io/finnts/index.html">finnts</a> can automatically do hierarchical forecasting. The package can even use external regressors (features) in the hierarchical approach! Today finnts supports hierarchical forecasting at the time series grain. Hopefully one day we will implement hierarchical forecasting at the date grain, stay tuned. This is the same package I use internally at my job, allowing my company to replace hundreds of billions of manual forecasts with machine learning. Check out the package and see for yourself.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Just as pilots navigate to higher altitudes to find smoother skies and better efficiency, so too must we elevate our approach to data granularity in forecasting when needed. By stepping back from the minutiae of daily or city-level data and ascending to monthly or country-level aggregations, we enable our models to capture more coherent patterns and deliver forecasts with improved precision. This strategic shift‚Äîfrom a granular view to a broader perspective‚Äîis not just about avoiding turbulence; it‚Äôs about leveraging stability to enhance predictability.</p>
<p>However, the real magic often lies in blending these approaches through hierarchical forecasting. This method combines the detailed insights available at lower levels with the clarity and simplicity of higher-level forecasts, ensuring both depth and breadth in our predictive capabilities. As we continue to refine our techniques and tools, like the finnts package, we are paving the way for a future where complex, multi-tiered forecasting is as streamlined as a flight cruising at 40,000 feet.</p>
<p>In your journey through data, remember that the right altitude can make all the difference. Rising above the noise can provide not just clearer views, but also far-reaching insights. So, buckle up‚Äîwe‚Äôre about to take forecasting to new heights.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-04-18-time-series-grain/</guid>
  <pubDate>Thu, 18 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-18-time-series-grain/image.png" medium="image" type="image/png" height="82" width="144"/>
</item>
<item>
  <title>Weekend Reads (4/12/24)</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-12-weekend-reads/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-12-weekend-reads/image.png" class="img-fluid"></p>
<section id="articles" class="level2">
<h2 class="anchored" data-anchor-id="articles">Articles</h2>
<ul>
<li><a href="https://tim.blog/wp-content/uploads/2020/01/17-Questions-That-Changed-My-Life.pdf">17 Important Questions</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Time Series First Principles: Garbage In, Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">Time Series First Principles: The Future Is Similar To The Past</a></li>
</ul>
</section>
<section id="videos" class="level2">
<h2 class="anchored" data-anchor-id="videos">Videos</h2>
<ul>
<li><a href="https://youtu.be/Z2BnqYArwaw?si=tfQXzl1Zu4Zfom9R">Telling Good Stories</a></li>
<li><a href="https://youtu.be/_ZJpU43NA0c?si=-kIn8vjU9-PYWaE2">Power of Being a Contrarian</a></li>
<li><a href="https://youtu.be/sgVDljNavSc?si=YnSlsK_JgUqlv6Lp">Making Friends as an Adult</a></li>
</ul>
</section>
<section id="podcasts" class="level2">
<h2 class="anchored" data-anchor-id="podcasts">Podcasts</h2>
<ul>
<li><a href="https://open.spotify.com/episode/1Ab1UcjpAvbhAhUslDt0kA?si=oGRuirSFQaWt1G6z_gawaw">Protocols to Improve Your Sleep, Huberman Lab</a></li>
</ul>
</section>
<section id="tweets" class="level2">
<h2 class="anchored" data-anchor-id="tweets">Tweets</h2>
<ul>
<li><a href="https://x.com/jasonfried/status/1775918262259536230">Power of Motivation</a></li>
<li><a href="https://x.com/JamesLucasIT/status/1777026561947963709">Our Planet Rocks</a></li>
<li><a href="https://x.com/DeepLearningAI/status/1777345409007952098">Shipping on Fridays</a></li>
<li><a href="https://x.com/Alkibiades_/status/1777418763316466080">Cool Idea Around Building the Pyramids</a></li>
</ul>
</section>
<section id="books" class="level2">
<h2 class="anchored" data-anchor-id="books">Books</h2>
<ul>
<li><a href="https://www.amazon.com/Sapiens-Humankind-Yuval-Noah-Harari-ebook/dp/B00ICN066A/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;dib_tag=se&amp;dib=eyJ2IjoiMSJ9.04mm2YQe9BvKUhAU-tuaKKTp_ekz9KX-YILKoI3_bfIEBt49_HpfeyFUJJlvIwSWAIPWrL_6wT9ArXYinHBdd26BvpLuJiO8L-PDmeH6Bp72MBNaBM3BIkDJczpjw__nnhyQBkyPBqqAxj5-FZHbC_wDc7NP_EpLiA253DUGUSSl1faJjZ8ThMS_HxUzPgC0WHQZRl5HkHwYJMTQgcFc-mGrBXCLrlR63IS6_1cyxs4.80oPRRInfnlKdy_MYM4yAdkI9w0YEtNugcGm04aOaDM&amp;qid=1712934638&amp;sr=8-1">Sapiens by Yuval Noah Harari</a></li>
</ul>


</section>

 ]]></description>
  <category>weekend-reads</category>
  <guid>https://mftokic.github.io/posts/2024-04-12-weekend-reads/</guid>
  <pubDate>Fri, 12 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-12-weekend-reads/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: The Future Is Similar To The Past</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-11-time-series-past-future/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the third principle of a good time series forecast, the future is similar to the past. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/"><strong>The Future Is Similar To The Past</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="here-comes-the-sun" class="level3">
<h3 class="anchored" data-anchor-id="here-comes-the-sun">Here Comes The Sun</h3>
<p>For whatever you‚Äôre trying to forecast, it will be a lot easier to do with with machine learning (ML) if the future is similar to the past. It‚Äôs as simple as that.</p>
<p>When you open the weather app on your phone, have you ever looked at when the sun is expected to rise and set? If you‚Äôre on the new human optimization craze about getting morning sunlight, you most likely have. That forecast is down to the minute, most likely even second, and has a high degree of accuracy. Is the forecast accurate because of expert human judgement, or the type of weather related <a href="https://vitalflux.com/what-are-features-in-machine-learning/">features</a> fed into a ML model? Nope. It‚Äôs accurate because the sun has risen and set at relatively the same time, based on time of year, for millions of years. We don‚Äôt expect future sun rises and sun sets to change that much going forward, that‚Äôs why your weather app gives you an exact time for the sun rise but gives you only a percent probability of rain. Even then, that chance of rain may not even be accurate. It‚Äôs almost a joke now how many times in Seattle I‚Äôve seen a dry forecast only to step out of my house and have it immediately start raining. At least I know the exact minute when the sun will set that day.</p>
</section>
<section id="handling-a-changing-future" class="level3">
<h3 class="anchored" data-anchor-id="handling-a-changing-future">Handling A Changing Future</h3>
<p>Your business is most likely not like the sun. It‚Äôs constantly changing, reacting to market forces and industry competitors. The best way to teach a model about your expectations of the future is to give it data about the past and future.</p>
<p>Let‚Äôs use an example of a monthly revenue forecast for a product. If you only use historical sales data to forecast the product, then you are making the assumption that the future of the product will be almost identical to the past, especially the most recent past. For some established products in mature industries this could be totally fine, but often this is not the case.</p>
<p>One thing to try is adding features that can explain how outside forces impact the product. For example, how much money consumers have to spend might greatly impact who buys your product. So using an economic feature like consumer sentiment can help a model adjust it‚Äôs predictions based on changes in consumer spending habits.</p>
<p>We can add features into our data in two ways. The first is to just give historical values of that feature. This will force us to only use historical lags of the feature when training a ML model, since we don‚Äôt know what the future value of that feature will be. We can take that original feature and create new features (this is called feature engineering). Ones that are a 3 month lag, 6 month lag, or 12 month lag of the original data. Often macro data like consumer sentiment can be a lagging indicator. Meaning their impact is delayed and takes a while to flow through the economy. Changes in consumer sentiment from 6 months ago can actually have a strong correlation with how customers purchase our product today.</p>
<p>A second way is to use both historical values and future values. We could use a future forecast of consumer sentiment in our model, in addition to using the historical data. That way a model can learn from any lagged relationships as well as understand how changes in consumer sentiment impact our product in real time. These future values can either come from an expert forecast (like from famous economists) or created by your own ML solution.</p>
</section>
<section id="the-future-must-always-learn-from-the-past" class="level3">
<h3 class="anchored" data-anchor-id="the-future-must-always-learn-from-the-past">The Future Must Always Learn From The Past</h3>
<p>You might have a ton of ideas for new kinds of future information your can encode as features to train a model. In order to use this data we need to make sure there are historical examples for a model to learn from. The upcoming 2024 presidential election in the US could have a large impact on your business, which will impact your future forecast for the rest of 2024. We know exactly when the election is going to happen, so it‚Äôs easy to give that information into a ML model to learn from.</p>
<p>The catch is we need to make sure that we show examples from the past to allow the model to learn how previous elections impact our business and how the model should handle similar events in the future. If we only have product sales data from the last three years, then we cannot feed it future election data because we don‚Äôt have the data from the 2020 or 2016 US presidential elections.</p>
<p>If we know something is going to happen in the future, but we can‚Äôt quantify it with historical data for a model, then we need to go old school. Instead we need to use our domain expertise about the business to take the ML output, without knowledge of the future event, and make a manual adjustment to the forecast based on the expected impact of the future. For the election example, maybe your product sales will grow as we get closer to the election, so if you don‚Äôt have enough historical data for a model to learn about the election‚Äôs impact you can make a manual adjustment to the ML forecast based on your assumption about the election‚Äôs impact.</p>
<p>This kind of hybrid approach, ML first with a light human touch second, can create a powerful combination. A ML model can do 80-90% of the initial work and a human can make the final manual adjustments based on their domain expertise. This allows a human to add more insight into a forecast that is not easily quantifiable for a ML model to learn from.</p>
</section>
<section id="new-time-series" class="level3">
<h3 class="anchored" data-anchor-id="new-time-series">New Time Series</h3>
<p>A new product at your company might be exciting, but is harder to forecast accurately with ML models. The lack of historical data will make it hard for any new ML model to learn from. Initial trends and seasonality may not always carry into the future. For example there might be a big spike in initial sales around release but then taper off over time. The new product may not even be on sale yet, so you are now tasked with forecasting something with zero historical data.</p>
<p>If the time series in question has some historical data, ideally more than one year of historical observations, a good way to deal with it is to train a ML model with the new time series alongside similar existing products with a lot of historical data. This is sometimes called a ‚Äúglobal model‚Äù, where a model learns from multiple time series instead of one. Training on one specific time series is sometimes called a ‚Äúlocal model‚Äù. Training a global model allows the ML model to learn general trends and seasonality patters across similar time series and apply it to the newer time series. This can work well if the other time series are similar to one another.</p>
<p>If the product you want to forecast has no historical data, then you are in a tough boat. Traditional time series methods cannot help you, since they all rely on quality historical data. What you can do is take a more traditional machine learning regression approach. This involves taking all historical products that have launched over time and training a model to understand the initial demand of a new product and how it either grows or shrinks over time. For example with iPhone sales, you can train a model on the initial sales of each iPhone model from V1 to V14, then use that model to predict the kind of demand the latest V15 iPhone might have. This type of approach would need a more detailed post to explain fully, but hopefully you get the broader picture.</p>
<p>To learn more about forecasting new time series, check out the <a href="https://otexts.com/fpp3/judgmental.html">forecasting bible</a> written by our forecasting godfather Rob Hyndman. The chapter on judgemental forecasts goes deep into forecasting new products and discusses other approaches you can take.</p>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>When using future values of a feature, there is a risk of compounding errors. Let‚Äôs go back to the consumer sentiment example. If you create your own expectation of future consumer sentiment, or use an economist‚Äôs prediction, there is a good chance the forecast will be wrong. If the forecast about consumer sentiment is wrong, and that forecast is fed into a model to predict your product‚Äôs sales, then your sales forecast will be even more wrong. The errors compound. Add in other features and you can see how the house of cards can tumble pretty fast. Always be weary about using future values of features where you don‚Äôt have 100% confidence in their future value. For example using future holiday features are great because they will always happen on a specific day with 100% certainty, but trying to tell a model where inflation is headed in the future can get you in trouble.</p>
<p>Having a human make manual adjustments after the initial ML forecast can add unneeded human bias to the final forecast. This bias can sometimes be wrong and hurt the accuracy of the forecast. It‚Äôs good practice to capture these adjustments and always report on the forecast accuracy of the pure ML model and the accuracy for the model + human adjustments. That way you can track how helpful the manual adjustments are, and remember why they were made in the first place.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>When embarking on the journey of time series forecasting, remember it‚Äôs more art than exact science‚Äîakin to predicting rain in Seattle. The key takeaway? Use the past as a guide but sprinkle in educated guesses about the future with caution. Whether you‚Äôre launching new products or navigating established markets, blending machine learning with a dash of human intuition can create robust forecasts. May your forecasts be as reliable as the sunrise, with just enough flexibility to handle an unexpected downpour.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-04-11-time-series-past-future/</guid>
  <pubDate>Thu, 11 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Garbage In, Garbage Out</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-08-time-series-garbage/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the second principle of a good time series forecast, garbage in garbage out. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/"><strong>Garbage In Garbage Out</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="youre-not-a-wizard-harry" class="level3">
<h3 class="anchored" data-anchor-id="youre-not-a-wizard-harry">You‚Äôre Not a Wizard, Harry</h3>
<p>A common I see people in finance make when trying to use machine learning (ML) is around approaching it like a magic wand. Thinking as long as they bring in some data and throw it over the fence to a ML process, a perfect forecast will come back to them. All shiny and clean. ML should be able to find all of the patterns in the data and do things we humans can‚Äôt fathom right? No, wrong. ML is not a cure all thing. Having a good ML forecast starts with having quality historical data for a model to learn from. Without good data, you won‚Äôt get a good forecast. It‚Äôs as simple as that. Let‚Äôs dive into ways data can be stinky and how we can sanitize it before training models. To help illustrate each point we‚Äôll use an example of a monthly sales forecast.</p>
</section>
<section id="amount-of-historical-data" class="level3">
<h3 class="anchored" data-anchor-id="amount-of-historical-data">Amount of Historical Data</h3>
<p>Ideally you want to get as much historical data as possible. If we want to forecast the next quarter of sales, it‚Äôs a bad idea to only use the last 12 months of historical data to train a model. Usually I try to get 5+ years of historical data before training any model. This allows for enough year over year observations for a model to learn from. For monthly forecasts, I won‚Äôt even start a forecast project if there is less than 3 years of historical data.</p>
<p>Having sufficient historical data creates the opportunity to have sufficient model back testing. Where we can see how models performed over the last few months of the data. We can then use that back testing accuracy as a proxy for what to expect for the future forecast. The less historical data you have, the less you can back test.</p>
<p>The more data you have, the longer you can forecast into the future. If I only have 3 years of historical data, it‚Äôs a bad idea to try and forecast the next 2 years. A good heuristic is to cap your forecast horizon (periods you want to forecast out) to less than 50% of the historical data. So if you want to forecast out the next 3 years of monthly sales, you need at least 6 years of historical data. When in doubt always get more data.</p>
<p>What if I have tons of historical monthly sales data, but a cool new feature I want to use around sales pipeline is only available for the last 2 years? Most of the time, stick with using more historical data even if it‚Äôs at the expense of using highly correlated features but less historical data. Feel free to try both approaches, but often the one with the most historical data wins.</p>
</section>
<section id="trend-and-seasonality" class="level3">
<h3 class="anchored" data-anchor-id="trend-and-seasonality">Trend and Seasonality</h3>
<p>Most time series can be broken into three pieces. First is trend, is your data going up or down over time. Second is seasonality, are there peaks and valleys in your data that happen at the same time each year. Finally is the ‚Äúerror‚Äù or ‚Äúresidual‚Äù component, which is anything left over after accounting for trend and seasonality. Think of it as noise in your data. This approach of breaking down a time series into separate pieces is called time series decomposition.</p>
<p>Having recurring trends and seasonality in your historical data make things 100% easier to forecast in the future. If your data has trends that change month to month and seasonal patterns that evolve over time, your data is basically all noise. A noisy dataset is a bad dataset, one that can‚Äôt be modelled effectively by any ML model. Take a look at the below time series, each broken out by trend, seasonality, and residual. Now tell me which one would be easier to train a model on? Which would produce a high quality future forecast?</p>
<p><img src="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/chart1.png" class="img-fluid"></p>
<p>You can deal with noisy data like this in a few ways. The first is to just change the grain of the data. For example, if this was forecasting a specific product SKU, maybe instead sum it up to a higher level like product category. That way more stable trends and seasonality might appear. You can also try to add features (variables to your training data) to try to teach a model why the seasonality and trends in the data are messy. For example the main COVID years from 2020-2022 really throw a wrench in any trends or seasonal patterns in most data sets. So adding information to a data set that tells a model that there was a special one-off situation for specific periods can help a model learn the right kinds of relationships in the data and generalize well to new unseen data going forward.</p>
</section>
<section id="missing-data" class="level3">
<h3 class="anchored" data-anchor-id="missing-data">Missing Data</h3>
<p>Missing data is the silent killer in forecasting. If you don‚Äôt specifically look for it you might never know it‚Äôs the reason your forecasts perform poorly. Missing data is important because many models (either statistical models or ML models) often need all sequential date observations of the historical data to train a model. Even one period of missing data can throw off an entire model and lead to poor performance.</p>
<p>Often times financial systems will not have tons of missing data. It‚Äôs important to know if the data that is missing should mean treated as actually missing or seen as a true zero value. For example, if we have product sales missing for a specific month, should we classify that value as truly missing or just hardcode that value to zero? Make sure you clarify that with whoever owns the data.</p>
<p>If the missing should be zero then that‚Äôs a quick fix, but if it‚Äôs truly missing then you now have another problem on your hands around what to do. Simply replacing the missing value with zero can throw off any trend or seasonality patters like we discussed earlier. Common ML advice is to replace missing feature data with the median or mean value of that feature, but this is terrible advice for time series forecasting. Usually the best approach is to use some sort of simple statistical model that can understand the trends and patterns of data around the missing value and impute what the value should be. This will keep existing trends and seasonality patterns in the data, meaning your future forecast will be more robust.</p>
</section>
<section id="outliers" class="level3">
<h3 class="anchored" data-anchor-id="outliers">Outliers</h3>
<p>An outlier in time series forecasting is an atypical data point that significantly deviates from the overall pattern of the data. They can occur multiple times in a historical time series or just be a one off for a particular period. Either way, their presence can greatly impact how a model learns from the data.</p>
<p>Outlier detection in time series forecasting often involves statistical methods, anomaly detection algorithms, or visual inspection to identify data points that significantly deviate from the typical patterns of the series. Techniques include setting thresholds based on standard deviations, using moving averages to smooth the series and highlight anomalies, applying machine learning models like isolation forests, or utilizing robust decomposition methods (like STL) to separate the series into components and identify outliers in the residuals.</p>
<p>Take a look at the chart below. See how just one large value towards the end of the time series completely changes the trend and seasonality. A model might take this data and produce a huge forecast going forward, since the trend changed drastically based on the outlier. It might also have a huge spike for that specific period next year, since it learned that seasonality recently changed.</p>
<p><img src="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/chart2.png" class="img-fluid"></p>
<p>There are a few ways we can handle the presence of outliers. First we can leave it alone, and let it‚Äôs presence impact our future forecast. Maybe after talking with the business domain expert they say that there is a foundational change in the business (new product launch, tax change) that means we expect to see similar values in the future. Second we can add some more information to our data to explain what happened in that period and if we expect it to happen again in the future. If the outlier was caused by a new product launch, we can label that as a feature in the data and also tell the model if we expect any product launches in the future. A model will then learn of these one off patterns and adjust the forecast as needed. The final method is to remove the outlier altogether. Once removed, we can treat it like a missing value and replace it with a value more in line with recent trends and seasonality. If it‚Äôs truly a one off thing that will never happen again then removing it is sometimes the best approach. The choice you make always depends on the context of what caused the outlier and how we expect similar things to happen going forward.</p>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>Sometimes having more historical data is a bad thing, and can hurt model performance. For example having 30 years of historical data could produce a good forecast, but do the trends and patterns of the business 20 years ago still apply to the business today? Often in fast changing industries this is not the case, so sometimes deliberately shortening your data is the right idea. Five years from now we may want to exclude data from pre-2023 to remove all impacts of COVID. How your customers purchased your services in 2019 is most likely very different than how they will buy them in 2024. Gee, thanks COVID.</p>
</section>
<section id="automatic-data-cleaning-with-finnts" class="level3">
<h3 class="anchored" data-anchor-id="automatic-data-cleaning-with-finnts">Automatic Data Cleaning with finnts</h3>
<p>Thankfully there is a solution to most of these problems. My package, <a href="https://microsoft.github.io/finnts/index.html">finnts</a>, helps solve a lot of the data sanitizing needed to produce a high quality forecast. It can handle outliers and missing values automatically for you. It abstracts away all of these hard topics and makes it easy to get up and running with a forecast in one line of code. Check it out.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Messy data will always lead to a messy forecast. ML models can‚Äôt save you from bad data. There‚Äôs no magic wand to cure common data problems. What you can do though is make sure your data has solid historical trends/seasonality, no missing data, and good approach to handling outliers. With these taken care of, you‚Äôre own your way to building a high quality forecast.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-04-08-time-series-garbage/</guid>
  <pubDate>Mon, 08 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Weekend Reads (4/5/24)</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-05-weekend-reads/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-05-weekend-reads/image.png" class="img-fluid"></p>
<section id="videos" class="level2">
<h2 class="anchored" data-anchor-id="videos">Videos</h2>
<ul>
<li><a href="https://youtu.be/_Y-7liNT1Ok?si=VDIlw5FTQXCu2kYm">Improving Focus</a></li>
<li><a href="https://youtu.be/ha1ZbJIW1f8?si=2D0Gp15NkD9iup01">Dopamine and Motivation</a></li>
<li><a href="https://youtu.be/gR_f-iwUGY4?si=JHewqUkP7dautU1v">Optimal Morning Routine</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLOhHNjZItNnOoPxOF3dmq30UxYqFuxXKn">Sequoia AI Summit</a></li>
<li><a href="https://youtu.be/4t4YkHSTZbw?si=qqjiCcKg4_YmotoA">SBF, Gov Spending, and More on All In</a></li>
</ul>
</section>
<section id="podcasts" class="level2">
<h2 class="anchored" data-anchor-id="podcasts">Podcasts</h2>
<ul>
<li><a href="https://open.spotify.com/episode/4iqmP3Gys5AkyLpcXNU26N?si=zGiCV0c0Ts-8baaJ2j2k8g">Scott Glenn on Tim Ferriss</a></li>
<li><a href="https://open.spotify.com/episode/7ExrANvf3Rhha46GRX5W8X?si=ON6IJbN-QT-SDo6a7fOjMQ">Seth Godin on Tim Ferriss</a></li>
<li><a href="https://open.spotify.com/episode/4tnRkV3qL3GulapNmzirsy?si=3NYI2OFhSVWpFevtfxvK6Q">Dr.&nbsp;Rhonda Patrick on The Knowledge Project</a></li>
</ul>
</section>
<section id="tweets" class="level2">
<h2 class="anchored" data-anchor-id="tweets">Tweets</h2>
<ul>
<li><a href="https://x.com/gdb/status/1772334028009652415">Sora Goodness</a></li>
<li><a href="https://x.com/EMostaque/status/1772594194315436266">Satya Nadella Stays Undefeated</a></li>
<li><a href="https://x.com/thegarrettscott/status/1771645169151901952">Bezos Leadership Principles</a></li>
</ul>
</section>
<section id="books" class="level2">
<h2 class="anchored" data-anchor-id="books">Books</h2>
<ul>
<li><a href="https://ravinkumar.com/GenAiGuidebook/book_intro.html">GenAI Guidebook</a></li>
</ul>


</section>

 ]]></description>
  <category>weekend-reads</category>
  <guid>https://mftokic.github.io/posts/2024-04-05-weekend-reads/</guid>
  <pubDate>Fri, 05 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-05-weekend-reads/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Domain Expertise</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the first principle of a good time series forecast, domain expertise. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/"><strong>Domain Expertise</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>Any data scientist worth their salt can create a time series forecast for you. They can pull some data, train some machine learning (ML) models, and give you a forecast. All with you out of the loop. If that‚Äôs the case at your company, run! This is a big red flag. While that can sometimes yield good results, often the most important ingredient is missing, which is strong domain expertise about what you‚Äôre trying to forecast. This is where strong understanding of the business and market forces come into play. You know, the stuff that finance people excel at. Pairing robust ML models with strong domain expertise about the area being forecasted always yields the most accurate forecast. It also increases trust in that forecast, since the humans using that forecast know the model took into account important factors that influence the business. In this post we‚Äôll use a hypothetical example of a company‚Äôs real estate spending to showcase the importance of domain expertise.</p>
</section>
<section id="translating-domain-expertise-into-features" class="level3">
<h3 class="anchored" data-anchor-id="translating-domain-expertise-into-features">Translating Domain Expertise Into Features</h3>
<p>How does domain expertise change how a ML model is created? This can manifest in many forms. The most common is changing the kind of data used in training a model. Variables that a model learns from are called ‚Äúfeatures‚Äù. Let‚Äôs apply this to our real estate spend forecast example. In the last few years, COVID and the work from home revolution have changed how people come into work. This changes how many people drink coffee, use the copier, and even which buildings stay in operation for a company. Simply pulling historical building expense data and training a model could get you ok results, but to get to peak performance you need domain expertise around what actually moves the needle for building expenses. Example features could be the square footage of a building, how many people actually badge into that building each month, even the periods where COVID was at its worse and a work from home mandate was in effect. All of these things are custom knowledge, most likely kept inside the heads of the finance workers who oversee the real estate space within a finance org.</p>
</section>
<section id="iteration-is-key" class="level3">
<h3 class="anchored" data-anchor-id="iteration-is-key">Iteration is Key</h3>
<p>Throwing all of your ideas as features into a model from the start is usually not a good idea. Instead having multiple rounds of iteration is key. In the real estate example, it‚Äôs best to start out with no external features. Just use historical spend to forecast future spend. Starting with this simpler approach can sometimes get you 90% of the accuracy you need, maybe even 100% if there are stable trends and seasonality that carries into the future. Run this first to see what the initial accuracy is, and if it doesn‚Äôt meet your requirements that when we can refine by adding new data.</p>
<p>Once you have the baseline, you can look deeper into the accuracy results to see where the forecast is performing poorly. This is where domain knowledge kicks in. Poor initial forecast performance can be fixed by asking the domain expert if there is a difference between what the model knows and what a human knows. If there is a gap, can that be quantified as data to teach a model? This kind of insight can be added into a model with easy to find numeric data, or even as binary yes or no values (1 or 0) to denote when a specific one off event happened. This iterative process is where the magic happens.</p>
<p>For the real estate forecast, maybe there was a period where expenses jumped sharply in one month and stayed at that new level for the rest of the year. This will be hard for a ML model to understand or even anticipate, but the domain expert of the real estate space knows that in that specific month there were two new building openings. So the expenses of course jumped up a significant degree and stayed like that going forward. Knowing this, we can get historical square footage information and add it into our model. We can even incorporate future buildings that might be removed or added going forward. This will help a model understand how changes in total buildings impact spend.</p>
<p>So we added total square footage to our model and the results improved compared to our initial baseline of no external features. But it didn‚Äôt move the needle that much. Even though our company might be adding more buildings, in recent years the spend may not have a perfect correlation with added square footage. Knowing this, the domain expert recommends using anonymous badge in data to see who is actually coming into work. Pre-covid this data may not have been useful, since most buildings were always at max capacity with everyone coming to work each day. Now in a post-covid world this has changed forever. Some teams might only be in their assigned building 2-3 days a week. Or maybe they never returned in person, deciding instead to buy ranches in Wyoming with fast WiFi. Combining the square footage and badge in data into the model yielded fantastic results, much better than the initial baseline.</p>
<p>After reviewing the improved results with the domain expert, the future forecast still seems a little low compared to the domain experts expectations. The domain expert has one last idea, trying to teach the model how COVID impacted spending. This can be quantified as a binary variable, where in all rows of the data we add a 1 if COVID was impacting the world, and 0 when it wasn‚Äôt. This means from early 2020 - early 2022 we have values of 1 and every period before and after we give a value of 0. A model can now understand that what happened over those two years was mostly a one off situation that is not expected going forward. After the ML model is trained with this new insight the back testing now looks great and the future forecast matches the expectations of the domain expert.</p>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>Getting high quality data to use as features in a model is always a good idea. There are times though where the amount of historical feature data might be lacking. For example, we may not be able to get more than 3 years of historical square footage data for our real estate expense forecast, even though we can get 5 years of historical spend data. What should we do? We can shorten the historical spend data to the last 3 years to match the square footage data, but having less data can sometimes degrade model performance. So in some cases choosing to use the full 5 years of historical spend without the square footage data is the best approach that yields the best accuracy.</p>
<p>When facing this dilemma, try both approaches and see how accuracy is effected. I‚Äôve seen many times that using more historical data of what you‚Äôre trying to forecast is often more accurate than shortening that data to combine it with external features.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Starting any ML process without a business domain expert in the room is always a bad mistake. They are the cheat code in the video game that gets you to level 20 in half the time. Involving them early and often while also adopting a quick iteration approach can create a world class forecast that is trusted by the ultimate end users, which often are the domain experts themselves. At the end of the day most ML forecasts come down to trust by the end user. That‚Äôs why domain expertise is the first principle in building quality time series forecasts.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/</guid>
  <pubDate>Tue, 02 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Thoughts on First Principles in Time Series Forecasting</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/image.png" class="img-fluid"></p>
<p>I‚Äôve been doing time series forecasting with machine learning (ML) most of my career. I believe it‚Äôs still the best AI opportunity in corporate finance, even with all of the latest Generative AI developments in recent years. If you work for the CFO, chances are you often create predictions about the future. Those predictions take time and can always be more accurate. Machine learning can help in both areas. Before you build machine learning solutions in your finance org, it‚Äôs important to understand the true building blocks of making good forecasts.</p>
<p>In this post I will overview each first principle, and have follow-up posts digging deeper into each one. Let‚Äôs dive in.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/"><strong>Domain Expertise</strong></a>: Knowing what factors actually influence what you are trying to forecast is more important than which ML model to train.</li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/"><strong>Garbage In Garbage Out</strong></a>: Training a model on bad data leads to bad forecasts.</li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/"><strong>The Future Is Similar To The Past</strong></a>: If you expect the future to be drastically different than past data, you will have a hard time training accurate models.<br>
</li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/"><strong>Higher Grain Higher Accuracy</strong></a>: Forecasting by country is often more accurate than forecasting by city. Forecasting by month is often more accurate than forecasting by day.</li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/"><strong>Order Is Important</strong></a>: When time is involved, how your data is ordered makes all the difference.</li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/"><strong>The Magic Is In The Feature Engineering</strong></a>: How you transform your data before model training can transform a mediocre forecast into a world class forecast.</li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/"><strong>Simple Models Are Better Models</strong></a>: Like occam‚Äôs razor, the best model is often the one with the least amount of inputs.</li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/"><strong>Capture Uncertainty</strong></a>: Showing the back testing results and future uncertainty of a model‚Äôs forecast builds more trust.</li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/"><strong>Model Combinations Are King</strong></a>: Usually a combination of multiple models is more accurate than just one model‚Äôs prediction.</li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/"><strong>Deep Learning Last</strong></a>: Deep learning isn‚Äôt as effective as more traditional ML models.</li>
</ol>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>This is not an exhaustive list, but instead principles that I find particularly important when creating a time series forecast. Having a firm understanding of these principles is enough to get the ball rolling on any type of forecast you‚Äôre working on. Thankfully, the very same approach I use in my job to do forecasting is open source and freely available through my R forecasting package called <a href="https://microsoft.github.io/finnts/index.html">finnts</a>. Even if you‚Äôve never done data science or used R before, finnts makes it easy to get off the ground fast without shooting yourself in the foot when dealing with the above principles. Stay tuned for more posts about each principle.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/</guid>
  <pubDate>Tue, 26 Mar 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Weekend Reads (3/22/24)</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-03-22-weekend-reads/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-03-22-weekend-reads/image.png" class="img-fluid"></p>
<section id="articles" class="level2">
<h2 class="anchored" data-anchor-id="articles">Articles</h2>
<ul>
<li><a href="https://world.hey.com/dhh/developers-are-on-edge-4dfcf9c1">DHH On Coding AI Agents</a></li>
<li><a href="https://collabfund.com/blog/the-thin-line/">When Confidence Backfires</a></li>
</ul>
</section>
<section id="videos" class="level2">
<h2 class="anchored" data-anchor-id="videos">Videos</h2>
<ul>
<li><a href="https://youtu.be/NdxFctWShtc?si=pFzzxt7EctaREoZY">Sahil Bloom on Building Life Systems</a></li>
<li><a href="https://youtu.be/jvqFAi7vkBc?si=v654_j6USVkIUGNx">Sam Altman on Lex Friedman</a></li>
<li><a href="https://youtu.be/lXLBTBBil2U?si=yEU-YxYkgusDFkEd">Jensen Huang at Stanford GSB</a></li>
<li><a href="https://youtu.be/uMajFsCkzxY?si=fwzKDf-jfCK-ZyKT">TikTok Ban, AI, and more on All In</a></li>
</ul>
</section>
<section id="podcasts" class="level2">
<h2 class="anchored" data-anchor-id="podcasts">Podcasts</h2>
<ul>
<li><a href="https://open.spotify.com/episode/11oI0JELKOohCSEwz8WOQD?si=1648IhapTUOdkaanwRM4Ug">Morgan Housel on Modern Wisdom</a></li>
</ul>
</section>
<section id="tweets" class="level2">
<h2 class="anchored" data-anchor-id="tweets">Tweets</h2>
<ul>
<li><a href="https://x.com/BallsackSports/status/1770999544911626266?s=20">Grateful that Kansas won their first round game</a></li>
<li><a href="https://x.com/george__mack/status/1769376253524496607?s=20">How Momentum Rules Your Life</a></li>
<li><a href="https://x.com/Feynmanism_/status/1753767517762596903?s=20">Cognitive Biases</a></li>
</ul>
</section>
<section id="books" class="level2">
<h2 class="anchored" data-anchor-id="books">Books</h2>
<ul>
<li><a href="https://www.amazon.com/Numbers-Game-Everything-About-Soccer-ebook/dp/B00BPDR3E2/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;dib_tag=se&amp;dib=eyJ2IjoiMSJ9.rmUGJqVG-UDO-EJgjWaUjPm0ug1og2SXZQ8GiC45XXxuQBtP7Zt81fkEWNptjyu6-K4Lk6Dq4pKN867arj5xDiLron2Zl7KN5-fjRRh1w15sNA0UvwnPtIKnfWqyQEGEd475FjsPSiNH_CVWG5956NWPyp4f2j9I5mYgWmxLceICvMI-j9WKDdR5Xn9MwPaRwzIg0N5GNI9U_r2gIstQ0RlZhNonJR2TyxycZexvtKY.7_sZ4hdeMz5-fU0BCzqT3rFyOo0klmjutMl3rZ_qpn4&amp;qid=1711121115&amp;sr=8-4">The Numbers Game by Chris Anderson</a></li>
</ul>


</section>

 ]]></description>
  <category>weekend-reads</category>
  <guid>https://mftokic.github.io/posts/2024-03-22-weekend-reads/</guid>
  <pubDate>Fri, 22 Mar 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-03-22-weekend-reads/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Weekend Reads (3/16/24)</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-03-16-weekend-reads/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-03-16-weekend-reads/image.png" class="img-fluid"></p>
<section id="videos" class="level2">
<h2 class="anchored" data-anchor-id="videos">Videos</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=-gfEjOgxBfI">The Algebra of Happiness</a></li>
</ul>
</section>
<section id="podcasts" class="level2">
<h2 class="anchored" data-anchor-id="podcasts">Podcasts</h2>
<ul>
<li><a href="https://open.spotify.com/episode/6fJYHF4NOQZvoCpaCFt7lX?si=GkalASEQRzq22cxs77TYNw">Chris Davis On The Knowledge Project</a></li>
<li><a href="https://open.spotify.com/episode/7LQzv0OKaE6KWEPPXPWuv7?si=CA0dzU35Q96xKYkIQipI0g">Kimbal Musk On Lex Friedman</a></li>
<li><a href="https://open.spotify.com/episode/5TAmY56aFV2J6Byn0qg94H?si=G2jTl8QFR4-P40FxjXd9jw">Dr.&nbsp;Cal Newport On Huberman Lab</a></li>
</ul>
</section>
<section id="tweets" class="level2">
<h2 class="anchored" data-anchor-id="tweets">Tweets</h2>
<ul>
<li><a href="https://twitter.com/heyitswindy/status/1764023423485812995?s=46&amp;t=8Xa2BngQ9d359SJzFrCFMA">Excellent Product Placement</a></li>
<li><a href="https://twitter.com/cognition_labs/status/1767548763134964000?s=46&amp;t=8Xa2BngQ9d359SJzFrCFMA">Meet Devin, Your AI Dev Intern</a></li>
<li><a href="https://twitter.com/spacex/status/1768267464062943676?s=46&amp;t=8Xa2BngQ9d359SJzFrCFMA">SpaceX Launch</a></li>
<li><a href="https://twitter.com/neilpatel/status/1768645880784314626?s=46&amp;t=8Xa2BngQ9d359SJzFrCFMA">ChatGPT Writing Prompt</a></li>
<li><a href="https://twitter.com/thekriskay/status/1768665799638553068?s=46&amp;t=8Xa2BngQ9d359SJzFrCFMA">Invest For The Long Haul</a></li>
</ul>
</section>
<section id="books" class="level2">
<h2 class="anchored" data-anchor-id="books">Books</h2>
<ul>
<li><a href="https://www.amazon.com/Great-Mental-Models-Thinking-Concepts-ebook/dp/B07P79P8ST/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;dib_tag=se&amp;dib=eyJ2IjoiMSJ9.nqDtFB0egHNRqbrwj7r9Q_VC72or3xvMfmy7E8Mp0Jf450tLXd7icWtUFMWvgbxO9dAyqa7WCJgjoHxY_gFVtNrfRN0jweiog60OU-ohIGe6cOiS0lNSskb-ZSLy3T-YZr62UAI1EL6NaBwfeB68kg4YW61sMl7qJdAYRlVMFg5ymByXAbbwp11EadBruMWEyWfCnPSoVU1CNUpFRnr4WUj_3IAu4Uv2sihpLbmPf1Q.wzJzOHHNv35HioL8ZTZcL1y-Tl7sbyX4KoL6E-Vge04&amp;qid=1710597095&amp;sr=8-1">The Great Mental Models Volume 1 by Shane Parrish</a></li>
</ul>


</section>

 ]]></description>
  <category>weekend-reads</category>
  <guid>https://mftokic.github.io/posts/2024-03-16-weekend-reads/</guid>
  <pubDate>Sat, 16 Mar 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-03-16-weekend-reads/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
</channel>
</rss>
