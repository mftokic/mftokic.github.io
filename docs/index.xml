<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Thoughts on Things</title>
<link>https://mftokic.github.io/</link>
<atom:link href="https://mftokic.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>A collection of thoughts on things from the mind of Mike Tokic</description>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Wed, 25 Sep 2024 07:00:00 GMT</lastBuildDate>
<item>
  <title>Thoughts On Time Series Forecasting Fundamentals</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-09-25-ts-fundamentals/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-09-25-ts-fundamentals/image.png" class="img-fluid"></p>
<p>The ability to create forecasts about the future is a superpower. Machine learning (ML) models take this to another level by increasing forecast accuracy while reducing the time spent creating the forecast. Using ML might seem a little scary at first. You may not know where to start. Type ‚Äúintro ML course‚Äù into Bing and you‚Äôll probably get millions of links. Which ones are good? Why do they cost $5,000? That‚Äôs why I wanted to create a gentle introduction to time series forecasting with ML. Where I start from first principles and work our way up to shipping forecasts in production. The intent is to cover the core theory of ML forecasting, and less on the code itself. The code can come later, but anyone who consumes the outputs from ML or helps train ML models needs a strong understanding of how this process works.</p>
<p>The sequence of what you learn is just as important as what content you learn. I have developed a learning path that takes you from absolute beginner and slowly adds new concepts until you‚Äôre a forecasting master! Please click on each link in order to get up to speed, or skip around to any topic you want to dive into again. Don‚Äôt try to read this all in a day, take your time, take notes, and maybe even paste some of the posts into your favorite AI tool to quiz yourself on the topics. Happy learning!</p>
<section id="our-learning-journey" class="level3">
<h3 class="anchored" data-anchor-id="our-learning-journey">Our Learning Journey</h3>
<ol type="1">
<li>What the heck is a time series?</li>
<li>Exploratory Data Analysis
<ul>
<li>Time Series Decomposition</li>
<li>Autocorrelation</li>
<li>Missing Values, Outliers</li>
<li>External Regressors</li>
</ul></li>
<li>Data Cleaning
<ul>
<li>Missing Values</li>
<li>Outliers</li>
<li>Box Cox Transformation</li>
<li>Stationary</li>
</ul></li>
<li>Univariate Models
<ul>
<li>ARIMA</li>
<li>Exponential Smoothing</li>
<li>Simple benchmarks</li>
</ul></li>
<li>Evaluation Metrics</li>
<li>Feature Engineering
<ul>
<li>Date</li>
<li>Target Variable</li>
<li>External Regressors</li>
</ul></li>
<li>Multivariate Models
<ul>
<li>Local Models</li>
<li>Global Models</li>
<li>Hyperparameter Tuning</li>
<li>Linear Regression</li>
<li>Decision Trees</li>
<li>Random Forest</li>
<li>Gradient Boosting (XGBoost, LightGBM)</li>
<li>Feature Selection</li>
<li>Mutlistep Horizon, Autoregressive</li>
</ul></li>
<li>Model Training Lifecycle
<ul>
<li>Train/Test Splits</li>
<li>Time Series Cross Validation</li>
<li>Evaluation Metrics</li>
</ul></li>
<li>Hierarchical Forecasting
<ul>
<li>Standard Hierarchy</li>
<li>Grouped Hierarchy</li>
</ul></li>
<li>Combining Models
<ul>
<li>Simple Averages</li>
<li>Weighted Averaged</li>
<li>Ensemble Models</li>
</ul></li>
<li>Prediction Intervals</li>
<li>Model Interpretability
<ul>
<li>Model Specific</li>
<li>Model Agnostic
<ul>
<li>Global Methods</li>
<li>Local Methods</li>
</ul></li>
</ul></li>
<li>Forecasts in Production
<ul>
<li>Parallel Computing</li>
<li>Model Training and Serving</li>
</ul></li>
</ol>


</section>

 ]]></description>
  <category>machine-learning</category>
  <category>time-series</category>
  <guid>https://mftokic.github.io/posts/2024-09-25-ts-fundamentals/</guid>
  <pubDate>Wed, 25 Sep 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-09-25-ts-fundamentals/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>My Path from Finance Intern to Senior Software Engineer at Microsoft</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-09-04-sde-career-path/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-09-04-sde-career-path/image.png" class="img-fluid"></p>
<p>I write code every day as a senior software engineer at Microsoft. But I didn‚Äôt use to. When I came to Microsoft as an intern in summer 2015, I was just your ordinary finance intern. The only code I knew was excel formulas. During the summer I realized the power of data+code and made it my mission to figure it out.</p>
<p>I have previously written about my journey in <a href="https://mftokic.github.io/posts/2024-02-19-frp-journey/">Microsoft‚Äôs Finance Rotation Program (FRP)</a>. That details the first half of my career and how I got my first engineering job, but not what I do today as a senior software engineer. Most of my time is spent on building and operating machine learning systems used by others across the finance organization. It‚Äôs been a hard journey, but also a fun one. If you‚Äôre like me and are interested in making the switch, here are some pieces of advice that might help you. Like all advice, take it with a grain of salt. Advice is like getting last years winning lottery numbers. They might have worked for that person in the past, but it may not work for you. Anyways I hope you find it helpful.</p>
<section id="focus-on-slope-over-intercept" class="level3">
<h3 class="anchored" data-anchor-id="focus-on-slope-over-intercept">Focus on Slope over Intercept</h3>
<p>When I came back as a full time employee after my internship, I kept seeing these new signs everywhere talking about something called a <a href="https://hbr.org/2016/01/what-having-a-growth-mindset-actually-means">‚Äúgrowth mindset‚Äù</a>. During my internship these words were never mentioned. Now they are taped to every blank wall on campus. What‚Äôs going on? Microsoft‚Äôs CEO, Satya Nadella, was in the midst of working on a culture shift at the company. Trying to go from employees that were ‚Äúknow it alls‚Äù to a more improved ‚Äúlearn it all‚Äù. This new attitude was perfect for me because I knew jack squat about coding, and I wanted to learn.</p>
<p>If you want to pivot your career into software engineering, there are two ways to go about it. The first and most common is to go to school and get a degree. The second is to figure it out on your own, aka the learn it yourself path. Both paths have their pros and cons. Going to school is the more direct path, but self-learning may have a different kind of benefit. If you‚Äôve only ever learned how to code on your own, then you are setting yourself up for lifelong learning. As opposed to someone who went to school, who is used to classroom learning. The learning never stops once you start coding for money, so having the skill to learn proactively and not on a predetermined schedule is a super power.</p>
<p>Having technical knowledge from school puts you ahead of someone else who is just starting to learn on their own. But if that self-taught person is learning at a pace higher than the school smart person, eventually their trajectories will cross and the self-taught coder will surpass them. Think of it like a graph. Where school knowledge gives you a high Y intercept, a good starting point, but the rate of your learning every day after is the slope. The most important thing is to focus on the slope over the intercept.</p>
<p><img src="https://mftokic.github.io/posts/2024-09-04-sde-career-path/image2.png" class="img-fluid"></p>
<p>The learning never stops. In fact it only get‚Äôs harder. Not the learning itself (maybe it does), but taking the time to learn. Build the habit now, stay curious. The biggest trap is getting into the mindset of ‚ÄúI‚Äôll start learning new things again once current xyz thing cools down‚Äù. That thing might be a new job, getting married, having a kid. As your responsibilities grow in life, your free time shrinks in proportion. Make sure learning stays a priority.</p>
<p>A good way to learn on your own while having an existing job is to make the learning part of your job. If you want to learn more about AI technology, go tell your boss that you want to work on a project with AI that could help the team. I‚Äôm 100% positive you‚Äôre boss will say yes. Getting free engineering help is usually not turned down by anyone. We all need better software in our lives. Once your boss expects you to deliver on something, with a deadline, then that‚Äôs the healthy pressure needed to go out and learn the damn thing. In addition to signing up for project work you can also do the same for traditional learning through books, courses, and certifications. For me back in the day I told my boss I was going to finish a Microsoft sponsored course in data science by xyz date. Each month I would provide her updates on my progress. Finishing that course become one of my deliverables in my job. That allowed me to spend some time each day at work on learning, without feeling guilty because it was actually part of my job to learn. Make it part of your job and things become a lot easier. Compared to squeezing in a few minutes before or after work without telling anyone. If your upcoming performance review is tied to reading a few books and building a new thing for your team, you will have all the motivation you need to get it done.</p>
<p>If you can‚Äôt find any good projects on your team, look elsewhere. You could go help another team at your company that might need some coding help, or a local charity or organization you care about. Towards the end of my time in the FRP, I stumbled across a team at Microsoft called the <a href="https://www.microsoft.com/en-us/corporate-responsibility/airband-initiative?msockid=0aa1dafa17d06b740264ce5816656a0b">Airband Initiative</a>. This team is focused on growing internet access to communities all around the world. They needed some help crunching data from policy documents and with zero experience I was able to come join the team as a temporary resource to help them with this analytics project. I was free labor so they didn‚Äôt have anything to lose, but I had everything to gain. I got good experience with natural language processing and working with unstructured data. Eventually the project grew. I ended up presenting the whole thing to Microsoft‚Äôs chief data scientist and even got to showcase the work at the 2019 Internet Governance Forum in Berlin. Where all my expenses were paid. All because I spoke up and offered to help. The Airband team got help, I learned a ton, and become an international data traveler. Talk about a win-win-win.</p>
<p>Maybe you‚Äôre still in school, but want to pick up some coding skills. Go see if any student organizations need help analyzing data or updating their website. Maybe the Football team could use another hand at analyzing what plays work best on third down, or if the basketball team needs to play a smaller lineup to score more points. The coffee shop in your business school might need some help on seeing what kind of sales promotions bring in the most revenue. The opportunities are endless. And because you‚Äôre offering your services for free they will not turn you down.</p>
<p>Last thing I‚Äôll say about learning is that I‚Äôve seen too many people get a masters in data analytics, AI, computer science, etc. only to get a job post-school that does none of that. It‚Äôs a shame. Don‚Äôt settle. Don‚Äôt give up. Keep learning.</p>
</section>
<section id="real-coders-ship" class="level3">
<h3 class="anchored" data-anchor-id="real-coders-ship">Real Coders Ship</h3>
<p>Real coders don‚Äôt just take classes and read books. They ship, meaning they build stuff and put it out into the universe. My learning only started when I put the books down and started pulling real data and training real machine learning models. Projects that actually helped my team in my current job, even if that job wasn‚Äôt software engineering. Shipping 10 projects is more impressive than acing 10 classes.</p>
<p>Some people create a ‚Äúportfolio‚Äù of projects and have it live on their GitHub. I don‚Äôt think that‚Äôs a good idea. Real work is messy, and is the best indicator of future performance to an employer. Projects in a portfolio allow you to control all the variables, which is not how the real world works.</p>
<p>You can have public GitHub repos that show as samples of your work. But they should not be sample projects. They should either be pieces of work that you applied in your real job made open source and accessible to others. Or side projects where you wanted to build something cool to help you in your personal life or another organization. For example if you built your own website for your wedding, and had the code in a GitHub repo for hiring managers to see your coding chops. Or you built a bot that constantly analyzes your fantasy football league‚Äôs waiver wire and automatically picks up the best players for you based on AI powered analysis. There are endless ideas. Get building. Go out and ship.</p>
</section>
<section id="ai-is-your-friend" class="level3">
<h3 class="anchored" data-anchor-id="ai-is-your-friend">AI is Your Friend</h3>
<p>In this post ChatGPT era, all technical work becomes 100x easier to learn. You don‚Äôt need school, or even books. You can, for free, start using a large language model (LLM) to learn coding concepts and even have it ask you technical questions to test your learning.</p>
<p>ChatGPT and other tools like GitHub Copilot now become your coding buddy, always there to help you when you get stuck or don‚Äôt know how to do something. Any new kid on the coding block can now get up to speed on languages like Python quickly because they have a personal AI tutor available 24/7. Go pay for a few of these AI services, and use them every day.</p>
</section>
<section id="use-your-competitive-advantage" class="level3">
<h3 class="anchored" data-anchor-id="use-your-competitive-advantage">Use Your Competitive Advantage</h3>
<p>Some people feel like coming into software engineering from a nontechnical discipline is a major handicap. I actually think it‚Äôs a superpower. If you come from a business background you actually have something other engineers do not. Business domain knowledge combined with technical knowledge is a deadly combination. You now understand both sides of the coin. You know how to make decisions about the business (through the lens of marketing, finance, sales) and can now go and build things that improve how the business serves customers.</p>
<p>For me having a finance background first, then learning coding second was a good combination. It created a lot of skills that are now invaluable. For example the skill of communicating things in simple terms. When I was building machine learning solutions for finance users, no one cared about the sophistication about my model or how much feature engineering I applied to the data. They just wanted to know how the solution ran at a high level and if it actually worked by showing improvements in a certain metric. For example, creating a more accurate revenue forecast for a Microsoft product. I‚Äôve seen traditional data scientists/engineers try to explain their work to regular people and it can sometimes be like nails on a chalkboard. It‚Äôs painful to watch because they cannot explain themselves in plain english. And if the end user can‚Äôt understand how something works, then they won‚Äôt trust it, which means they won‚Äôt use it.</p>
<p>Usually being 80% competent at two skills can create more impact than being 99% competent in a single skill. Focus on <a href="https://personalexcellence.co/blog/talent-stack/">combining skills</a>. If you can build and you can sell, you will be unstoppable.</p>
</section>
<section id="baby-steps-not-one-giant-leap" class="level3">
<h3 class="anchored" data-anchor-id="baby-steps-not-one-giant-leap">Baby Steps, Not One Giant Leap</h3>
<p>Don‚Äôt go and build the next social networking app on your first coding project. Crawl, walk, then run with new technology. Here is a good workflow for learning progression.</p>
<ol type="1">
<li>Crawl: Get some code working on your computer.</li>
<li>Walk: Move that code up into the cloud and have it run on a different machine that‚Äôs not yours.</li>
<li>Run: Build a production ready system with proper CI/CD, unit testing, version control, etc.</li>
</ol>
<p>Those steps can take years, and that‚Äôs ok. You might even get a software engineering job with just knowledge of #1. But as you progress you will eventually figure out #2 and #3. The same can be said for the actual job progression.</p>
<ol type="1">
<li>Business Partner: You are the business person who works with the engineers.</li>
<li>Program Manager: Now you are the person in the middle between the business partner and the software engineer. This is an engineering role, and in some companies like Microsoft your compensation will be the same as a software engineer.</li>
<li>Software Engineer: You now build things, in collaboration with the program manager who tells you what the business partner needs.</li>
</ol>
<p>Going from business partner, to program manager, to software engineer is a lot easier than skipping directly to software engineer. When I graduated the FRP my first role title was ‚ÄúIT Solution Manager‚Äù. I still don‚Äôt know what that means. Overall I was basically a program manager that coded on the side when no one was looking. Eventually that side coding turned into real solutions I pitched to my manager and got into production. Which eventually led me to working solely on building things. Then I finally got the title of software engineer. It didn‚Äôt happen overnight, but it was possible.</p>
</section>
<section id="getting-promoted" class="level3">
<h3 class="anchored" data-anchor-id="getting-promoted">Getting Promoted</h3>
<p>Ok so maybe you now have a sweet engineering gig. Now what? How do you grow your career? Let‚Äôs talk about getting promoted.</p>
<p>You get promoted when what you build has impact. Not just for building cool or complex things. Most impact with technology can boil down to a few things.</p>
<ol type="1">
<li>Time savings</li>
<li>Money savings</li>
<li>Increasing revenue</li>
</ol>
<p>You work needs to boil down to numbers. Not what you shipped but what happened after you shipped it. Most impact is money and time. Either saving it or getting more of it. <strong>What gets measured gets managed. What gets measured gets promoted.</strong> Training a deep learning model is cool but shipping a simple linear regression in half the time might get you promoted twice as fast. Everyone is in sales. Once you build it, you have to sell it. If you can build and sell, you will be unstoppable. Ask your boss what you need to do this year to get promoted, then do those things. Ruthlessly prioritize and execute. Ignore everything else.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Learning to code is important. Life is all about leverage. The best types of leverage in the world are permissionless leverage. They have no marginal costs of replication. These are things like code and content (podcasts, blog posts, etc.). Why do you think I wrote this? Because this kind of content has leverage, meaning it can scale. It‚Äôs easier to write this once then to give the same advice to 100 people who ask for it. Save your keystrokes folks. That‚Äôs why this personal blog exists. Hopefully it‚Äôs helpful to you my dear reader üòä.</p>
<p>Pretty soon everyone will be a manager, just not managing people. You will manage AI people, or bots/agents, who will do your work for you. Knowing how technology works will help you grow your leverage, whether you‚Äôre a software engineer or not. Most things 50 years from now will boil down to AI doing the thinking plus executing digital tasks, and robots executing physical tasks. What do you think those run on? You guessed it, code. Software will eat the entire world, it‚Äôs just a matter of time. I‚Äôd rather be giving our robot overlords instructions then taking instructions from them. Everyone will have a choice. Take the red pill. Go all the way down the rabbit hole. Learn some code.</p>
<p>Last thing I‚Äôll say is that it takes guts to move to a new field. People who only do what their boss tells them to do will never have the guts to do it. It takes a certain amount of high agency to do it. If you‚Äôre considering or are already in the process of doing it, I salute you. The world needs more people like you. Good luck!</p>


</section>

 ]]></description>
  <category>life</category>
  <category>finance</category>
  <category>career</category>
  <guid>https://mftokic.github.io/posts/2024-09-04-sde-career-path/</guid>
  <pubDate>Wed, 04 Sep 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-09-04-sde-career-path/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Weekend Reads (8/30/24)</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-08-30-weekend-reads/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-08-30-weekend-reads/image.png" class="img-fluid"></p>
<section id="articles" class="level2">
<h2 class="anchored" data-anchor-id="articles">Articles</h2>
<ul>
<li><a href="https://www.muskfoundation.org/">Elon Musk‚Äôs Foundation Page</a>: money talks, wealth whispers</li>
<li><a href="https://thielfellowship.org/apply">Thiel Fellowship App</a>: should be the application for most jobs</li>
<li><a href="https://eugeneyan.com/writing/prompting/">Prompt Engineering Fundamentals</a></li>
</ul>
</section>
<section id="videos" class="level2">
<h2 class="anchored" data-anchor-id="videos">Videos</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=WO5m-roVzjg">24 Controversial Truths About Success and Failure on Modern Wisdom</a></li>
<li>One of a Kind Guy on My First Million
<ul>
<li><a href="https://www.youtube.com/watch?v=SVwLEocqK0E">Part 1</a></li>
<li><a href="https://www.youtube.com/watch?v=W_oJJtSvCcA">Part 2</a></li>
</ul></li>
<li><a href="https://www.youtube.com/watch?v=TRnoCjPzmV8">Incentive Masterclass on My First Million</a></li>
<li><a href="https://www.youtube.com/watch?v=qDw8zT7pCdQ">WHOOP CEO on Diary of a CEO</a></li>
<li><a href="https://www.youtube.com/watch?v=ROuPd-Cwrbo">New OpenAI Developments on AI for Humans</a></li>
</ul>
</section>
<section id="tweets" class="level2">
<h2 class="anchored" data-anchor-id="tweets">Tweets</h2>
<ul>
<li><a href="https://x.com/ajassy/status/1826608791741493281">Amazon CEO Automates Tech Debt with AI</a></li>
</ul>
</section>
<section id="products" class="level2">
<h2 class="anchored" data-anchor-id="products">Products</h2>
<ul>
<li><a href="https://elemindtech.com/">Headband That Puts You to Sleep</a></li>
</ul>
</section>
<section id="songs" class="level2">
<h2 class="anchored" data-anchor-id="songs">Songs</h2>
<ul>
<li><a href="https://open.spotify.com/track/0dbTQYW3Ad1FTzIA9t90E8?si=Zky16-6eRf--T-1ZIV82NQ&amp;nd=1&amp;dlsi=16a2bd4780d24517">Mona Lisa by Lil Wayne</a></li>
</ul>


</section>

 ]]></description>
  <category>weekend-reads</category>
  <guid>https://mftokic.github.io/posts/2024-08-30-weekend-reads/</guid>
  <pubDate>Fri, 30 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-08-30-weekend-reads/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Weekend Reads (8/23/24)</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-08-23-weekend-reads/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-08-23-weekend-reads/image.png" class="img-fluid"></p>
<section id="articles" class="level2">
<h2 class="anchored" data-anchor-id="articles">Articles</h2>
<ul>
<li><a href="https://mftokic.github.io/posts/2024-08-12-ml-fcst-faq/">Simple Answers to Hard ML Forecasting Questions</a></li>
<li><a href="https://info.deeplearning.ai/llm-price-war-black-forests-powerful-open-image-generator-the-high-cost-of-ai-leadership-machine-translation-goes-agentic?ecid=ACsprvurcocB85Yj2tmm-kaWhU7MjIxZkryFxNSQQ4iYRIhIzhvbptMewhYJmzlP30TSYdWmaxLc&amp;utm_campaign=The%20Batch&amp;utm_medium=email&amp;_hsmi=320139015&amp;utm_content=320139015&amp;utm_source=hs_email">LLM Performance vs Price</a></li>
<li><a href="https://fortune.com/2024/08/14/google-eric-schmidt-working-from-home-ai-openai/">Ex Google CEO on Work From Home</a></li>
<li><a href="https://www.wired.com/story/the-english-premier-league-has-a-new-iphone-powered-offside-detection-system/">Fixing Offsides in Soccer for Good</a></li>
<li><a href="https://www.popsci.com/technology/ai-tongue-scanner/">AI Tongue Scans Spot Disease Early</a></li>
</ul>
</section>
<section id="videos" class="level2">
<h2 class="anchored" data-anchor-id="videos">Videos</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=FyLwVyJ9uXw">Breathing Expert on Diary of a CEO</a></li>
</ul>
</section>
<section id="podcasts" class="level2">
<h2 class="anchored" data-anchor-id="podcasts">Podcasts</h2>
<ul>
<li><a href="https://open.spotify.com/episode/2DFsA7qIdjZZukcCRbBRhI?si=mck3VapCQLilUMkwV1xkhA&amp;nd=1&amp;dlsi=d2c18c0011b14af7#login">Peter Thiel on Joe Rogan</a></li>
<li><a href="https://open.spotify.com/episode/6KBpL2XfR9VdojbKNpE7cX?si=74CSFwnNQXavZTUYX6VpIA&amp;context=spotify%3Acollection%3Apodcasts%3Aepisodes&amp;nd=1&amp;dlsi=fced64e3e9304a60">Pieter Levels on Lex Fridman</a></li>
</ul>
</section>
<section id="movies" class="level2">
<h2 class="anchored" data-anchor-id="movies">Movies</h2>
<ul>
<li><a href="https://truefilms.com/kumare/">Kumare</a></li>
<li><a href="https://truefilms.com/10-mph/">10 mph</a></li>
<li><a href="https://truefilms.com/bill-cunningham/">Bill Cunningham New York</a></li>
</ul>


</section>

 ]]></description>
  <category>weekend-reads</category>
  <guid>https://mftokic.github.io/posts/2024-08-23-weekend-reads/</guid>
  <pubDate>Fri, 23 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-08-23-weekend-reads/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>FAQ on Machine Learning Forecasting</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-08-12-ml-fcst-faq/</link>
  <description><![CDATA[ 





<p>Over the last few years I‚Äôve presented to hundreds of people outside of Microsoft around how we approach machine learning (ML) forecasting within Microsoft finance. A lot of great questions were asked during those conversations. In this post I want to highlight some of the most commonly asked questions and my take on answering them. Hopefully this can be a quick reference for anyone ML curious or want to deepen the ML work being done on their teams. Use the table of contents to skip around to the sections you‚Äôre most interested in. If there are any topics missing please reach out to me via <a href="https://www.linkedin.com/in/michaeltokic/">LinkedIn</a> and I will continue to update this post.</p>
<section id="toc" class="level2">
<h2 class="anchored" data-anchor-id="toc">FAQ Table of Contents</h2>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<ul>
<li>Getting high quality historical data</li>
<li>Using third party data</li>
<li>New info not found in the training data, one time events</li>
<li>Handling outliers</li>
</ul>
</section>
<section id="technical" class="level3">
<h3 class="anchored" data-anchor-id="technical">Technical</h3>
<ul>
<li>Interpreting the black box</li>
<li>What models to use, should we use deep learning</li>
<li>What programming language or framework to use</li>
<li>Using large language models like ChatGPT to forecast</li>
<li>What level of accuracy is good</li>
</ul>
</section>
<section id="humans" class="level3">
<h3 class="anchored" data-anchor-id="humans">Humans</h3>
<ul>
<li>How to make forecast owners accountable for the ML number</li>
<li>Building trust in the ML forecast</li>
<li>Who owns the ML creation process</li>
<li>How to get started with ML</li>
<li>Going from ML as a triangulation point to replacing manual human forecasts</li>
<li>Building data science talent in finance</li>
</ul>
</section>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<section id="historical-data" class="level3">
<h3 class="anchored" data-anchor-id="historical-data">Getting high quality historical data</h3>
<p><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage in, garbage out</a>. That‚Äôs probably the most common saying in the world of ML. If you cannot get high quality historical data, then there is no easy way to produce an accurate ML forecast. You can‚Äôt work your way around noisy or incomplete data. If your data is messy, hard to find, and comes from 10 different systems, then ML is not something you should be worried about. A nice bow on top of a pile of crap is still a pile of crap. Fix your data first, then focus on ML after.</p>
<p>Back to Table of Contents</p>
</section>
<section id="third-party-data" class="level3">
<h3 class="anchored" data-anchor-id="third-party-data">Using third party data</h3>
<p>Your company‚Äôs business is most likely impacted by greater market forces outside of your control. For example the health of the economy or how much money customers have to spend. Adding data from outside of your company (third party data) as features in your ML models is a good way to improve forecast accuracy, while also being able to describe what outside forces impact your business the most. Some data is freely available, while others have to be paid for. What data you use is up to your domain knowledge of your business.</p>
<p>Free Data</p>
<ul>
<li><a href="https://fred.stlouisfed.org/">FRED</a></li>
<li><a href="https://data.worldbank.org/">World Bank</a></li>
<li><a href="https://data.imf.org/?sk=388dfa60-1d26-4ade-b505-a05a558d9a42">International Monetary Fund</a></li>
<li><a href="https://data.un.org/">United Nations</a></li>
<li><a href="https://trends.google.com/trends/">Google Trends</a></li>
</ul>
<p>Paid Data</p>
<ul>
<li><a href="https://www.idc.com/data-analytics">IDC</a></li>
<li><a href="https://tradingeconomics.com/indicators">Trading Economics</a></li>
</ul>
<p>Back to Table of Contents</p>
</section>
<section id="one-time-events" class="level3">
<h3 class="anchored" data-anchor-id="one-time-events">New info not found in training data and one time events</h3>
<p>If you are changing the price of your product in three months, this is most likely going to impact your future revenue forecast. But if you have never changed the price of your product before, then a ML model cannot learn from that information. For a model to learn from one time events, it needs to be present in the historical data a model is trained on. <a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The future must always learn from the past</a>.</p>
<p>Back to Table of Contents</p>
</section>
<section id="outliers" class="level3">
<h3 class="anchored" data-anchor-id="outliers">Handling Outliers</h3>
<p><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/#outliers">Outliers</a> in your data can have a bad impact on your future forecast. It can hurt accuracy or give false signals of the future. There are many ways to deal with outliers. The easiest way is to use statistical methods to identify and remove them. Treating them as a missing value you can then replace. Sometimes outliers are more subtle, and take a trained eye to spot them. This is where the <a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">domain expertise</a> of a person comes into play.</p>
<p>Back to Table of Contents</p>
</section>
</section>
<section id="technical" class="level2">
<h2 class="anchored" data-anchor-id="technical">Technical</h2>
<section id="black-box" class="level3">
<h3 class="anchored" data-anchor-id="black-box">Interpreting the black box</h3>
<p>This one is a toughie. When a person creates a forecast manually, most often using excel, someone else can come into that financial model and trace cell by cell exactly what‚Äôs going on. Going from input data, to assumptions, to the formulas that create the final output. This is the kind of exactitude that allows accountants to sleep peacefully at night. Everything is in order and everything is perfectly understood. But we are not accountants. This is finance. We have to make calls about the future that are uncertain. We can never have 100% certainty that something is going to happen. If that‚Äôs the case then your company is doing something illegal. Get out now!</p>
<p>The biggest paradigm shift someone has to make with machine learning is giving up this total control of the forecast. And in essence take a <a href="https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/#leap-of-faith">leap of faith</a>. Machine learning models are enigmas. Sometimes akin to magic. The cannot be perfectly understood because the capture non-linear relationships in data that a human never could. That‚Äôs why we have them, because they can work better and faster than our human brains in some tasks.</p>
<p>There are ways to understand these models, but they cannot be perfectly audited like a manual forecast done in excel. Instead they have to be interrogated. Not like a criminal wanted for war crimes but more like a therapist talking to their patient. There is no way for a therapist to know exactly what‚Äôs going on inside of their patients mind. But they can start to ask questions that can give clues into what‚Äôs going on and see why the person has made past decisions in their life.</p>
<p>The best resource I know on explaining ML models is <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a> by Christoph Molnar. Here‚Äôs a quick overview of the method‚Äôs described in the book.</p>
<ul>
<li><strong>Model Specific:</strong> These are models like linear regression or a single decision tree where we can see exactly what‚Äôs going on under the hood. The model structure is more like an excel formula we can trace step by step. But because they are easy to explain, they are simple in nature and may not produce the most accurate forecast. That is the tradeoff between having a forecast that can be easily explained versus having a forecast that is the most accurate. The more accurate the forecast, the more likely you cannot explain it perfectly.</li>
<li><strong>Model Agnostic:</strong> For more advanced models like gradient boosted trees and deep learning, we can use methods that approximate what‚Äôs going on under the hood of a complex model. This is when we have to act like a therapist and start asking questions to our model and see what answers it gives back. There are two ways of doing this.
<ul>
<li><strong>Global Interpretability:</strong> This uses methods that can see overall what‚Äôs impacting the model the most. For example you can see what input variable (or feature) is the most important in the model overall.<br>
</li>
<li><strong>Local Interpretability:</strong> This uses methods to see what‚Äôs going on for each individual forecast data point. For example you can see for a specific future forecast what‚Äôs impact that number the most.</li>
</ul></li>
</ul>
<p>The last thought I‚Äôd leave you with is this. Have you ever not used ChatGPT because you couldn‚Äôt get an explanation of its answer? For example maybe you asked it to help you write some excel formulas to format dates. Do you trust the output it gave you because the excel formula was correct or because it could tell you exactly how it came to that conclusion? What if the explanation it gave was made up or a hallucination? If the excel formula is correct you would still use it right? Even the CEO of OpenAI, Sam Altman, cannot explain how models like GPT-4 think under the hood. But hey, ChatGPT was still the fastest growing product of all time. Sometimes imperfect interpretability is ok. But maybe your CEO is still demanding an explanation of the forecast numbers, so this is <a href="https://mftokic.github.io/posts/2023-02-11-three-levels-of-ml-adoption/">still a hard problem to solve in finance</a>.</p>
<p>Back to Table of Contents</p>
</section>
<section id="models" class="level3">
<h3 class="anchored" data-anchor-id="models">What models to use, should we use deep learning</h3>
<p>People are always attracted to the hot new thing, and I can‚Äôt blame them. New is exciting. When it comes to ML forecasting, new isn‚Äôt always better. The newest trend in ML is all about deep learning. Or models that can mimic the human brain. While they work really well for things like analyzing photos and text, using them on tabular data (aka excel data) hasn‚Äôt always worked out well. That‚Äôs why I <a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">recommend using deep learning last</a>.</p>
<p>Here are the models to use first, then you can always resort to deep learning if need be.</p>
<ul>
<li><strong>Univariate Models</strong>: These are the simplest forecasting models. Since they only need one variable, hence the name univariate. If you want to forecast revenue, then you only need historical revenue and you‚Äôre off and running. They run extremely fast and can scale to millions of data combinations without spending too much on cloud compute. Here are a few popular ones.
<ul>
<li><strong>ARIMA</strong>: An ARIMA (AutoRegressive Integrated Moving Average) model predicts future values in a time series by combining differencing (modeling the difference between periods), autoregression (using past values), and moving averages (using past forecast errors). It‚Äôs the most common univariate model in the forecasting game.</li>
<li><strong>Exponential Smoothing</strong>: Forecasts future values in a time series by applying decreasingly weighted averages of past observations, giving more importance to recent data points to capture trends and seasonal patterns.</li>
<li><strong>Seasonal Naive</strong>: Predicts future values by simply repeating the observations from the same season of the previous period, assuming that future patterns will mimic past seasonal cycles. Don‚Äôt sleep on this one! You‚Äôd be surprised how often it comes in handy as a good benchmarking model to compare with more complicated models.</li>
</ul></li>
<li><strong>Traditional ML Models</strong>: After trying univariate models, it‚Äôs time to try more traditional machine learning models. These are models built specifically for tabular data, or data that can live in a SQL table or excel spreadsheet. These models are multivariate, which allow them to incorporate outside variables as features to improve their forecast accuracy. They require more handling than a model like ARIMA, since they need <a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">feature engineering</a> and proper <a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">time series cross-validation</a>. Multivariate models can also learn across multiple time series at the same time, instead of being trained on just a single time series like a univariate model. Here are a few common multivariate models.
<ul>
<li><strong>Linear Regression</strong>: Predicts future values by fitting a line to the historical data, where the line represents the relationship between the dependent variable and one or more independent variables.</li>
<li><strong>XGBoost</strong>: Predicts future values using an ensemble of decision trees, boosting their performance by iteratively correcting errors from previous trees, resulting in a highly accurate and robust prediction model.</li>
<li><strong>Cubist</strong>: Predicts future values by combining decision trees with linear regression models, creating rule-based predictions that incorporate linear relationships within each segment of the data for greater accuracy.</li>
</ul></li>
</ul>
<p>Back to Table of Contents</p>
</section>
<section id="language" class="level3">
<h3 class="anchored" data-anchor-id="language">What programming language or framework to use</h3>
<p>Should I use python? But what if I learned R in my statistics class? What about stata or good ole javascript? Ask 10 data scientists what programming language to use and you‚Äôll probably get 10 different answers. There is no right answer. It‚Äôs kind of like arguing what hammer to use when building a house. Shouldn‚Äôt we just be worried about getting the house built? And make sure we don‚Äôt screw it up?</p>
<p>Here is my take on the ML language wars. You should use both, python and R. Different languages offer different things, and depending on the task you might want to use one over the other. Often it‚Äôll boil down to specific open source software that might only be available in one language but not the other. So over the course of a long data career it‚Äôs probably a good idea to be competent at both.</p>
<p>With that said, if you could only learn one programming language, learn python. That‚Äôll get you the farthest the fastest in terms of useful knowledge to get building. I think in a few years these debates will go away, because large language models (LLM) will come and save the day. Imagine writing code in your favorite language, using the packages you like, and then have a LLM take your code and translate it into blazing fast machine code that runs like a race car. So it won‚Äôt matter if you only know one language or the other. That‚Äôs where I hope we‚Äôre headed.</p>
<p>Back to Table of Contents</p>
</section>
<section id="llm" class="level3">
<h3 class="anchored" data-anchor-id="llm">Using large language models like ChatGPT to forecast</h3>
<p>With the explosion of large language models (LLM) that can do everything from tell dad jokes to write production grade code, can‚Äôt we just offload all forecasting work to them? In essence you could, but I think it‚Äôs kind of overkill and leaves a lot to be desired. LLMs take in text, and spit out text. They are good with words, but not that good with numbers. They can‚Äôt perform on par with a calculator, because that‚Äôs not how they were designed. So you can‚Äôt just copy a table from excel, give it to ChatGPT, and hope to get next quarters revenue forecast. It might give it to you, but it won‚Äôt be a good forecast. It might even make something up.</p>
<p>The more sensible route to take is to have the LLM write code that can create forecast models and have it execute it for you. For example use the code interpreter feature in ChatGPT to have it take your uploaded CSV file and execute a bunch of python code against it to get a final forecast. This kind of workflow might be good for initial exploration, but it shouldn‚Äôt be used in a production setting where you need an updated forecast each month. It‚Äôs kind of like needing a place to sleep and each night you build a new house from scratch, only sleep there one night, then the next night build another house from scratch. Having LLMs produce code on the fly each time can lead to inconsistent results that may not be reproducible when you ask ChatGPT to do it again. You could take the code from the first forecast iteration, save it, and either run it yourself each time or give it to ChatGPT as a prompt. But even then you are still missing out. Some automated forecasting packages, like the one I own called <a href="https://microsoft.github.io/finnts/index.html">finnts</a> have over 10,000 lines of code. So a LLM will most likely not be writing that much code to answer one prompt, and if they did having you trying to save and manage that code is out of the question.</p>
<p>I think LLMs can still have some part in the forecasting process. They are useful before and after the actual model training is done. For example you can have code interpreter analyze your data for things like outliers or help in running correlation analysis to see what variables could help improve your forecast accuracy. Then you will take those learnings and run the forecast outside of the LLM environment, or you could have the LLM call an outside function to kick off a forecast. This is called ‚Äúfunction calling‚Äù, where a LLM can use an outside tool (most often calling an API via code) that can accomplish the task a LLM cannot (like getting the current weather). LLMs can then be used after the forecast process is ran to then analyze the final forecast outputs. Tools like code interpreter can make charts and analyze historical back testing accuracy.</p>
<p>Maybe in the future the LLM can help explain how the final forecast was created, or better yet answer questions about forecast variance once actuals land in the future. But using it to actually create predictions or train models itself is still a tall order. With that said there are <a href="https://docs.nixtla.io/">new time series specific LLMs</a> that are being released, so this is an exciting area to watch closely.</p>
<p>Back to Table of Contents</p>
</section>
<section id="accuracy" class="level3">
<h3 class="anchored" data-anchor-id="accuracy">What level of accuracy is good</h3>
<p>There is no right answer. It all depends on the specific data you are using and how it compares to non-ML approaches you have done previously.</p>
<p>A common metric in time series forecasting is the MAPE error metric. MAPE stands for mean absolute percentage error. This is very similar to variance to forecast percent metrics you might already use. Think of it as the average percent error (as an absolute value) across every forecasted data point. There are plenty of other metrics out there, but MAPE is a good one since it‚Äôs not dependant on scale and percents are easy for anyone to wrap their head around.</p>
<p>A MAPE of 5% means that on average, your forecast is off plus or minus 5%. So the closer to zero the better. If you ask any finance person what kind of MAPE they want for their forecast, almost all of them will say less than 1%. Or more than 99% accuracy. Think of accuracy as the inverse of MAPE. Maybe the ML forecast has a MAPE of 10%, but your previous manual excel model ended up having a 20% MAPE. The ML forecast has a 50% reduction in forecast error, even though it‚Äôs still in the double digits. So evaluating ML is always relative to what kind of performance you got with other forecast methods probably done in excel.</p>
<p>With that said, here are some rules of thumb I use when running my own ML forecasts.</p>
<ul>
<li><strong>Daily and Weekly Data</strong>: Less than a 10% MAPE is terrific. But even MAPEs as high as 30% are ok, because often we might sum up a daily forecast to a monthly or quarterly level. And when evaluated at that aggregate level you may start to have drastically improved MAPEs.</li>
<li><strong>Monthly, Quarterly, Yearly Data</strong>: There are a few levels of accuracy that I think are good at this level. Again, it‚Äôs all relative to what kind of performance you had before using ML.
<ul>
<li>Less than a 5% MAPE is good</li>
<li>Less than a 3% MAPE is great</li>
<li>Less than a 1% MAPE is amazing</li>
</ul></li>
</ul>
<p>Even going from a 3% MAPE to a 2% MAPE is a big achievement. Because it‚Äôs a lot harder to do that than to go from a 15% MAPE to a 10% MAPE. It‚Äôs still the same level of improvement but once you get closer to zero the MAPE improvements seem to happen on a log scale. Where each percentage point you reduce continues to get harder as you get closer to zero.</p>
<p>Back to Table of Contents</p>
</section>
</section>
<section id="humans" class="level2">
<h2 class="anchored" data-anchor-id="humans">Humans</h2>
<section id="accountability" class="level3">
<h3 class="anchored" data-anchor-id="accountability">How to make forecast owners accountable for the ML number</h3>
<p>Having a ML model automate your forecast process is great, until you have to tell your CFO why your quarterly forecast is off by xyz%. ‚ÄúIt was ML‚Äôs fault‚Äù is not the right answer. But it‚Äôs hard for a human to be on the hook for work that a machine did for them. Maybe the ML forecast came from another engineering team, so the final owner of the forecast maybe had no part in creating the forecast. This can make accountability hard.</p>
<p>In order to get people on board with transitioning more of their work to ML, you need senior leadership buy in. People at the top need to be invested in the promise of ML and the tradeoffs it might provide. Since you cannot audit a ML forecast like you can with an excel model (tracing cell by cell) there is an essential leap of faith that has to happen.</p>
<p>In addition to having senior leadership buy in to improve accountability, being able to adjust the output from ML can also help. Financial analysts can impart their domain expertise about their business by making small manual adjustments to the ML forecast. That way ML can get you 80% of the way to a completed forecast, and a human might make adjustments on that final 20% to finalize the forecast. Having the forecast owners be ‚Äúhumans in the loop‚Äù of a ML process adds a sense of ownership, which can then improve accountability.</p>
<p>A final way to improve accountability is through the <a href="https://insidebe.com/articles/the-ikea-effect/">Ikea effect</a>. Which states that we value things more if we are involved in making them. The best way for this to work is if the final forecast owners create their own ML forecast through self-serve tools that abstract away the complex of ML and allows analysts to upload data and get back forecasts with a few clicks of their mouse. This is exactly <a href="https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/">what we did in Microsoft finance</a> and it has worked out well so far.</p>
<p>Back to Table of Contents</p>
</section>
<section id="trust" class="level3">
<h3 class="anchored" data-anchor-id="trust">Building Trust in the ML forecast</h3>
<p>Building trust in the ML forecast is the hardest part of using ML. There are three ways that have worked well in helping non-technical financial forecast owners warm up to and fully transition to using ML to forecast.</p>
<ul>
<li><strong>Historical Accuracy</strong>: The best way to get someone on board with a forecast into the future is to show them how well a similar forecast has performed in the past. It‚Äôs not a perfect proxy for future performance but gives the end user a good idea of how well a ML model is performing. Common performance metrics to use are <a href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error">MAPE</a> (mean absolute percentage error) and <a href="https://en.wikipedia.org/wiki/Root_mean_square_deviation">RMSE</a> (root mean squared error). MAPE is similar to existing variance to forecast calcs already done by financial analysts so it‚Äôs a good error metric to convey past performance. When calculating historical performance, aka back testing, it‚Äôs a good idea to create hypothetical forecasts for the most recent periods of your data. Making sure you cover enough historical time to ensure your ML model is robust. For example with a monthly forecast. You might want to just forecast the next 3 months into the future. If you have 5+ years of historical data you could have 4 historical back tests where you train a model and produce a 3 month forecast for each of the last 4 quarters in your historical data.</li>
<li><strong>Prediction Intervals</strong>: Understanding the uncertainty of the future forecast can also help build trust. Think of prediction intervals as upper and lower bounds of the future forecast that convey a certain percent of probability that the future value will land in between these bounds. Common prediction intervals are 80% and 95%. For example, you might have a future forecast of $100 for next month, with a 80% prediction interval of $80 and $120. This means there is an 80% chance that the actual value of next month will be between $80 and $120. So the tighter the prediction interval, the better. Having an prediction interval that‚Äôs +-5% of the forecasted value gives more comfort to the end user than one that‚Äôs +-30%. Just make sure that the end user of the forecast knows that those upper and lower bounds are not a ‚Äúbull case‚Äù and ‚Äúbear case‚Äù of what could happen in the future, like they might be used to in other financial modelling. But instead just a way to capture the uncertainty of the future prediction.</li>
<li><strong>Interpretability</strong>: Finally another good way to build trust is being able to explain how a future ML forecast was created. You will not be able to perfectly explain it like you can an excel model (by tracing through it cell by cell). But you can use methods to poke and prod a model to see what might be going on under the hood.</li>
</ul>
<p>Back to Table of Contents</p>
</section>
<section id="ownership" class="level3">
<h3 class="anchored" data-anchor-id="ownership">Who owns the ML creation process</h3>
<p>Being at Microsoft we are lucky to have engineering teams (sometimes called IT in other companies) that sit inside of finance and report to the CFO. This allows us to have software engineers who are solely focused on improving the lived experience of each of the 5,000+ employees under the CFO.</p>
<p>On those engineering teams there are people like myself who work on ML. Most of my time is spent helping produce and evangelize ML forecasting. My team has built two unique ways of how ML gets created and consumed by financial analysts.</p>
<ul>
<li><strong>Standard API</strong>: This allows other engineering teams to call upon ML on demand to get a forecast. Without needing to set up the infrastructure to train and serve models. This process works hand in hand with <a href="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/">other forecast centralization efforts we‚Äôve done</a>.</li>
<li><strong>Self-Serve UI</strong>: On the other end of the spectrum is to have tools that allow any financial analyst to produce their own forecast with a few clicks of their mouse. This allows us to scale ML to every single person in finance, without needing to rely on an engineering team to help produce a forecast. The tool we built for this is called Finn, and you can <a href="https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/">learn more about it here</a>.</li>
</ul>
<p>Back to Table of Contents</p>
</section>
<section id="starting" class="level3">
<h3 class="anchored" data-anchor-id="starting">How to get started with ML</h3>
<p>Getting the ball rolling can seem like an insurmountable task. Thankfully it‚Äôs been easier than ever to get started with ML. <a href="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/">Check out how we got started</a> back in 2015.</p>
<p>Another way to make quick wins is to use something pre-built off the shelf that‚Äôs ready to go. Thankfully you can use it for free! We have open-sourced our ML forecasting code into a standard package called <a href="https://microsoft.github.io/finnts/index.html">finnts</a>. You can have your engineers take this code and quickly get up and running with ML forecasts within a week. It condenses almost 10 years of trial and error in Microsoft finance down to code that works at scale in production. Try it!</p>
<p>Back to Table of Contents</p>
</section>
<section id="migrating" class="level3">
<h3 class="anchored" data-anchor-id="migrating">Going from ML as a triangulation point to replacing manual human forecasts</h3>
<p>There‚Äôs a saying that 80% of ML projects <a href="https://venturebeat.com/ai/why-do-87-of-data-science-projects-never-make-it-into-production/">never make it into production</a>. This definitely applies to ML forecasting, but in a different way. Better put, 80% of ML forecast projects never make it past the triangulation point phase.</p>
<p>There are three distinct phases of adopting a ML forecast.</p>
<ol type="1">
<li><strong>Initial Development</strong>: Going from initial idea to a first prototype. Trying to see what a ML forecast would look like and if accuracy is good or not.</li>
<li><strong>Triangulation Point</strong>: Using a ML forecast every month or quarter alongside your existing manual forecast process. ML is another data point you ‚Äútriangulate‚Äù with the actual forecast you will be using. So ML is nice to have but not required.</li>
<li><strong>Baseline Forecast</strong>: Using a ML forecast to replace your manual process. Where now the first step in your forecast cycle is to produce the ML forecast and build on top of it. Using the ML forecast as the initial ‚Äúbaseline‚Äù foundation. You have now ‚Äúburned the ships‚Äù of your manual process never to look back. You are in it to win it with ML.</li>
</ol>
<p>It‚Äôs easy to go from phase 1 to phase 2. With tools like <a href="https://microsoft.github.io/finnts/index.html">finnts</a> you can get a new forecast up and running in less than a day. But most ML forecasts go to the triangulation point stage and die. It‚Äôs kind of a bummer but a fact of life. Here are some tips to break through the resistance and get all the way to phase 3.</p>
<ul>
<li>Have a timeline. Finance teams are busy. There is always more to do in the time allotted to do it. So having deadlines helps move work along. Instead of a ‚Äúone day we will switch to ML‚Äù attitude you need a deadline like ‚Äúwe will switch to ML within the next two quarters‚Äù. This instills a sense of urgency and gets things moving. I‚Äôve seen ML projects stall for 2+ years because there wasn‚Äôt a timeline set in place. An answer like ‚Äúwe‚Äôll move to ML sometime this fiscal year‚Äù is not good enough. Keep pushing until a deadline is agreed upon, and do all you can to stick to it.</li>
<li>Get leadership buy in. The quickest way to change a financial analysts behavior is to have them hear it from their boss. Getting senior leadership buy in helps with the ongoing issue of prioritization. If the leaders make it a priority then every person on their team has no choice but to make it a priority. Things start to move a lot faster once that happens.</li>
<li>If accuracy is a concern, establish a benchmark to beat. If you ask any forecast owner what kind of forecast error do they want, they will probably say something like ‚Äúless than 1% error‚Äù. Meaning 99% accuracy. This is all well and good, but if the previous manual forecast process had a forecast error of 10% then having ML be 9% or less is already a win. Make sure you compare the approaches and agree beforehand on switching once a certain benchmark is beat.</li>
<li>Squeaky wheel gets the grease. Constant contact with the end ML forecast user helps speed things up. I can‚Äôt tell you how many times I‚Äôve seen finance teams come to me, over the moon excited about using ML to revolutionize their team. Only to have that same team fizzle out their ML usage over the next 2 months. ML projects take time, and momentum with business partners can be lost quickly. Something I‚Äôve realized is simply staying in contact with them, even as little as 1-2 times a month, helps move people from ML curious to ML power users rather quickly. It may seem like you might be bugging them too much but keeping ML at the top of their mind will help in prioritization.</li>
</ul>
<p>Back to Table of Contents</p>
</section>
<section id="talent" class="level3">
<h3 class="anchored" data-anchor-id="talent">Building data science talent in finance</h3>
<p>Hiring technical talent into the finance org is no easy feat. Here are a few challenges you have to overcome.</p>
<ul>
<li>Lack of defined career path. A data scientist working in finance will most likely not work for a data scientist manager. If they are lucky they might work for a software engineering manager, but I haven‚Äôt seen that happen too many times in Microsoft finance. This makes it hard to attract strong data science talent and get them to stick around for many years. If they keep getting promoted there are not many other jobs they can rise up to and take. The more successful they get, the more likely they will have to move on outside of finance to grow their career. And that‚Äôs ok. So make sure you cultivate the career path of your data scientists, even if that means leaving your team.</li>
<li>Lack of mentors. Because there aren‚Äôt a lot of data scientists in finance, there is often no one they can go to for help. For example, I‚Äôve never had a boss review my code. Not even once. So to know if what I built is any good I needed to go consult the opinions of others. Sometimes these can be other data scientists in the finance org, but often I‚Äôve had to go outside of finance to other mature data science teams to get advice and feedback on my work. So for your data scientists, make sure they are connected to a larger data and AI community at your company. And if there isn‚Äôt one, ensure they are finding community outside of the company at conferences and online spaces.</li>
<li>Lack of data engineering. <a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage in, garbage out</a>. If there isn‚Äôt good data, no amount of data scientists can create value for you. Even worse, don‚Äôt rely on a data scientist to fix your data problems first before they start training models. Building robust data infrastructure in finance should be the job of data engineers, not data scientists. Those two jobs cannot often be combined into one, because they require different skill sets and even different mindsets. So make sure you get your data house in order first before even thinking of getting data scientists.<br>
</li>
<li>Lack of complexity. Sometimes data scientists can get lost in building the coolest thing possible, instead of building something that helps the business. For example, a data scientist might use the latest hot thing from the world of deep learning to create a solution for the business problem, when in reality a simple linear regression could have taken 1/10th the time and give you the same results. Also when explaining their work to the end user in finance, they might dive right into the technical aspects of the project, and all of the cool statistics they performed to save the day and create something awesome. This kind of explanation works well with other data scientists, but it will scare off every finance person I know. One of the main jobs of a data scientist in finance is to abstract away all of the complexity of the craft and make their work simple to understand for the average finance person. It may feel like the data scientist is ‚Äúdumbing down‚Äù all of their hard work, but this part is crucial. If the end finance user cannot understand what they are being given from the data scientist, then they will not trust it, which means they will not use it. So gone are the fun technical presentations showing off all of the bells and whistles of their work. Now all they need to do is explain in simple terms if what they built worked, maybe how it worked under the hood (simplest explanation possible), and how the finance end user can leverage the work to do their job better.</li>
</ul>
<p>Ok, now that we know the challenges we must overcome to grow strong data scientist talent, here are some more tips around making data scientists successful in finance.</p>
<ul>
<li><a href="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1">Borrow, rent, then buy</a>. If you are starting from zero. Then I recommend trying to borrow a data scientist from elsewhere in your company. Then once you‚Äôve gotten a few projects off the ground rent some outside vendors/contractors to keep the work going. Then once you‚Äôre really kicking butt either hire those vendors as full time employees or go out and hire other data scientists in full time positions.</li>
<li>Start small. If your team has the budget to hire 5 data scientists, maybe just hire two for now and see how it goes. Then ramp up over time. When new technology is booming, like the current generative AI wave, people have the tendency to overinvest. So start small and grow over time. Having too much work for your 1-2 data scientists to do is way better than having to lay off 1 of your 5 data scientists because of budget cuts that always seem to happen every few years.</li>
<li>Connect with mentors outside of finance. Make sure your data scientists have a community of like minded people whom they can share their work with and get feedback. This will most likely be with data scientists in other departments like marketing, sales, and product teams.</li>
<li>Always answer the ‚Äúso what‚Äù question. Once you hear this concept you cannot unhear it. A boss of mine recently started asking me one simple question after I told him about a project I recently worked on, ‚Äúso what‚Äù? I recently moved our model training infrastructure to a new Azure resource, so what? I‚Äôm working on improving the model ensembling techniques used in our ML forecasting models, so what? I‚Äôve been asked to present to some outside customers about the work we do in Microsoft finance, so what? It‚Äôs a simple question but also a powerful one. If you cannot answer the so what question, you probably should go work on something else. Often the answer to the so what question boils down to a metric or number to measure impact of the work. If you cannot convey that to your manager, then you might be in trouble of making yourself busy instead of productive. Always be able to answer the so what question with data or customer testimonials.</li>
<li>Hire for more applied ML, not research ML. It‚Äôs better to hire someone who has spent their time putting ML solutions into production than someone who just got their PHD in deep learning. There is a difference between applied ML and research ML. If you do hire a PHD to work on ML stuff, they most likely will not be happy, this goes back to the lack of complexity I called out earlier. We‚Äôre not building rockets in finance, often times we <a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">don‚Äôt even need deep learning</a> to get the job done. So this kind of work may not entice the PHD to stick around long. Instead hire someone who can bring data science to business expertise, which brings me to my next point.</li>
<li>Convert financial analysts into data scientists. The perfect scenario is to take existing financial analysts, who are already experts in the business, and either give them tools that turn them into data scientists without the code. Or actually have them take the red pill and go all the way down the ML rabbit hole, becoming a true data scientists. This fixes most problems. Your data scientist already has strong domain expertise in the business, they can already speak like a normal person to business partners without all of the technical jargon, and they‚Äôve only ever known applied ML so they won‚Äôt waste time trying to build custom models from scratch. In my opinion it‚Äôs a no brainer and something that should become the standard going forward.</li>
</ul>
<p>Back to Table of Contents</p>


</section>
</section>

 ]]></description>
  <category>finance</category>
  <category>machine-learning</category>
  <category>forecasting</category>
  <guid>https://mftokic.github.io/posts/2024-08-12-ml-fcst-faq/</guid>
  <pubDate>Mon, 12 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-08-12-ml-fcst-faq/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Microsoft Finance ML Forecasting Journey: Part Three</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/image.png" class="img-fluid"></p>
<p>This is a multipart series:</p>
<ul>
<li><a href="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/">Part One</a></li>
<li><a href="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/">Part Two</a><br>
</li>
<li><a href="https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/">Part Three</a></li>
</ul>
<p>By now you should know how we started our machine learning (ML) forecast journey and how we applied it to Microsoft‚Äôs largest forecast process. But the fun doesn‚Äôt stop there. We were able to transform the biggest forecast process, but not every forecast process. There are hundreds more forecast processes in finance that are still in the dark ages. Essentially people with paper and pen creating these forecasts (written down inside excel models). Not knowing the potential ML can have on their job. All of these forecasts are important, but cannot scale in centralized tools like we discussed in <a href="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/">part two</a>. Something else had to be done. Centralizing processes can help solve many problems but not all problems. Sometimes you have to build democratized tools that give the power back to the people. That‚Äôs exactly what we did with ML forecasting. Keep reading to find out how.</p>
<section id="total-addressable-market" class="level3">
<h3 class="anchored" data-anchor-id="total-addressable-market">Total Addressable Market</h3>
<p>There are around 5,000 full time employees who work for Amy Hood, the CFO of Microsoft. They do a lot of different jobs. Some are considered finance roles while others are not. About 40% of these employees do some sort of predicting the future. This is the ‚Äúplanning‚Äù in ‚Äúfinancial planning and analysis‚Äù roles in corporate finance. These predictions are often around future financial metrics. Like how much revenue a product will make next month or how much headcount we will have on a specific engineering team. Let‚Äôs run the numbers to see how much time, and in essence money, is spent doing this one simple job of forecasting.</p>
<ul>
<li>5,000 finance employees</li>
<li>40% create forecasts</li>
<li>Spend ~4 days a quarter creating forecasts (very conservative number)</li>
</ul>
<p>Doing the math this equals around 24,000 days of human effort spent every year forecasting (24,000 = 5,000 x .40 x 12). If the average finance headcount costs Microsoft $250,000 year (salary, benefits, office space, etc) then the total cost of forecasting is around $24,000,000 ($1,000 per day x 24,000 days). Yep, that‚Äôs 24 million a year just crunching numbers in excel. This is also a pretty conservative estimate. I know some teams who spend weeks every quarter just forecasting, so it could easily be higher.</p>
<p>Think of this 24 million as the total addressable market (TAM) for forecasting in Microsoft finance.</p>
<p>The largest forecast process we discussed in part two only saves 10% of this 24 million. So we have a long ways to go in making a dent in this TAM. Unfortunately we cannot create 9 more centralized forecast solutions to cover the TAM. There is a long tail effect here, where the last 50-60% of forecasting might be done by 100+ forecast processes. We simply cannot create ML forecasts for everyone. There are not enough data scientists and data engineers needed to do that.</p>
<p>But hang on a second, what if we flip the script on that idea? Instead of having a dozen data scientists and data engineers maintain 100 ML forecast solutions. What if we have 100 regular finance people maintain their own single ML forecast? This is how a tool called ‚ÄúFinn‚Äù was born.</p>
</section>
<section id="self-serve-ice-cream-prototype" class="level3">
<h3 class="anchored" data-anchor-id="self-serve-ice-cream-prototype">Self Serve Ice Cream Prototype</h3>
<p>I graduated <a href="https://mftokic.github.io/posts/2024-02-19-frp-journey/">Microsoft‚Äôs Finance Rotation Program</a> (FRP) in the fall of 2018. My full time post program role was on the same business intelligence team as my fourth rotation. Basically I stuck around until they gave me a job. In that job I supported the Bing finance team. I helped them build ML forecasts for things like search volume and revenue on the Bing platform. It was my first true ML job where I wrote code and had legit business partners. It was awesome. Writing code to train models each day was a dream come true.</p>
<p>Initially I wrote code to create models to forecast search volume. Pretty soon that spread to trying to calculate search rates, or how much money we could make off ‚Äúx‚Äù amount of search volume. This PxQ approach would be used to get to search revenue. I created monthly forecasts, then weekly, then daily. Each time I had to rewrite the code to purpose fit it for the specific task at hand.</p>
<p>Pretty soon I started to turn the custom code written for each forecast task into reusable components that could be used across all forecast projects. Just take the code and plug ‚Äôn play with new historical data. It was fun to build and I was learning a lot. This meant that I was always on the hook for producing new forecasts though. Each time the Bing finance team needed an updated forecast, I had to be there to run it, create a report, and send it to them. I was now the data scientist training the ML models, the ML engineer serving the final outputs, and also the PM creating the final report and working with the business partner. I quickly learned that this was impossible to scale.</p>
<p>What really cemented the scale issue was when another team, called Device Market Intelligence (DMI), heard about the work in Bing and wanted it applied to their market analytics of the Windows PC ecosystem. I was able to take the reusable code and apply it to DMI PC shipment forecasts, but I was still the human in the loop. Always on call to update forecasts and make tweaks as I got business partner feedback. I needed to scale myself.</p>
<p>It was around this time that I was at my desk having just come back from lunch. I was thinking about ice cream, and how awesome those self serve ice cream machines are. Where you can make the worlds largest ice cream cone or a huge sundae in a bowl. As a random thought I wrote down ‚Äúself serve machine learning‚Äù on a sticky note and stuck it on my computer monitor. I didn‚Äôt give much thought to it, but thought it was cool enough to write down to make sure I didn‚Äôt forget it.</p>
<p>Months rolled by. I continued to run ML forecasts for both the Bing and DMI finance teams. Each time manually pulling the data and kicking off ML runs. Then sending them the outputs via excel report. It was at this time that I spoke with a coworker about an automation project that blew my mind. They were able to create an API that could kick off a large forecast process on demand. Without having to manually run code. Basically you press a button and the rest of the process takes care of itself. This started to turn the wheels in my head. What if I could take my reusable ML forecasting code and put it behind this kind of automated process?</p>
<p>I couldn‚Äôt stop thinking about it. How could I create a way to let someone kick off a ML forecast run without needing to come talk to me? Could they pull their own historical data, upload it inside of some tool, then an hour later get back a forecast they could use? I had no clue how to do this but wanted to figure how.</p>
<p>Over a few months I was able to put together a god awful prototype. One that allowed a user to fill out an excel template that had tabs to add their historical data and tabs to control certain inputs like forecast horizon. They could then upload that file into a Microsoft Power Automate flow. This flow would take their file, copy it to a shared server, then kick off an API that would run the ML forecast process. After it finished running the user would get an email with a link to where they could download the ML output results. It was all built with chewing gum and duct tape, but it worked. It actually worked. I felt like a super hero.</p>
<p>All great tools need a name. Most things at Microsoft are acronyms, which everyone hates. There is literally an internal company acronym lookup tool because there are so many. So I wanted to create a name that wasn‚Äôt an acronym, but also kind of described what it did. In essence it was a tool for financial forecasting. I settled on a one syllable word that kind of had finance in the name. I called it ‚ÄúFinn‚Äù. When I told this to my manager, they said ‚Äúare you sure‚Äù? And ‚Äúok we can always update the name later‚Äù. The name stuck and is still around today. Sadly people still think it‚Äôs an acronym.</p>
<p>I rolled out an early beta of Finn at Microsoft‚Äôs company wide hackathon in July 2019. Finance employees bravely signed up to be my guienne pigs for three days. They would stop by, bring some historical data they want forecasted, and attempt to use the tool. They got confused at times. And even more times the tool broke on a corner case with their data. It was glorious. Fixing bugs on the fly and seeing people use something I‚Äôve built, even be a little excited about it, was like nothing I‚Äôve ever experienced. I knew this was it for me, I had to make this work.</p>
<p>The hackathon was a medium success. Less than 10 people showed up, but those who did gave amazing feedback that I used to make the tool better. I was now ready to officially launch it to the masses. Before the big launch, I demoed it to my BI team, and people laughed. Yep, laughed. People said it was too complex and no one in finance would follow the steps needed to submit a forecast run. It was hard to hear but good feedback. I continued to iterate.</p>
<p>By the fall of 2019 Finn was ready for launch. We held training sessions, presented at CVP level all hands meetings, and basically told everyone I knew that they could start using Finn. Machine learning was still so new to people, so there were always people willing to try it out. With each passing month I would gather feedback, making improvements, and slowly usage grew.</p>
</section>
<section id="scaling-up" class="level3">
<h3 class="anchored" data-anchor-id="scaling-up">Scaling Up</h3>
<p>Finn was an interesting tool with a terrible UI. The UI was an excel template uploaded through a Power Automate flow. It wasn‚Äôt pretty but it worked. The tool needed a facelift, something to take it to the next level. Thankfully at this time my BI team was working on another tool called ‚ÄúReplay‚Äù. It was touted as an internal portal for custom built apps specific to Microsoft Finance. The first big feature was going to be centralized reporting. They were also interested in adding other apps into the mix. We pitched them Finn and thankfully they agreed to add a Finn app.</p>
<p>The newest Finn app launched alongside the new Replay site in March 2021. People could now go to a legit website, upload an excel or csv file, click through a UI to adjust various inputs, then click submit. The ML magic would happen behind the scenes. Then they could come back to the site and see when their ML run finished. Even download the output files. A user could submit a ML forecast run in 10 clicks of their mouse. It was magnificent.</p>
<p>In order to make this happen we had to completely rebuild the ML backend of Finn. Moving away from on-prem servers and into the cloud on Azure. This involved things like data lakes and Azure Machine Learning. We were able to migrate the Finn API to an Azure ML pipeline. Allowing for better scale and monitoring. The core modeling code also got an upgrade. We partnered with the Chief Economist team within Microsoft Research to make the ML brain of Finn more robust. The data scientists on the Chief Economist team were amazing mentors to me. I learned years worth of knowledge within months. It goes to show how a good mentor can make all the difference.</p>
<p>After giving the core ML code a makeover, we realized that we could package it up and make it open-source. Allowing other finance teams at other companies to use the exact same forecast process we use at Microsoft. This was released as an R packaged called <a href="https://microsoft.github.io/finnts/index.html">finnts</a>. Anyone can now take the code off the shelf and use it, free of charge.</p>
<p>We also realized that not everyone wants to use the self-serve UI to get a forecast. Sometimes people still want to use code, so we built a more general purpose API that allows anyone to call Finn on demand via API and give them a forecast on their specific data lake. Various engineering teams in Treasury, FinOps, etc now call Finn via API to produce forecasts on demand and at scale. This is how the Fusion tool mentioned in <a href="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/">Part Two</a> uses Finn today.</p>
</section>
<section id="future-state" class="level3">
<h3 class="anchored" data-anchor-id="future-state">Future State</h3>
<p>Finn continues to change almost on a monthly basis. There are hundreds of things we want to add or improve in the tool. Here are a few that come to mind.</p>
<ol type="1">
<li><strong>Forecast Accuracy</strong>: Always a top priority. No one will use a ML forecast if it‚Äôs less accurate than the previous manual forecast. There are countless ways we can make Finn more accurate. Thankfully all of them will be made available in the open-source <a href="https://microsoft.github.io/finnts/index.html">finnts</a> package.</li>
<li><strong>Model Interpretability</strong>: Once finance users get the level of accuracy needed, their next question is always around knowing how these models created the forecast. This is our next top priority that we are now starting to work on.</li>
<li><strong>Forecast Adjustments</strong>: ML forecasts may not capture everything about the future. There might be changes in business strategy, new product launches, or tax changes that could affect the forecast. This is where human domain knowledge comes in the form of manual adjustments. We want to create a way for ML to do the initial 80% of the work, and allow humans to come in for the last 20% and easily make and track manual forecast adjustments when needed.</li>
</ol>
</section>
<section id="lessons-learned" class="level3">
<h3 class="anchored" data-anchor-id="lessons-learned">Lessons Learned</h3>
<section id="paradigm-shift" class="level4">
<h4 class="anchored" data-anchor-id="paradigm-shift">Paradigm Shift</h4>
<p>Going from a manual forecast done in excel to a ML forecast requires a large paradigm shift in the brain of a finance person. You‚Äôre going from a deterministic process, where you know all the inputs and formulas used to create the forecast. To a probabilistic one, where there is this black box that takes in data and spits out a forecast. There are ways to explain what‚Äôs going on in the black box, but they are not perfect. You will not be able to trace through a ML forecast step by step like you can with a good excel model. So finance people need to change their mindset and know the potential tradeoffs of using ML. It boils down to a change in control. From letting a human control the entire forecast to now letting a machine control most of the process. As AI technology gets better, we will all turn into a form of a manager. Instead of managing people, we will manage little AI employees (or agents) to do our work for us. The sooner you embrace this, the bigger the impact you will have going forward into this AI future.</p>
</section>
<section id="duplication-of-work" class="level4">
<h4 class="anchored" data-anchor-id="duplication-of-work">Duplication of Work</h4>
<p>Most of the time people do not stop their manual forecast process and immediately switch to a 100% ML powered forecast. Often there needs to be overlap between the old process and the new process. This can be a few months (or even years) where ML is ran alongside the manual forecast process. With ML serving as a triangulation point until the team is ready to make the switch. This creates a duplicate of effort. Finance people barely have enough time to do their current job to sometimes learn a new way of doing it. That‚Äôs when senior leadership needs to kick in to help their teams prioritize using ML. Not just as a nice to have but to eventually change how their jobs get done. To do that brings us to the next lesson, taking the leap of faith.</p>
</section>
<section id="leap-of-faith" class="level4">
<h4 class="anchored" data-anchor-id="leap-of-faith">Leap of Faith</h4>
<p>You know that <a href="https://youtu.be/DjffIi2Pl7M?si=UWMjRJphxhIKnY-u">scene in the Dark Knight Rises</a> where Batman escapes the third world prison? The reason he escaped was because he literally took a leap of faith, without the rope. Using ML as a triangulation point is like jumping with the rope attached to you. You will never truly have ML help you because you can always fall back on the manual forecast. I‚Äôve seen some teams use Finn for years but only ever as a triangulation point. In order to truly get the benefits of ML, you need to take the leap of faith. Jump without a rope. Ditch the manual forecast. It seems scary at first, but soon you‚Äôll be laughing at how much time you spent forecasting each month.</p>
<p>The best way to take a leap of faith is having senior leaders give you the kick in the butt to make the jump to full ML. If senior leaders aren‚Äôt expecting to see ML usage grow on their team, adoption will slow to only those employees brave enough to jump by themselves. That‚Äôs a small number. Get higher ups to buy in and lead the way.</p>
</section>
<section id="invest-in-youth" class="level4">
<h4 class="anchored" data-anchor-id="invest-in-youth">Invest in Youth</h4>
<p>It‚Äôs a lot easier to teach someone fresh out of school to use ML compared to a senior employee who has built excel models for 20 years. Younger people are more open to technology. They have no bad habits to unlearn. They don‚Äôt know any other way of doing things. These are the people you want to equip with ML tools. If they use ML in their initial jobs, once they become a senior executive they will make sure ML use becomes standard on their teams. This is kind of like planting trees. It takes time to see results, but one day you will have an entire forest of ML experts leading the charge.</p>
</section>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>This wraps up our three part series on ML forecasting in Microsoft finance. I started at the beginning with our first ever solution in <a href="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/">Part One</a>, then discussed how we built tools to centralize the forecasting process in <a href="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/">Part Two</a>, and finally I wrapped up with telling you how we decentralized ML to everyone in Finance in <a href="https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/">Part Three</a>. If you‚Äôd like to use the same forecasting techniques as we do at Microsoft, check out our free forecasting package called <a href="https://microsoft.github.io/finnts/index.html">finnts</a>.</p>
<p>May the MAPE ever be in your favor. Happy forecasting!</p>


</section>

 ]]></description>
  <category>finance</category>
  <category>machine-learning</category>
  <category>forecasting</category>
  <guid>https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/</guid>
  <pubDate>Mon, 15 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/image.png" medium="image" type="image/png" height="82" width="144"/>
</item>
<item>
  <title>Inner Circles of Life</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-06-28-life-circles/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-06-28-life-circles/image.png" class="img-fluid"></p>
<p>One of my siblings recently got married and had a baby. As they were living this new life, myself and other family members realized something. They now had less time for us. Instead of coming around to the standard family events like they used to, they now had to ‚Äúsqueeze us in‚Äù between other things going on with their life. This become the most apparent around Christmas time. My sibling and their family now only stopped by for a few hours around the Christmas holiday, and on Christmas day this meant only seeing them for 2-3 hours. In previous years we would have been together nonstop. This broke my Mom‚Äôs heart. She could no longer be with all of her kids 24/7 during holidays and major life events.</p>
<p>It was during this Christmas that I realized something. My sibling had to deprioritize us for their spouse and eventually their new child. And that‚Äôs perfectly ok. They now had a new top priority in life, and myself and other family and friends are now lower down on the list. A new inner circle of priorities formed for them, and myself and others were no longer in it. We had to make due with this new reality and understand it could get worse in the future. This kind of deprioritization also happens with friends too. It just gets hard to prioritize people in your life as new people come into it.</p>
<p>The list of life priorities is something I like the call the ‚Äúinner circles of life‚Äù. Sounds fancy but it‚Äôs just a list of what you truly prioritize and make time for in life. Your inner most circle is your top priority. Then circles form around and outward. With each new circle, your priorities of things in that circle drops. Let‚Äôs see how that changes as we get older.</p>
<p><img src="https://mftokic.github.io/posts/2024-06-28-life-circles/image2.png" class="img-fluid"></p>
<section id="age-0-1" class="level3">
<h3 class="anchored" data-anchor-id="age-0-1">Age 0-1</h3>
<ol type="1">
<li><strong>Mom</strong></li>
</ol>
<p>When you‚Äôre born, the only person that exists is your mother. Dad, who‚Äôs Dad? What‚Äôs a Dad? You have no idea. The only person you recognize and bond with is your Mom. No one else comes close.</p>
</section>
<section id="age-1-6" class="level3">
<h3 class="anchored" data-anchor-id="age-1-6">Age 1-6</h3>
<ol type="1">
<li>Mom</li>
<li><strong>Dad</strong></li>
<li><strong>Grandparents, Aunt/Uncle</strong></li>
</ol>
<p>Ok, now Dad comes into the picture as the second circle of your life, but Mom still holds the inner most circle. You also now get to know there are other people who love you unconditionally. These people are called Grandparents and they‚Äôre awesome. All they do is give you hugs and tasty food, life is great when they are around. Aunts and Uncles come into the picture to. They kind of look like Mom and Dad but smell different and always seem to get out of having to change your diaper. Lucky them.</p>
</section>
<section id="age-7-12" class="level3">
<h3 class="anchored" data-anchor-id="age-7-12">Age 7-12</h3>
<ol type="1">
<li>Parents</li>
<li><strong>Friends</strong></li>
<li><strong>Siblings</strong></li>
<li>Relatives</li>
</ol>
<p>Your parents still keep the top spot, but now they are more like a combined unit. Not just separate people but a singular force. Now they are telling you to do chores and keep an eye on your brothers and sisters. Who the heck are they? These people look like me but have different interests and personalities. And hey, they‚Äôre mean! We don‚Äôt get along that well. We always fight. So they are definitely farther down on the list. Same goes with Grandparents and other relatives. We still enjoy seeing them, but we‚Äôd rather hang out more with these people who go to school with you. People your own age who have the same interests as you. Who you see every day at school. People who give you their pudding cup just because you looked hungry. These people are your friends, and they are the best. They now take a higher spot on the list. Life starts to revolve more around your friends and less around your other family members.</p>
</section>
<section id="age-13-25" class="level3">
<h3 class="anchored" data-anchor-id="age-13-25">Age 13-25</h3>
<ol type="1">
<li>Friends</li>
<li><strong>Romantic Partners</strong></li>
<li>Siblings</li>
<li>Parents</li>
<li>Relatives</li>
</ol>
<p>This is what I have come to call the ‚Äúdark decade‚Äù. A time where you kinda suck as a person. You might hate your parents. You might hate school. You might just hate the world. Don‚Äôt worry that‚Äôs just your emo phase, it‚Äôll pass. Friends become your top priority in life. School might be on that list too but it most likely won‚Äôt come until your college years, so I‚Äôve left it off the list for now. There are now others who might have initially looked like friends. But who you now see in a different light. They smell nice. They have shiny hair. They seem cool. You‚Äôd like to get to know them more. Maybe even kiss them right on the mouth. Yes, these are people I call romantic partners. Hopefully they do not occupy the top spot on the list. You know the saying, bros before ____ right? Wrong. At times a girlfriend, boyfriend, or someone you admire (who might not know you even exist) could take the top spot. You might even go to a specific college on the other side of the country for them. This dark decade is where mistakes happen. Where you fail a lot. Do dumb things. Thankfully all of this dumb stuff happens when you‚Äôre at a school of some sort, so the mistakes are temporary. These mistakes are things you learn from and grow into a better person (hopefully).</p>
</section>
<section id="age-22-26" class="level3">
<h3 class="anchored" data-anchor-id="age-22-26">Age 22-26</h3>
<ol type="1">
<li><strong>Job</strong></li>
<li>Romantic Partners</li>
<li>Friends</li>
<li>Parents</li>
<li>Siblings</li>
<li>Relatives</li>
</ol>
<p>Now you‚Äôre in the real world. And you need money to live. Work has now become your top priority. Don‚Äôt believe me? How many of your friends took jobs in different cities after college graduation? Did you break up with your romantic partner because you both were headed to different cities to start your careers? Yup, it happens. It‚Äôs ok to have your job be the top priority. You need to establish yourself at this stage in life. Build a career that‚Äôs going somewhere. You also might be in a serious relationship with someone who smells nice and has shiny hair. Lucky you. This person might even move in with you. Your roommate used to be your best friend. Now you‚Äôve kicked them out for a different kind of best friend, one you may want to spend every day of the rest of your life with. Friends are still high up on the list, but they might live in a different city now. You take trips to visit them, but you only have so many vacation days off work. You also need to balance that with time to see your parents and other family members. Now you have too many life balls in the air to juggle, so some might get dropped. When was the last time you called your Grandparents? Call them now.</p>
</section>
<section id="age-25-35" class="level3">
<h3 class="anchored" data-anchor-id="age-25-35">Age 25-35</h3>
<ol type="1">
<li><strong>Spouse</strong></li>
<li>Job</li>
<li>Parents</li>
<li>Siblings</li>
<li>Friends</li>
<li>Relatives</li>
</ol>
<p>By now you might have married your roommate who smells nice. Where you live and sometimes changing jobs are based on this other person. They are now the center of your world. Having a job and good friends are still high up on the list, but those fall by the wayside compared to your new spouse. You are now out of the ‚Äúdark decade‚Äù, so naturally your parents and siblings become fun again. You genuinely enjoy hanging out with them. And miss them when they‚Äôre not around. Now that you‚Äôre starting to see your family more, and your job responsibilities are heating up, all of a sudden you can‚Äôt see your friends +3x a week. Some of your friends might move away, back to their or their spouse‚Äôs hometown. It happens, and it kinda sucks. But that‚Äôs life. They‚Äôre also dealing with their own priorities just like you.</p>
</section>
<section id="age-28-38" class="level3">
<h3 class="anchored" data-anchor-id="age-28-38">Age 28-38</h3>
<ol type="1">
<li><strong>Kids</strong></li>
<li>Spouse</li>
<li>Job</li>
<li>Parents</li>
<li>Siblings</li>
<li>Friends</li>
<li>Relatives</li>
</ol>
<p>The most beautiful thing in the world happens. A baby comes into your life. Everything else is meaningless. The only the thing that matters is making sure this child is happy and healthy. Your kids become your inner most circle. Everything else gets bumped down the list of priorities. You now realize you need baby sitters, because eventually you might have to return to work. This is where parents and siblings come in. Now you‚Äôre closer than ever with them. Friends visit you, but that weekly poker game or all night weekend party is now out of the question. You have someone to feed and someone to love. Life is beautiful. Work falls on the list too. Late night and weekend working sessions become harder. Now you have to tradeoff time at work with time with your child. This becomes a hard choice that has been argued thousands of times by smart people. The answer is hard. Thankfully by now you have built up some career capital. Meaning you can use your seniority and expertise at your company to guard your time more. Only work on the biggest impact items instead of the grunt work you did at the start of your career. They say you can have everything in life, just not all at once. Choices have to be made. Just know the tradeoffs of each one. Make sure you define your own definition of success in life. Which can be truly anything. No one has the right answer. No one has it all figured out.</p>
</section>
<section id="age-35" class="level3">
<h3 class="anchored" data-anchor-id="age-35">Age 35+</h3>
<ol type="1">
<li>Kids</li>
<li>Spouse</li>
<li>Parents</li>
<li>Siblings</li>
<li>Friends</li>
<li><strong>Job</strong></li>
<li>Relatives</li>
</ol>
<p>Hopefully by your mid to late thirties you are able to find a nice smelling person. Maybe even raise some rugrats. By this time you have also built up a lot of career capital. Maybe this means you can now do your job how and when you‚Äôd like. Maybe your job is still demanding most of your time. Good news, things will only get worse. More people will ask for your time. Pull you in a thousand directions. Ask you to do more. Then more. Then once you get all of that work done, your reward is more work. Congrats! Maybe you tell yourself you‚Äôll retire early. That way you can then have more time to spend with your family. Like I said before, I don‚Äôt know the right answer. I don‚Äôt think anyone does. So again I‚Äôll say that life is all about priorities and tradeoffs. How you define success in life could be different than someone else. And that‚Äôs ok. I think in a perfect world your family and friends are still high on the list as you get older. You can spend more time with them, and maybe less time on that job. Or maybe your job fulfills you immensely. You know the work contributes to making the world better. So working more is a worthwhile tradeoff. Do Presidents of nations feel bad that they cannot spend time with their family every day? Maybe they do, maybe not. For me I don‚Äôt think retiring to a beach for the rest of my life is any fun. I‚Äôd like to be like Charlie Munger, working into his 90s. I assume he wasn‚Äôt working nights and weekends in his 90s. Instead he still worked, but also made time for other relationships in his life. Again, there‚Äôs no right answer.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>When we‚Äôre on our deathbed. I don‚Äôt think any of us will say ‚ÄúI wish I worked harder‚Äù. The quality of our life boils down to the quality of our relationships. The quality of your relationship with your kids, parents, siblings, friends, and extended family. Relationships at work can serve a purpose too, but it‚Äôs hard for a job to replace these other types of relationships. You can have everything in life, just not all at once. Everything comes with a tradeoff. In the end, define what success looks like to you and have zero f#### for anyone else who tells you how to live your life. Prioritize accordingly.</p>


</section>

 ]]></description>
  <category>life</category>
  <guid>https://mftokic.github.io/posts/2024-06-28-life-circles/</guid>
  <pubDate>Fri, 28 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-06-28-life-circles/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Microsoft Finance ML Forecasting Journey: Part Two</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/image.png" class="img-fluid"></p>
<p>This is a multipart series:</p>
<ul>
<li><a href="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/">Part One</a></li>
<li><a href="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/">Part Two</a><br>
</li>
<li><a href="https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/">Part Three</a></li>
</ul>
<p>The success of Microsoft finance‚Äôs first machine learning (ML) forecast spread like wildfire throughout finance. The ML forecast was shared with all finance leaders. So naturally knowledge of ML‚Äôs potential trickled down to more people across the organization. Eventually the news came to a team in central finance (CFT). Think of this team as Microsoft‚Äôs core FP&amp;A team across the entire company. After seeing the accuracy at a worldwide level, this team knew it could help in the biggest forecast process at Microsoft. Something called the commercial field forecast. This forecast is created by finance members who sit in the ‚Äúfield‚Äù. The field is just a cool way to say regional offices all around the world. These field finance teams support the sales teams who also sit in the field. How could they take a worldwide forecast by product and break it down into specific countries all across the world? Well buckle up gang, it‚Äôs time to find out! This is how a tool called ‚ÄúCommercial Predict‚Äù was born in 2017.</p>
<section id="how-things-used-to-work" class="level3">
<h3 class="anchored" data-anchor-id="how-things-used-to-work">How Things Used to Work</h3>
<p>Before we dive into all the ML goodness, we have to understand how the old way used to work. I know, it‚Äôs kind of like eating your vegetables. But we just have to do it real fast then we can get to the fun parts.</p>
<p>In the past, each finance team in the field was responsible for their own forecast each quarter. These forecasts would happen in ‚ÄúCFO forecast cycles‚Äù. With cycles happening in October, January, and April. Microsoft‚Äôs fiscal year runs from July - June so these forecast cycles happen at the start of Q2, Q3, and Q4. The forecast at the start of Q1 is budget (that‚Äôs a story for a different day). Each cycle, a forecast would be created for the remainder of the fiscal year.</p>
<p>Microsoft sells products in over 100 countries. Most of those countries have a sales team that tries to sell products to companies in that geographical region. If there‚Äôs a sales team, then there is a finance team who supports them. This means there are dozens of sales finance teams creating quarterly forecasts for the rest of the fiscal year each CFO forecast cycle. Each team had their own secret recipe of how the forecasting was done. Often a custom excel model that would create the forecast. This model needed to be handled with care. Since each quarter it would have to be rolled over and prepared for the next forecast cycle. Anyone who has ever created and owned a financial model in excel knows the anxiety faced with trying to build and maintain one. These models were complex, and you said a little prayer every time you opened the file. Hoping it wouldn‚Äôt crash your machine because it was so large.</p>
<p>Once each team in the field had their forecast for their geography, it would get sent up the food chain. Forecasts from each country would be combined to form higher level aggregations in Microsoft‚Äôs sales territories. Each aggregation added more countries and continents together. This continued until you got the total worldwide number for the entire commercial business. Each time the forecasts got combined together at a higher level, senior finance leaders had the opportunity to make adjustments to that forecast. Based on their domain knowledge of the business. Eventually the final forecast the CFO, Amy Hood, saw was something completely different than what was initially created by each sales finance team for their specific geography.</p>
<p>Layers upon layers of bias were added to the forecast. Some was good bias that could improve forecast accuracy, but often it was too many cooks in the forecast kitchen. Too many people touching a forecast that didn‚Äôt need to be touched. Resulting in worse accuracy and more confusion once the books were closed at quarter end. This process would take upwards of a month every quarter. From the initial forecast created by field team all the way up the food chain to the CFO. In the spirit of every good infomercial, ‚Äúthere just has to be a better way!‚Äù.</p>
<p>Now you know why finance had to do something different. Drastically different.</p>
</section>
<section id="excel-prototype-built-in-a-redmond-garage" class="level3">
<h3 class="anchored" data-anchor-id="excel-prototype-built-in-a-redmond-garage">Excel Prototype Built in a Redmond Garage</h3>
<p>All good things start from humble beginnings. The team in CFT wanted to centralize the field forecast process for the commercial business. Create a single way that everyone in the field would follow to create a forecast. To make this a reality, they started with the swiss army knife of every finance professional. That‚Äôs right. You guessed it. They started with excel. Like any innovative project, it quickly became their baby. And all babies need a name. The named it Commercial Predict.</p>
<p>The team created an excel prototype of a single model that every field finance team could use. It was a combination of the old and the new. First was old but reliable PxQ forecasts. Where you take what‚Äôs in the sales pipeline for a quarter and multiply it by how many deals on average have closed in similar historical quarters. Second was classic CAGR and year over year percentage growths, which actually still work quite well. These traditional methods were combined with more statistical rigor. Something more along the lines of machine learning. They built by hand, formula by formula, exponential smoothing statistical models. Which is a common model in time series forecasting. It‚Äôs more stats than machine learning, but still performed really well. Today exponential smoothing is a simple function call in excel, but this team built it from scratch. I tip my cap to them, because that was hard to do.</p>
<p>Now there were multiple forecasting methods in this mega excel model. The beauty of the idea is that someone could come into the model and choose what methods they wanted to use to forecast a specific geography, customer segments, and products. Users could even combine multiple methods together to get a more accurate forecast. This was powerful because someone could use the PxQ sales pipeline method for products that depended on big customer deals landing, and use the other methods for things that had more stable trends and seasonality.</p>
<p>It was genius prototype. The team was able to take this to their leadership team and show how one single approach to do forecasting in the field could save thousands of days of combined human effort across the field every year. One mega model to rule them all. It had the promise to cut forecasting down by 50%. Would it work though? To test it out, the team ran this excel model alongside the traditional bottoms up forecast process from each field team. They could then compare the results and even track accuracy across the old and new ways. The results were good. The new prototype was the same or even better than the existing process, but was 50% faster.</p>
<p>The new approach was fast, and it was accurate. The final roadblock before adopting the new approach was to get everyone in the field to agree on what level they should forecast at. This historically was a difficult subject to discuss, with everyone having differing opinions. Thankfully this was solved by getting the buy in from the top senior finance leaders in central finance and the field. Once that happened everyone was able to get on board.</p>
<p>Ok, so the team had a cool prototype that knew worked well. But if they just used that excel model, then they are still maintaining a model that is messy and requires constant upkeep. It would be hard to scale. They needed something more robust. A real tool that was built by engineers. Thankfully there was a team who could do just that.</p>
</section>
<section id="building-the-tool" class="level3">
<h3 class="anchored" data-anchor-id="building-the-tool">Building the Tool</h3>
<p>The vendor team who was created to take over the <a href="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/">initial ML forecasts</a> was the up to the task. They had the data science knowledge, but the data engineering and software engineering needed to build a software tool to scale out the excel prototype was missing. So the team got other vendors to fill in those gaps. Now there was a team of engineers all capable of making the tool a reality.</p>
<p>The first version of the production level Commercial Predict tool had to be built fast, before an upcoming CFO forecast cycle. V1 was built into excel within six months as an add-in field users could download and connect to. It needed to combine new machine learning methods with traditional PxQ and CAGR/YoY run rate methods.</p>
<p>Here‚Äôs how it worked.</p>
<ol type="1">
<li>Data engineers would pull historical revenue and sales pipeline data. All the forecasts methods were precomputed and saved in a database that was turned into a cube. This would be the starting point for all field members. Instead of calculating these forecasts by hand in their old excel models, it would be precomputed for them. At scale.</li>
<li>The forecasts in the cube were then served to users in a custom excel file. Each field team could come into the tool and select the geography, products, and segments they were responsible for forecasting. After making these selections, all the forecast methods would populate in the excel.</li>
<li>Users could then see each forecast method and see which ones pass their smell test of what they expect to happen in the business based on their domain knowledge. They could choose a specific forecast method to use, or combine multiple methods together to get a more robust forecast. Finally there might be things these forecast methods don‚Äôt know about. Like upcoming tax changes or product strategy changes. Field users could ultimately make manually adjustments to get a final forecast.</li>
<li>Once the final forecast was created, they could save it back to the cube. This allowed finance leaders to see the forecast creation in real time. Also it would prevent the classic ‚Äúexcel crash without saving‚Äù headache we‚Äôve all been through in the past.</li>
<li>Once the forecast was complete for each field team, a static output file was created at the touch of the button. Teams could take this output and load it into the final planning system.</li>
</ol>
<p>Before official launch, training sessions were held to make sure everyone knew how to use it. It was also a good opportunity to fix any bugs in the tool. This resulted in some late nights and even weekend shifts, but the job got done. The tool was launched on time and the rest is history.</p>
<p>We were able to go from a forecast process of 21 business days each quarter, down to just 10. It was a revolution. This saved Microsoft millions of dollars each year of human capital. Finance teams in the field could now forecast faster, with less headaches, and prevent the layering of bias that was a staple of the previous way.</p>
</section>
<section id="evolutions" class="level3">
<h3 class="anchored" data-anchor-id="evolutions">Evolutions</h3>
<p>After this officially launched in 2018, the Commercial Predict tool has gone through a lot of iterations. What started in excel then moved into a web based tool. Then back to excel. With each iteration, we got better at the machine learning methods. Better at adding more features to give users more control over the final forecast.</p>
<p>Eventually Commercial Predict evolved into a much broader solution called ‚ÄúFusion‚Äù in 2022. Think of it as a tool that could still do the commercial field forecast process but now also take on other forecasts within Microsoft finance. A true one stop shop for all things planning. Fusion is an excel add-in with a built in UI on the side of excel. Kind of like how excel copilot opens on the side of your excel tab, Fusion does the same. A user could select what forecast they want to do, select the parts they‚Äôre responsible for forecasting, and Fusion would populate the blank excel file with all the information they need to finalize their forecast. Methods like PxQ and machine learning are still ran ahead of time. The UI was truly dynamic. You could take any excel file and open the Fusion app inside to get going on the forecast. Fusion allowed finance to scale the learnings of Commercial Predict to so many other forecast processes. Improving the impact ML and centralization can have on forecasting.</p>
<p>Planning tools like Fusion will most definitely change in the future. As the business evolves, so should our way of forecasting it. What doesn‚Äôt change is how ML has become a central part of the forecast process.</p>
</section>
<section id="lessons-learned" class="level3">
<h3 class="anchored" data-anchor-id="lessons-learned">Lessons Learned</h3>
<section id="iterate-iterate-iterate" class="level4">
<h4 class="anchored" data-anchor-id="iterate-iterate-iterate">Iterate Iterate Iterate</h4>
<p>Rome wasn‚Äôt built in a day. Instead of building this complex centralized forecasting tool from the start, we started small. Built a prototype. Got senior leadership buy in. And continued to make it better every 6-12 months. Even as I write this we are in the process of improving the ML accuracy of the commercial field forecast. If you‚Äôre coasting, you‚Äôre going downhill. You need to continue to iterate.</p>
</section>
<section id="combine-the-old-with-the-new" class="level4">
<h4 class="anchored" data-anchor-id="combine-the-old-with-the-new">Combine the Old with the New</h4>
<p>ML didn‚Äôt outright replace every part of the commercial field forecast. Instead we combined ML techniques with older methods like PxQ sales pipeline methods. This allowed us to use the strengths of each approach based on what product was being forecasted. Some products are sensitive to large customer deals closing, so PxQ works best. Others have stable trends and seasonality, that‚Äôs where ML shines. Using both gives us the best of both worlds.</p>
</section>
<section id="senior-leadership-buy-in" class="level4">
<h4 class="anchored" data-anchor-id="senior-leadership-buy-in">Senior Leadership Buy In</h4>
<p>A forecast process is like a ship. The bigger the ship, the harder it is to change course. So the bigger the forecast process, the higher the buy in needed from a senior leader. Getting a GM or CVP level support allowed us to supercharge the change management. It‚Äôs easy to get bogged down in arguing with senior finance managers about how a forecast process should be done. Once a CVP (someone who reports to the CFO) comes in and says this is how we‚Äôre going to do it. Then everyone gets on board and starts turning the wheel of the ship together to change direction. The commercial field forecast had to get support from GM and CVP level leaders or else it would have taken years to change it instead of months.</p>
</section>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>The hardest part of any ML project comes down to people. Training models is easy, convincing people to use them is hard. It takes time. It takes senior leader buy in. It takes an open mind to rethink how your job can be done. It might be hard, but in the end it‚Äôs worth it.</p>
<p>Often we get asked by finance teams outside the company if they can take our ‚ÄúCommercial Predict‚Äù or ‚ÄúFusion‚Äù tool off the shelf and start using it at their own company for forecasting. Sadly you cannot. We build a lot of these custom tools because we don‚Äôt have a choice. Microsoft‚Äôs business is complex. Often we need custom solutions that are hard to standardize in external products. Thankfully the machine learning methods we use are available for free as an <a href="https://microsoft.github.io/finnts/index.html">open-source R package</a>. Check it out if you‚Äôd like to learn more.</p>


</section>

 ]]></description>
  <category>finance</category>
  <category>machine-learning</category>
  <category>forecasting</category>
  <guid>https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/</guid>
  <pubDate>Wed, 26 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Microsoft Finance ML Forecasting Journey: Part One</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/image.png" class="img-fluid"></p>
<p>This is a multipart series:</p>
<ul>
<li><a href="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/">Part One</a></li>
<li><a href="https://mftokic.github.io/posts/2024-06-26-msft-ml-fcst-journey-2/">Part Two</a><br>
</li>
<li><a href="https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/">Part Three</a></li>
</ul>
<p>Ever wonder how Microsoft Finance got started with machine learning? It didn‚Äôt just happen overnight. It started small and grew from calculated steps. In this post and a few others I want to tell the journey of how we got started. Gather round children! It‚Äôs story time.</p>
<section id="paradigm-shift" class="level3">
<h3 class="anchored" data-anchor-id="paradigm-shift">Paradigm Shift</h3>
<p>In the summer of 2015 AI and machine learning (ML) weren‚Äôt terms you‚Äôd hear every day. Maybe you‚Äôd hear the word ‚Äúbig data‚Äù being thrown around business circles but no one had a clue what it meant. There was a lot of data being captured about our world. Somehow we could ‚Äúmine‚Äù the data to get some value out of it. No one really knew.</p>
<p>Earlier that year, something interesting happened at Microsoft. A new product called <a href="https://techcrunch.com/2015/02/18/microsoft-officially-launches-azure-machine-learning-big-data-platform/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuYmluZy5jb20v&amp;guce_referrer_sig=AQAAAE95NY6ZC1q7S3u9eB-VRujlcx7hEFnu35ya9daztiyij3JgOLZWH9VqOfmNl6FuEi2KTbr67hu2aoyrIMUYcUGPoDhzEDsXRJ01LUAF0c-VUU_lHdUhGzLcW5FQYKZwXQJ93c3qzwocA3_WIi-5Z4VXcKJGnaD-1E40xMI6LRnB">Azure Machine Learning</a> was officially released. The service allowed anyone to start mining their data up in the cloud. You could train models and serve them through APIs. It was basically magic. Unfortunately in finance, those words meant nothing. To a Microsoft finance worker the term ‚Äútrain a model‚Äù meant training the new employee on building excel models. Everything was done by hand and with care. Especially forecasting our financial statements. The CFO of Microsoft, Amy Hood, thought differently. What if we could use the new product to improve some of the manual work we did in finance? Could we have these models be trained to forecast our business? It was a tough question. No one in finance at the time was really qualified to answer it. She had to go ask the expert.</p>
</section>
<section id="getting-the-ball-rolling" class="level3">
<h3 class="anchored" data-anchor-id="getting-the-ball-rolling">Getting The Ball Rolling</h3>
<p>Amy went to the legend himself. The head of Microsoft‚Äôs cloud, Scott Guthrie. King of the cloud and wearing red polo shirts. She wanted to see if Scott‚Äôs engineering team could help finance build machine learning models. Allowing finance to forecast the business. Thankfully Scott said yes and lent a few data scientists to help the finance org get off the ground with ML.</p>
<p>The big ticket item was forecasting revenue. Instead of starting small with one specific area we started very high level. Amy wanted a quarterly global revenue forecast by each of Microsoft‚Äôs major products. This forecast could be used internally to compare against the manual forecasts. Which are created by sales finance and product finance teams. The ML forecast could either confirm or contradict these bottoms up forecasts made by humans. Allowing finance to either adjust their forecasts. Or make sure they know why they are different than ML.</p>
<p>The results were strong. The ML forecast was around 1%-2% off on average, compared to the manual human forecast error of 2%-4%.</p>
</section>
<section id="keeping-the-ball-rolling" class="level3">
<h3 class="anchored" data-anchor-id="keeping-the-ball-rolling">Keeping The Ball Rolling</h3>
<p>The game officially changed. The finance team could now just rely 100% on ML going forward right? Not so fast! Who would keep training these models? What if we wanted to forecast at a more granular level? Scott‚Äôs data scientists couldn‚Äôt help forever. To fix this Amy had to hire some data science talent. People who knew what they were doing. Like the engineers on Scott‚Äôs team.</p>
<p>Hiring your first data scientist is a hard thing to do. Creating a career path for them in a non-technical team like finance makes it harder. As a first step, a team of vendor data scientists were hired. This was enough help to take the work done by Scott‚Äôs team and keep it going. Even expand it to other areas. The hope was to eventually turn a vendor data scientist team into a team of full time employees.</p>
</section>
<section id="lessons-learned" class="level3">
<h3 class="anchored" data-anchor-id="lessons-learned">Lessons Learned</h3>
<p>Going from zero ML work to your first forecast solution takes hard work and perseverance. Here are a few lessons Microsoft finance learned when starting out.</p>
<section id="borrow---rent---buy" class="level4">
<h4 class="anchored" data-anchor-id="borrow---rent---buy">Borrow -&gt; Rent -&gt; Buy</h4>
<p>Initially data scientists were borrowed from other teams at the company. Then they were rented from outside companies as vendors. Then finally once a strong data science practice was established after a few years, full time employees were hired. Many were vendors who turned into full time employees. This process was slow, but allowed finance the time to make sure a data science practice and career path could be built.</p>
</section>
<section id="whats-the-biggest-opportunity" class="level4">
<h4 class="anchored" data-anchor-id="whats-the-biggest-opportunity">What‚Äôs the biggest opportunity?</h4>
<p>The biggest opportunity to forecast with ML was revenue. We could have spread ourselves thin and tried to do the entire income statement. But we knew revenue was the hardest to forecast. So that‚Äôs where we started first.</p>
</section>
<section id="start-at-the-top-work-your-way-down" class="level4">
<h4 class="anchored" data-anchor-id="start-at-the-top-work-your-way-down">Start at the top, work your way down</h4>
<p>Starting first with worldwide revenue allowed finance to get good results without getting too deep into the weeds first. If we wanted to get an accurate daily forecast down to the sku level, that would have taken forever. Instead we started big and then eventually worked our way down. This process may not initially replace the manual forecast work being done. But it starts to get others in finance comfortable using ML in the decision making process. After finance leaders got used to seeing these ML forecasts, we could then start working on more granular forecasts that could replace more manual work.</p>
</section>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Ok now you know how the ML ball got rolling in Microsoft finance. Before reading this article you might have thought we had this amazing ML kick-off with millions invested in the space. We definitely did not. Instead we started small in areas that had the highest ROI and worked our way from there. If your company is just starting out on your ML journey, I suggest you do the same. Small, incremental change can compound into enormous impact over the long run. That‚Äôs the kind of change that lasts.</p>


</section>

 ]]></description>
  <category>finance</category>
  <category>machine-learning</category>
  <category>forecasting</category>
  <guid>https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/</guid>
  <pubDate>Wed, 12 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-06-12-msft-ml-fcst-journey-1/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>How To Master Storytelling</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/image.png" class="img-fluid"></p>
<p>Our dumb caveman brains can‚Äôt remember everything. There is too much information passing through our heads each day. The only thing that sticks are stories. Either ones we tell ourselves or ones we hear from others. Don‚Äôt believe me? Let‚Äôs play a game. Tell me what you had for lunch last week on Tuesday. Now tell me the plot to the first Star War‚Äôs movie. Ha gotcha. Stories stick. More than anything else in this world. Whoever can tell the best stories has the power to do great things in our world.</p>
<p>Shaan Puri is an entrepreneur and content creator. Known for his work in the tech industry and his popular podcast ‚ÄúMy First Million.‚Äù A few months ago he was on the popular <a href="https://www.youtube.com/watch?v=Z2BnqYArwaw&amp;list=PLrxUIdme2aVnB6z5BwgIJx0rJjKrl7Qlc&amp;index=12">‚ÄúHow I Write‚Äù podcast</a>. While on the show he blew my mind with tons of great ideas around storytelling. I learned a lot and wanted to share the best ideas here.</p>
<section id="aaron-sorkins-30-second-masterclass" class="level3">
<h3 class="anchored" data-anchor-id="aaron-sorkins-30-second-masterclass">Aaron Sorkin‚Äôs 30 Second Masterclass</h3>
<blockquote class="blockquote">
<p>‚ÄúI worship at the alter of intention and obstacle.‚Äù</p>
<p>‚Äî Aaron Sorkin</p>
</blockquote>
<p>Aaron Sorkin is a famous screenwriter. Creating hits like the TV show ‚ÄúThe West Wing‚Äù or movies like ‚ÄúThe Social Network‚Äù. His says that every story needs to have a clear intention and obstacle.</p>
<p>In any story, the main character has a have a clear intention. What do they want? Why do they want it? After that you need an obstacle. Who or what is trying to stop them from getting their intention? It‚Äôs fundamental to every story, but doesn‚Äôt have to be life and death. Every movie you have ever seen has this. If it didn‚Äôt have a strong intention or obstacle in the first 10 minutes, you probably hated the movie. For example, all Harry Potter wants is to live a normal life with a loving family and close friends. Since the day he was a born a dark wizard is trying to kill him. Intention (living), and obstacle (trying to kill him).</p>
</section>
<section id="hooks-vs-frames" class="level3">
<h3 class="anchored" data-anchor-id="hooks-vs-frames">Hooks vs Frames</h3>
<p>Everyone has seen Twitter/X threads that start with ‚ÄúThe unbelievable story of XYZ person doing XYZ thing‚Äù. It‚Äôs clickbait and makes me cringe every time I see it. These are tactics that try to hook a reader into continuing to engage in the content. Instead of creating hooks, Shaan recommends creating the right frame for a story. Hooks are about the words you‚Äôre going to write. Frames are about the idea. And how you‚Äôre going to connect many ideas together to make it relevant to an audience.</p>
<p>Check out the following tweets about the popular audio app Clubhouse.</p>
<p><img src="https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/image2.png" class="img-fluid"></p>
<p>This first tweet is by someone who founded a similar audio app before Clubhouse. He should be very knowledgeable on the subject. The tweet gained little traction because he told no story. It was dry and full of technical industry facts and jargon.</p>
<p><img src="https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/image3.png" class="img-fluid"></p>
<p>Now take a similar tweet by Shaan. In his tweet he told a story. He put in the frame of ‚Äúevery one thinks X, but I think Y, and here‚Äôs how I think it‚Äôs going to go down‚Äù. It had millions of views. The thread is a mini screenplay and masterclass on how the right frame can make an idea turn into a powerful story.</p>
<p>Another powerful example is Dave Chappelle trying to get the rights back to his famous sketch show. He didn‚Äôt complain to Viacom (owners of Comedy Central which initially hosted the show). Instead he told a story with a powerful framing. It wasn‚Äôt a funny story. But one that made his fans start boycotting the show on streaming platforms. Hoping to get paid for his original hard work. This was all due to a powerful story with the right framing. Check <a href="https://youtu.be/5N4UFl9G00A?si=a9AwDMvbQ8IrLqSU">out this video</a> to hear the full story.</p>
</section>
<section id="tell-100-stories" class="level3">
<h3 class="anchored" data-anchor-id="tell-100-stories">Tell 100 Stories</h3>
<p>If you don‚Äôt know who Mr.&nbsp;Beast is than you live under a rock with no internet connection. People always ask how they can be like him. His advice is simple. Make 100 videos. Each time do one thing better than the last video. He says it‚Äôs the perfect advice. Because either no one actually goes through with making 100 videos. Or the people that do never reach out to him again because once they make 100 videos they figured out how to make them successful.</p>
<p>To tell great stories you need to tell hundreds of stories, maybe thousands. You need to get intelligent reps in. With each rep you get a little better. These gains can compound a powerful skill over time.</p>
</section>
<section id="make-a-story-go-viral" class="level3">
<h3 class="anchored" data-anchor-id="make-a-story-go-viral">Make A Story Go Viral</h3>
<p>There are companies out in the world whose sole purpose is to make content go viral. Now that‚Äôs a wild job. They know the only way something goes viral is how many people share the content. People will normally share content if it creates an emotional reaction. Shaan calls these emoji reactions. Things like ü§£üòçüòéü§îüòÆü§¨. When the company is creating a piece of content, they start with the emotion they want the content to create. Then they work back from that emotion to create the content. The north star is always the emotion the want the audience to feel. If their first draft doesn‚Äôt create that emotion, they will adjust until they get it right.</p>
</section>
<section id="storyworthy-book" class="level3">
<h3 class="anchored" data-anchor-id="storyworthy-book">Storyworthy Book</h3>
<p>Matthew Dicks wrote the book on storytelling. No he actually did. He wrote a book called <a href="https://www.amazon.com/Storyworthy-Engage-Persuade-through-Storytelling-ebook/dp/B07CV2PFYJ/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;dib_tag=se&amp;dib=eyJ2IjoiMSJ9.2Tt6q12dGtz0elc2vBXMeyV883OkegYe9BTrbUg1DoV1313B1TaDUMm32fkWouW4yp32_9uzvUNcJg8SWX1bGlXVs0rAmqsbMtWi7m90beyC1bkyuNxQ_w7cFfM4foQXbGN4ZxWU-E0I-RSOKZeQQgyvwjJsGFhW_O6QjXUle0AU4TCHQgNX3mzmorrmy14UX1OJ8Ez7pjoHN-DywVpZf8yD2CjeErvvkPZOVB0m3HVbHH9OaXsgLacUEK-QYz65c8GAERMLkmOsBjiMxrBCmMaMePTvVA3bQu5wYEkgSmw.d0fzZ1oE--2p9Td8xF85u2X5bHNOCPuYjR81yxxZnM4&amp;qid=1717428290&amp;sr=8-1">‚ÄúStoryworthy‚Äù</a>. I‚Äôm currently reading it and love the book, future post coming soon. Shaan calls out a few kew ideas from the book in the podcast.</p>
<p>First is every story needs stakes. You need to make clear what‚Äôs at stake if the main character doesn‚Äôt get what they want (their intention). These stakes don‚Äôt have to be life or death though. Stakes come from the emotion. Something closer to regular life has the biggest impact because other people can see themselves in the story. Telling a heartwarming story about learning life lessions from your kids at the dinner table can be more powerful than a shark attack story.</p>
<p>No one cares about your vacation or crazy party story. If you have something that‚Äôs generally interesting about your vacation, then only keep the parts that are relevant to the story. If you got pickpocketed at the Vatican, but were able to chase down the bad guy and get your wallet back, then just tell that part of the story. It may not matter that you were on the Amolfi coast the previous day or you were even at the Vatican. No one cares. Just keep your story as long as it is interesting. Not a second longer.</p>
<p>At its core, a story is a five second moment of change. Everything comes up to one moment where a thing or person is transformed. If the story doesn‚Äôt have change, then it‚Äôs just an anecdote. A sequence of events. Not a real story. Stories start with the world one way, and end with it another way. Shaan says a good example is every romantic comedy you‚Äôve ever seen. Whatever the main character is, they are 100% going to be the opposite at the end of the movie. Is the woman a fast paced lawyer who never made time for love? By the end of the movie she will have a romantic partner and will take more time away from work. Is the guy a ladies man who will never settle down? By the end of the movie he will fall for someone who will make him rethink everything and start hearing wedding bells. It‚Äôs always about change.</p>
</section>
<section id="adding-humor-in-storytelling" class="level3">
<h3 class="anchored" data-anchor-id="adding-humor-in-storytelling">Adding Humor in Storytelling</h3>
<p>Humor is the sauce but not the meal in most stories. All humor is just surprise. If you see the punch line coming it‚Äôs not very funny. Try to add humor into your storytelling. Just don‚Äôt make it the whole story. Leave that up to the comedians.</p>
</section>
<section id="binge-bank" class="level3">
<h3 class="anchored" data-anchor-id="binge-bank">Binge Bank</h3>
<p>If someone wanted to learn more about you, what would they do? Maybe they‚Äôre a recruiter trying to offer you the perfect job. Maybe they‚Äôre an entrepreneur trying to find their next co-founder. Or maybe they‚Äôre a cute girl you‚Äôre about to have dinner with. Chances are the only things they can find on you is your social media presence, and hopefully not any mugshots.</p>
<p>Shaan recommends creating a binge bank for yourself. Think of it as a bank of content that someone can go down the rabbit hole on you. Blogs, videos, newsclippings. Hopefully a collection of stories. Whatever content that gives someone all access pass to how your brain works and who you are as a person. By the end a person‚Äôs opinion of you should drastically change. This binge bank can become more powerful than a resume, because it shows your true self. While also showcasing how you communicate.</p>
<p>After hearing this I started to curate my own binge bank. I added a <a href="https://mftokic.github.io/start_here.html">Start Here</a> section to my personal site. On this page I have links to posts that best describe me and my capabilities. Right now it‚Äôs small but one day I plan to grow it to the binge banks of people like <a href="https://ryanholiday.net/best-articles/">Ryan Holiday</a>, <a href="https://tim.blog/new-start-here/">Tim Ferriss</a>, or <a href="https://markmanson.net/articles">Mark Manson</a>.</p>
</section>
<section id="growing-an-audience" class="level3">
<h3 class="anchored" data-anchor-id="growing-an-audience">Growing An Audience</h3>
<p>Shaan has a friend with a cool rule about being interesting. If you tell someone something interesting, they will say ‚Äúwow that‚Äôs interesting‚Äù. If you tell someone two interesting things, the will say ‚Äúthose are interesting‚Äù. If you tell someone three interesting things, they will say ‚Äúok, now you are interesting‚Äù. To build a following on the internet or at your job, you need to demonstrate your insight constantly. Instead of doing this three times you may have to do this 100 times to get people to come back and pay attention.</p>
<p>Another powerful point that Shaan learned from a branding expert is people will follow you to the ends of the earth if you can give them a feeling more consistently than anyone else. Why do people listen to podcasts from comedians? It‚Äôs not because they have groundbreaking insight about current events or they perform their standup routine. It‚Äôs because when people listen they feel like ‚Äúone of the guys‚Äù who like to hang out with their friends and crack jokes at one another. How people feel after consuming your story is the end goal.</p>
</section>
<section id="closing-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h3>
<p>Storytelling is more art than science. It takes reps to get it right. If you can tell good stories, you can do almost anything in this world. Get out there and start telling better stories today. Just not ones about your vacation.</p>


</section>

 ]]></description>
  <category>podcast</category>
  <category>learning</category>
  <guid>https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/</guid>
  <pubDate>Tue, 04 Jun 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-06-04-how-to-master-storytelling/image.png" medium="image" type="image/png" height="78" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Deep Learning Last</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the tenth and final principle of a good time series forecast, deep learning last. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/"><strong>Deep Learning Last</strong></a></li>
</ol>
</section>
<section id="the-shiny-new-thing" class="level3">
<h3 class="anchored" data-anchor-id="the-shiny-new-thing">The Shiny New Thing</h3>
<p>Deep learning is the latest frontier in the field of machine learning. It‚Äôs a subset of machine learning that uses neural networks with many layers (hence ‚Äúdeep‚Äù) to model complex patterns in data. These neural networks are built to resemble how human brains work. There are a lot of different types of deep learning models. Even the latest large language models from OpenAI are using deep learning techniques.</p>
<p>Since deep learning is getting all the hype nowadays, it can be tempting to go straight to training deep learning models when starting a new forecasting project. This is a bad idea. While deep learning can be very effective, there are many reasons I‚Äôll call out in this post that make deep learning hard to use for forecasting projects. You can still use a deep learning model in your forecast, but I recommend exhausting all other avenues before trying deep learning. Let‚Äôs dive into why deep learning should be tried last.</p>
</section>
<section id="reasons-to-use-deep-learning-last" class="level3">
<h3 class="anchored" data-anchor-id="reasons-to-use-deep-learning-last">Reasons To Use Deep Learning Last</h3>
<section id="lack-of-quality-data" class="level4">
<h4 class="anchored" data-anchor-id="lack-of-quality-data"><strong>Lack of Quality Data</strong></h4>
<p>Deep learning can work well if you have thousands, or better yet millions, of observations in your historical data. In my job we might be trying to forecast a monthly time series for a single product, but only have the last three years of historical data. That‚Äôs 36 data points. This lack of data is a common problem at my company, where new products are released constantly (meaning they have limited data) and our business shifts so often that even historical years from six years ago may not be relevant to where our business is headed. If you don‚Äôt have tons of historical data, it becomes very hard to train an accurate deep learning model.</p>
</section>
<section id="expensive-hardware" class="level4">
<h4 class="anchored" data-anchor-id="expensive-hardware"><strong>Expensive Hardware</strong></h4>
<p>Deep learning requires millions of matrix algebra calculations. Think of it as multiplying two sets of tables together. Regular computers have CPUs (central processing unit), which are designed for sequential processing. Even if you have 10+ CPUs on a computer, it will take a while to crank through the millions of matrix operations needed to train a deep learning model. GPUs on the other hand, are specialized to have thousands of cores and parallelize matrix operations effectively. They were initially built for video game graphics, hence the name graphical processing unit, but in recent years have stumbled across a new use case in training deep learning models. This is why Nvidia is the third most valuable company at the time of this writing, since they are the leading manufacturer of GPUs. These GPUs are hard to build, making them expensive to buy or rent from a cloud provider. Because they are expensive to use, they make it harder for anyone to start using them. With non-deep learning models you can start training them on your local computer, but to train a deep learning model you either have to camp out at Best Buy to purchase a Nvidia H100 or jump through a lot of hoops with a cloud provided to rent one by the minute. The juice may not be worth the squeeze.</p>
</section>
<section id="bigger-black-box" class="level4">
<h4 class="anchored" data-anchor-id="bigger-black-box"><strong>Bigger Black Box</strong></h4>
<p>Deep learning models are often harder to interpret than other machine learning models. This is due to them having up to billions of parameters (model inputs) that are abstracted between multiple layers. This means one layer of parameters feeds into another layer of parameters. There are ways to interpret the inner workings of these models, but they are often just an educated guess. Can anyone explain how a deep learning model like GPT-4 came up with its answer? Not likely.</p>
</section>
</section>
<section id="what-to-use-instead" class="level3">
<h3 class="anchored" data-anchor-id="what-to-use-instead">What to Use Instead</h3>
<p>Maybe I‚Äôve convinced you to not chase after the shiny thing and try something non-deep learning first. What model should I use instead? Here is what to try first. Once you have tried these models and evaluated their performance, you can then see if the juice is worth the squeeze with deep learning.</p>
<section id="univariate-models" class="level4">
<h4 class="anchored" data-anchor-id="univariate-models"><strong>Univariate Models</strong></h4>
<p>These are models that only need one variable, historical values of what you‚Äôre trying to forecast. Univariate models are more statistics than machine learning, and are custom built for time series. They train very fast and are tuned for each specific time series in your data. One weakness is some of these models cannot take in outside data in the form of features. With that said they are a terrific starting point for any new forecasting project. Often they can get you the required accuracy needed, and if they don‚Äôt they can serve as the benchmark to beat with other models. Here are a few common univariate models to try first.</p>
<ul>
<li><strong>ARIMA</strong>: An ARIMA (AutoRegressive Integrated Moving Average) model predicts future values in a time series by combining differencing (modeling the difference between periods), autoregression (using past values), and moving averages (using past forecast errors).</li>
<li><strong>Exponential Smoothing</strong>: Forecasts future values in a time series by applying decreasingly weighted averages of past observations, giving more importance to recent data points to capture trends and seasonal patterns.</li>
<li><strong>Seasonal Naive</strong>: Predicts future values by simply repeating the observations from the same season of the previous period, assuming that future patterns will mimic past seasonal cycles.</li>
</ul>
</section>
<section id="traditional-ml-models" class="level4">
<h4 class="anchored" data-anchor-id="traditional-ml-models"><strong>Traditional ML Models</strong></h4>
<p>After trying univariate models, it‚Äôs time to try more traditional machine learning models. These are models built specifically for tabular data, or data that can live in a SQL table or excel spreadsheet. These models are multivariate, which allow them to incorporate outside variables as features to improve their forecast accuracy. They require more handling than a model like ARIMA, since they need <a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/index.html">feature engineering</a> and proper <a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/index.html">time series cross-validation</a>. Multivariate models can also learn across multiple time series at the same time, instead of being trained on just a single time series like a univarite model. Here are a few common multivariate models.</p>
<ul>
<li><strong>Linear Regression</strong>: Predicts future values by fitting a line to the historical data, where the line represents the relationship between the dependent variable and one or more independent variables.</li>
<li><strong>XGBoost</strong>: Predicts future values using an ensemble of decision trees, boosting their performance by iteratively correcting errors from previous trees, resulting in a highly accurate and robust prediction model.</li>
<li><strong>Cubist</strong>: Predicts future values by combining decision trees with linear regression models, creating rule-based predictions that incorporate linear relationships within each segment of the data for greater accuracy.</li>
</ul>
</section>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>Do mega retail corporations like Amazon or Walmart only use ARIMA or linear regression models when trying to forecast the millions of product skus in their universe? Probably not. When the stakes are that high, and they can hire hundreds of data scientists to forecast, then they most likely build their own custom deep learning approaches that can learn from billions of data points to produce robust forecasts. With limitless resources and data, deep learning becomes easy. Assume you are not them.</p>
<p>Exciting startups like Nixtla have been doing great work on deep learning transformer models. These are the types of models that power products like GPT-4 from OpenAI. They built something called <a href="https://docs.nixtla.io/">TimeGPT-1</a>, which is a generative model for time series. They trained this model on billions of publicly available time series, creating the first GPT model tailored to time series. What required special hardware and tons of data to train can now be a simple API call in any programming language. This is a potential game changer and can completely change how forecasting is done, turning it more into a software engineering problem than a data science problem. Keep a close eye on this space as innovations like this can move at light speed.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>While deep learning holds great promise and can offer high accuracy in certain scenarios, it is often not the best starting point for most forecasting projects. The need for extensive data, expensive hardware, and the complexity of interpreting deep learning models make it a less practical choice compared to more traditional methods. Starting with simpler, well-established models like ARIMA, exponential smoothing, or traditional machine learning models often provides sufficient accuracy with lower costs and greater interpretability. As innovations continue to emerge, especially with models like TimeGPT-1, the landscape of time series forecasting may shift, making deep learning more accessible and practical. However, for now, prioritize simpler, more transparent models and reserve deep learning as a last resort when simpler methods fall short.</p>
</section>
<section id="series-wrap-up" class="level3">
<h3 class="anchored" data-anchor-id="series-wrap-up">Series Wrap Up</h3>
<p>That‚Äôs a wrap on our <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">First Principles in Time Series Forecasting</a> series! My goal was to walk through core concepts of creating a strong time series forecast. Instead of diving deep into code and super technical concepts, I wanted to give timeless knowledge that will serve anyone who builds or consumes time series forecasts. Hopefully you enjoyed the series and learned a lot ü§û.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/</guid>
  <pubDate>Fri, 31 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/image.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Model Combinations Are King</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the ninth principle of a good time series forecast, model combinations are king. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/"><strong>Model Combinations Are King</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="wisdom-of-the-crowds" class="level3">
<h3 class="anchored" data-anchor-id="wisdom-of-the-crowds">Wisdom of the Crowds</h3>
<p>In 1906, famed statistician Francis Galton went to a county fair for some fun. While there he came upon a competition to guess the weight of an ox. Eight hundred people entered the competition but the guesses were all over the place, some too high, some too low. Francis was a big numbers guy, so he took all of the guesses home with him and crunched the data. He found out that the average of all the guesses was only one pound away from the actual weight of the ox, which weighed 1,198 pounds. That‚Äôs an error of less than 0.08%. What he stumbled upon that day is now know as the wisdom of the crowds.</p>
<p>The concept of wisdom of the crowds states that the collective wisdom of a group of individuals is usually more accurate than that of a single expert. When guessing the weight of the ox, the overestimates and underestimates of regular people cancelled each other out. Creating an average prediction that was more accurate and any single person‚Äôs estimate.</p>
<p>This principle is important in machine learning forecasting. Usually it‚Äôs not one single model that performs the best, but instead a combination of multiple models. Let‚Äôs take a look at how we can combine models into more accurate forecasts.</p>
</section>
<section id="types-of-model-combinations" class="level3">
<h3 class="anchored" data-anchor-id="types-of-model-combinations">Types of Model Combinations</h3>
<p>There are many different ways individual model forecasts can be combined to create more accurate forecasts. For today we‚Äôll cover the most common approaches. If you‚Äôd like to dive deeper I recommend this <a href="https://robjhyndman.com/publications/combinations/index.html">amazing paper</a> by our forecasting Godfather Rob Hyndman.</p>
<ol type="1">
<li><strong>Simple Average</strong>: As simple as it sounds. Just take the forecasts from individual models and average them together.</li>
<li><strong>Ensemble Models</strong>: Feed the individual model forecasts as features into a machine learning model, and have the model come up with the correct weighted combination. This is also known as ‚Äúmodel stacking‚Äù.</li>
<li><strong>Hierarchical Reconciliation</strong>: This involves forecasting at different aggregations of the data set based on its inherent hierarchies, then reconciling the down to the lowest level (bottoms up) using a statistical process. For example forecasting by city, country, continent, and global level then reconciling each forecast down to the city level. This reconciliation can be thought as combining different forecasts together to create something more accurate. This approach has more nuances, and will be covered in another post.</li>
</ol>
</section>
<section id="model-combination-example" class="level3">
<h3 class="anchored" data-anchor-id="model-combination-example">Model Combination Example</h3>
<p>Let‚Äôs walk through a simple example around how combining the predictions of more than one model can outperform any single model. Below is an example monthly time series. We will try to back test the last 12 months of the historical data.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/chart1.png" class="img-fluid"></p>
<p>To keep things simple we can just run a few models to get the back testing results for the last year of the data. We‚Äôll use various univariate time series models. Ignore the types of models used. Instead, let‚Äôs just see how each model did on it‚Äôs own. Learn more about accuracy metrics in a <a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">previous post</a>.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/chart2.png" class="img-fluid"></p>
<table class="caption-top table">
<caption>Accuracy by Single Model</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>MAPE</th>
<th>MAE</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>arima</td>
<td>1.97</td>
<td>3.76</td>
<td>4.68</td>
</tr>
<tr class="even">
<td>croston</td>
<td>10.18</td>
<td>19.54</td>
<td>20.01</td>
</tr>
<tr class="odd">
<td>nnetar</td>
<td>9.77</td>
<td>18.37</td>
<td>26.00</td>
</tr>
<tr class="even">
<td>stlm-ets</td>
<td>1.92</td>
<td>3.68</td>
<td>4.59</td>
</tr>
<tr class="odd">
<td>tbats</td>
<td>1.86</td>
<td>3.51</td>
<td>4.05</td>
</tr>
<tr class="even">
<td>theta</td>
<td>2.46</td>
<td>4.71</td>
<td>5.52</td>
</tr>
</tbody>
</table>
<p>It looks like the tbats model performs the best across the board with stlm-ets and arima not far behind. What if we averaged the three of them together? Let‚Äôs see how the results change.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/chart3.png" class="img-fluid"></p>
<table class="caption-top table">
<caption>Accuracy for Average Model</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>MAPE</th>
<th>MAE</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>arima_stlm-ets_tbats</td>
<td>1.84</td>
<td>3.51</td>
<td>3.99</td>
</tr>
</tbody>
</table>
<p>Even better results! See how creating simple model averages can improve the results? Averaging the results can help smooth out any under or over forecasts, creating more accurate models.</p>
<p>Simple model averages are often the quickest way to improved forecast accuracy. Another way is to create an ensemble model that can create the weights on its own. Let‚Äôs feed the predictions from each model into a linear regression model and have it determine the optimal weights.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/chart4.png" class="img-fluid"></p>
<table class="caption-top table">
<caption>Accuracy for Ensemble Model</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>MAPE</th>
<th>MAE</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ensemble</td>
<td>1.81</td>
<td>3.40</td>
<td>3.65</td>
</tr>
</tbody>
</table>
<p>Alight more accurate results! By feeding each individual model forecast into a final ensemble model, we were able to get a more accurate forecast.</p>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>When trying to combine models, there is always a risk of overfitting. Meaning the combination approach (like simple average or ensemble) could have great accuracy on the back test data but not generalize well to new unseen data in our future forecast. To prevent that we can make sure to back test on enough historical data to prove our combination approach works well for more than just a period or two. We can also have separate validation and test splits in the back testing to see how combinations made on one data set can generalize well when tested on the other.</p>
<p><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/#future-uncertainty">Prediction intervals</a> are harder to create. Simply combining the 80% and 95% prediction intervals of multiple models together is not going to fully capture the uncertainty of forecasts created by the new model combination. So we would need to re-create the intervals based on the results of the new combined model.</p>
<p>Similar to prediction intervals, combining models can also make it harder to interpret them. Instead of just understanding one model and its predictions, we now have to understand how multiple models work and are combined to get the final forecast.</p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>Model combinations can be hard to do effectively. Thankfully my forecasting package, <a href="https://microsoft.github.io/finnts/index.html">finnts</a>, is here to help! It automatically handles every kind of model combination method listed in this post. Check out the package and see just how easy forecasting can be.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Just like the county fair crowd nailed the ox‚Äôs weight, combining multiple models in time series forecasting yields more accurate predictions by balancing out individual errors. When you‚Äôre forecasting, remember to embrace the collective wisdom of models for better results!</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/</guid>
  <pubDate>Tue, 28 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Multistep Horizon Forecasting With finnts</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-13-finnts-multistep-horizon/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-13-finnts-multistep-horizon/image.png" class="img-fluid"></p>
<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TL;DR</h3>
<p>I‚Äôm excited to announce that we just released a new feature in our machine learning forecast package, <a href="https://microsoft.github.io/finnts">finnts</a>, centered around multistep horizon forecasting. It‚Äôs a mouthful to say but at a high level it helps improve forecast accuracy by optimizing models to be accurate at each period of a forecast horizon. For example, a 3 month forecast would then be optimized for the forecast in each month (period) of the future forecast.</p>
<p>Let‚Äôs dive in to how multivariate modeling used to work in the package and how multistep horizon forecasting can help.</p>
</section>
<section id="how-it-used-to-work" class="level3">
<h3 class="anchored" data-anchor-id="how-it-used-to-work">How It Used to Work</h3>
<p>Let‚Äôs use an example of a monthly revenue forecast for our business‚Äôs main product. In practice we would want more than a year of data but let‚Äôs just keep it simple today.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Date</th>
<th>Revenue</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2023-01-01</td>
<td>120</td>
</tr>
<tr class="even">
<td>2023-02-01</td>
<td>135</td>
</tr>
<tr class="odd">
<td>2023-03-01</td>
<td>140</td>
</tr>
<tr class="even">
<td>2023-04-01</td>
<td>145</td>
</tr>
<tr class="odd">
<td>2023-05-01</td>
<td>150</td>
</tr>
<tr class="even">
<td>2023-06-01</td>
<td>155</td>
</tr>
<tr class="odd">
<td>2023-07-01</td>
<td>160</td>
</tr>
<tr class="even">
<td>2023-08-01</td>
<td>165</td>
</tr>
<tr class="odd">
<td>2023-09-01</td>
<td>170</td>
</tr>
<tr class="even">
<td>2023-10-01</td>
<td>175</td>
</tr>
<tr class="odd">
<td>2023-11-01</td>
<td>180</td>
</tr>
<tr class="even">
<td>2023-12-01</td>
<td>185</td>
</tr>
</tbody>
</table>
<p>If we wanted to forecast the next 3 months of revenue using multivariate machine learning models we would have to do some <a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">feature engineering</a> to get our data in good shape. This involves creating lags on our <span style="text-decoration: underline; cursor: help;" title="What we want to forecast, in this case Revenue.">target variable</span>. Let‚Äôs try create some lags on this data.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Date</th>
<th>Revenue</th>
<th>Revenue_Lag1</th>
<th>Revenue_Lag2</th>
<th>Revenue_Lag3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2023-01-01</td>
<td>120</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>2023-02-01</td>
<td>135</td>
<td>120</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2023-03-01</td>
<td>140</td>
<td>135</td>
<td>120</td>
<td></td>
</tr>
<tr class="even">
<td>2023-04-01</td>
<td>145</td>
<td>140</td>
<td>135</td>
<td>120</td>
</tr>
<tr class="odd">
<td>2023-05-01</td>
<td>150</td>
<td>145</td>
<td>140</td>
<td>135</td>
</tr>
<tr class="even">
<td>2023-06-01</td>
<td>155</td>
<td>150</td>
<td>145</td>
<td>140</td>
</tr>
<tr class="odd">
<td>2023-07-01</td>
<td>160</td>
<td>155</td>
<td>150</td>
<td>145</td>
</tr>
<tr class="even">
<td>2023-08-01</td>
<td>165</td>
<td>160</td>
<td>155</td>
<td>150</td>
</tr>
<tr class="odd">
<td>2023-09-01</td>
<td>170</td>
<td>165</td>
<td>160</td>
<td>155</td>
</tr>
<tr class="even">
<td>2023-10-01</td>
<td>175</td>
<td>170</td>
<td>165</td>
<td>160</td>
</tr>
<tr class="odd">
<td>2023-11-01</td>
<td>180</td>
<td>175</td>
<td>170</td>
<td>165</td>
</tr>
<tr class="even">
<td>2023-12-01</td>
<td>185</td>
<td>180</td>
<td>175</td>
<td>170</td>
</tr>
<tr class="odd">
<td>2024-01-01</td>
<td>???</td>
<td>185</td>
<td>180</td>
<td>175</td>
</tr>
<tr class="even">
<td>2024-02-01</td>
<td>???</td>
<td>???</td>
<td>185</td>
<td>180</td>
</tr>
<tr class="odd">
<td>2024-03-01</td>
<td>???</td>
<td>???</td>
<td>???</td>
<td>185</td>
</tr>
</tbody>
</table>
<p>We added some rows onto the bottom of the data, to allow us to forecast out the next 3 months after our historical data ends. That‚Äôs what we have question marks ‚Äú???‚Äù for those values. We also added lags for a 1 month, 2 month, and 3 month lag. But hey, looks like we have a problem. We have more question marks for a few future months for lag 1 and lag 2. If we wanted to forecast the next three months we wouldn‚Äôt be able to use those lags, since once we get out further in the <span style="text-decoration: underline; cursor: help;" title="How far out we want to forecast, in this case we have a forecast horizon of 3.">forecast horizon</span> we start to have missing lag data.</p>
<p>This means that the smallest lag we could use would always be equal to or greater than the forecast horizon. Since our forecast horizon is 3 than the smallest lag we could use to train a model on would be lag 3. This approach can yield good results, but it removes a lot of potential signal in the data. Revenue next month is most likely impacted by how revenue grew in the current month, but if our forecast horizon is long a lot of this insight has to get thrown away before we can train models. Imagine a forecast horizon of 12. For a monthly forecast this limits our lags to 12 months or more, which is a really bummer since our business can drastically change within 3-6 months, and not using that information in our model can hurt forecast accuracy.</p>
</section>
<section id="how-multistep-horizon-forecasting-works" class="level3">
<h3 class="anchored" data-anchor-id="how-multistep-horizon-forecasting-works">How Multistep Horizon Forecasting Works</h3>
<p>Multistep horizon helps fix this issue that allows us to use smaller lags while still being able to have long forecast horizons. In our 3 month forecast horizon example, we can keep the lag 1 and lag 2 features, but how the model gets trained will be different.</p>
<p>In the non-multistep horizon approach, a specific model is trained once on the data using lags that are equal or greater than the forecast horizon. When we run a multistep horizon approach, we can actually train multiple sub models under the hood of a specific model. In our 3 month forecast horizon, here‚Äôs how one model like linear regression will be trained.</p>
<ul>
<li>For the first month in the forecast horizon, we can use all available lags. Lag 1, lag 2, and lag 3 of revenue will all be used to predict the first month.</li>
<li>In the second month of the forecast horizon, we will use lag 2 and 3 to predict the second month.</li>
<li>In the third month of the forecast horizon, we will use lag 3 to predict the third month.</li>
</ul>
<p>Are you starting to get the hang of it? With multistep horizon forecasting we can still have one model that under the hood has multiple sub models that are each optimized on forecasting out a specific part of our forecast horizon. This allows us to have greater accuracy in the first few periods of our forecast horizon. In a non-mulitstep horizon approach, we are always optimizing for the last period in a forecast horizon. If the forecast horizon is 12 months, the way we do the feature engineering and train models is optimized for forecasting out the 12th month. When running a multistep horizon approach, we instead optimize for every period of the forecast horizon.</p>
<p>This kind of approach is so crucial to forecasters in the corporate finance space. Often these financial analysts are tasked with always forecasting out the rest of the entire fiscal year, even though they might only care about the next 3 months, since they are most likely going to be re-creating a new forecast in the following quarter. Multistep horizon forecasting allows these analysts to still forecast out long forecast horizons like 9 or 12 months, while still being able to optimize for the next 1-3 months. How cool is that!</p>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>If each specific model can have 2-5 sub models under the hood, the amount of time needed to train these models can multiply by the same amount. Make sure to keep that in mind if run time is a big factor in your forecasting process.</p>
<p>A multistep horizon forecast may not result in a more accurate forecast for smaller forecast horizons. Some time series may have a strong relationship with a 12 month lag, but less with a 1 month or 2 month lag. This means there is strong yearly seasonality in the data. If there is not a strong relationship with 1 month or 2 month lag, then having multiple sub models optimize for each future month in a multistep horizon approach may not result in more accurate forecasts. Consider doing some <span style="text-decoration: underline; cursor: help;" title="Data analysis that helps us better understand our historical data before we start feature engineering and training models.">exploratory data analysis</span> to see what kind of relationships there are with historical lags of your target variable and other features.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>The new multistep horizon forecasting approach in finnts allows users to create even more accurate forecasts, regardless of their forecast horizon length. If you‚Äôd like to learn more check out the <a href="https://microsoft.github.io/finnts/articles/models-used-in-finnts.html#multistep-horizon-models">official finnts documentation</a> to see how you can use the newest multistep horizon feature!</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <category>finnts</category>
  <guid>https://mftokic.github.io/posts/2024-05-13-finnts-multistep-horizon/</guid>
  <pubDate>Mon, 13 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-13-finnts-multistep-horizon/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Capture Uncertainty</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the eighth principle of a good time series forecast, capture uncertainty. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/"><strong>Capture Uncertainty</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="building-trust" class="level3">
<h3 class="anchored" data-anchor-id="building-trust">Building Trust</h3>
<p>Would you give your retirement savings to a hedge fund manager because they asked nicely? Probably not. Instead, you would like to do your research about them. Ask them how well they performed in the market historically, and also see how they expect the future markets to unravel in the near term. If their answer to those questions are, ‚ÄúI don‚Äôt have a historical track record‚Äù and ‚ÄúI have no clue what the future holds‚Äù then you are probably not going to give them one penny of your hard earned money. The same holds true for using a time series forecast created by machine learning (ML) models. In order to build trust with the end user of the forecast, you need to show them how a similar forecast would have performed historically and also quantify some aspect about the future. Let‚Äôs dive into each one.</p>
</section>
<section id="past-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="past-uncertainty">Past Uncertainty</h3>
<p>Before a ML model can be used to forecast the future, we need to see how it has handled the past. This is called back testing, where we see how a model performed historically. This can give us a good proxy around how it could perform in the future.</p>
<p>Back testing at its core is all about training a model on a portion of your historical data set (training data), then using the trained model on another portion of the historical data (testing data). This can be as simple as using the first 80% of your historical data to train a model, and use the last 20% for testing. Check out a <a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">previous post</a> to learn more about why the order of that train/test split is important.</p>
<p>There are also more advanced methods of doing this, like time series cross-validation. This involves many rounds of training a model and then creating a prediction on the testing data. Time series cross-validation can be used to tune model hyperparameters (inputs a model cannot learn from but must be given by a human) but is especially useful for model back testing. Check out the chart below that shows how we can effectively back test using a time series cross-validation approach. Each pass has its own train and test split, and the testing splits can overlap from one pass to another.</p>
<dl>
<dt><img src="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/chart1.png" class="img-fluid"></dt>
<dd>
<p>Source: Uber Engineering</p>
</dd>
</dl>
<p>In order to capture how accurate the back testing is, we need to calculate a metric that summarizes the model‚Äôs performance on the testing data splits. There are countless metrics we can use, each with their own pros and cons. That kind of discussion is out of scope for this post but let‚Äôs highlight a few common ones you could use in determining how accurate a model is during back testing.</p>
<ol type="1">
<li><strong>Mean Absolute Error (MAE)</strong>
<ul>
<li><strong>Description</strong>: MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It‚Äôs calculated as the average of the absolute differences between forecasts and actual observations.</li>
<li><strong>Strengths</strong>: MAE is straightforward and easy to interpret as it directly represents average error magnitude.</li>
<li><strong>Weaknesses</strong>: MAE treats all errors with the same weight, thus large errors have the same influence as small ones, which might not be optimal for all applications.</li>
</ul></li>
<li><strong>Root Mean Squared Error (RMSE)</strong>
<ul>
<li><strong>Description</strong>: RMSE is the square root of the mean of the squared errors. It measures the average magnitude of the error, with the squaring giving higher weight to larger errors.</li>
<li><strong>Strengths</strong>: RMSE is sensitive to outliers and provides a measure of how large errors are when they occur, which can be crucial for many practical applications.</li>
<li><strong>Weaknesses</strong>: Like MSE, RMSE can be heavily influenced by outliers and large errors, possibly leading to overestimations of the typical error if the error distribution is skewed.</li>
</ul></li>
<li><strong>Mean Absolute Percentage Error (MAPE)</strong>
<ul>
<li><strong>Description</strong>: MAPE expresses accuracy as a percentage, and it measures the size of the error in percentage terms. It is calculated as the average of the absolute errors divided by the actual values, expressed as a percentage.</li>
<li><strong>Strengths</strong>: MAPE is scale-independent and provides a clear interpretation in terms of percentage errors, making it easy to communicate.</li>
<li><strong>Weaknesses</strong>: MAPE can be highly skewed when dealing with values close to zero, and it disproportionately penalizes underestimations compared to overestimations.</li>
</ul></li>
</ol>
</section>
<section id="future-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="future-uncertainty">Future Uncertainty</h3>
<p>Now that we‚Äôve quantified how well our model works historically, we can just give the future forecast to our end user right? Not so fast. Our model might say that next month our company‚Äôs product will make $100, but if that‚Äôs all the info we provide to the end user of that forecast that‚Äôs not a good way to build trust. Instead we need to show how confident we are in that $100 forecast. How likely are we to hit that number? That‚Äôs where prediction intervals come in.</p>
<p>Prediction intervals help quantify the future uncertainty in our model‚Äôs forecast. They are statistical ranges, typically based on the forecast error, used to indicate the likelihood that the future value of a time series will fall within a specified range at a certain confidence level. Common ranges for a prediction interval are 80% and 95%. For example, the future forecast may be $100 but have a 95% prediction interval of $75 and $125. This means that there is a 95% likelihood that the future value will fall between $75 and $125. The tighter the range, the less uncertainty there is in the forecast. Below is an example forecast with 80% and 95% prediction intervals.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/chart2.png" class="img-fluid"></p>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>The back testing process can only ever be a proxy of what kind of results to expect on the future forecast. It follows the assumption that <a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">the future will be similar to the past</a>. Sometimes this is not the case, and future results may be worse than historical back testing performance.</p>
<p>While prediction intervals help quantify uncertainty, they also do not do a perfect job. There may be times where the future forecast will fall outside of the ranges. It‚Äôs not the end of the world when it does, but instead shows that the future is often different than what happened in the past. This is where strong domain knowledge comes in to understand what‚Äôs truly an outlier and what‚Äôs a new fundamental factor in your business going forward. For example, a new product launch in the future is hard to quantify with a prediction interval, but once it happens we can learn from that information and try to capture it the next time we train our model.</p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>Back testing and prediction intervals is tough work. Thankfully my forecasting package, <a href="https://microsoft.github.io/finnts/index.html">finnts</a>, takes care of both of these for you. You can even customize the back testing process to fit your needs. Check out the package and see just how easy forecasting can be.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Capturing uncertainty in time series forecasting is essential for creating robust forecasts that stakeholders can rely on. Utilizing back testing and prediction intervals not only strengthens the credibility of forecasts but also provides users with a clearer perspective on potential risks and variations. In the end these approaches help build trust with the forecast end user. The more trust we can build, the more likely the ML forecast will be used.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/</guid>
  <pubDate>Tue, 07 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Simple Models Are Better Models</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the seventh principle of a good time series forecast, simple models are better models. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/"><strong>Simple Models Are Better Models</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="occams-ml-model-razor" class="level3">
<h3 class="anchored" data-anchor-id="occams-ml-model-razor">Occam‚Äôs ML Model Razor</h3>
<p>William of Ockham was a 14th-century English Franciscan friar, philosopher, and theologian. In his work he preached that for most things in life the simplest explanation is the correct one. I‚Äôve learned this inadvertently in my life many times. For example, when I was studying for the ACT in high school a teacher told me that on the english questions it‚Äôs usually the shortest answer that is often correct. You could get a decent score just by following this one rule, even if you couldn‚Äôt read or speak english. This one tip saved my ass more than I‚Äôd like to admit, and I could read and speak english. Or so I thought.</p>
<p>Often in life, just like the ACT english section, it‚Äôs usually the simplest approaches that provide the best results. You can hire a fitness coach and buy all the supplements in the world but you‚Äôll probably get similar results following a handful of simple exercise and eating tips. The same applies in the world of machine learning. The more complexity you add to your data and models, the less likely they are going to be useful in the end. Let‚Äôs walk through how simplicity helps in all aspects of machine learning, from the data you use all the way down to models you train.</p>
</section>
<section id="more-features-more-problems" class="level3">
<h3 class="anchored" data-anchor-id="more-features-more-problems">More Features, More Problems</h3>
<p>In the world of time series forecasting, there are so many ways we can do feature engineering. Learn more about feature engineering in a <a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">previous post</a>. A dataset containing two columns, a date and value column, can be transformed into 100+ new features. This can easily get out of hand once we add external regressors (outside variables like consumer sentiment or inflation data) and create new features from them.</p>
<p>Each feature you add to a dataset hurts your model in multiple ways.</p>
<ol type="1">
<li>Train Time: It can slow down model training, meaning it will take longer to train the model. This may not seem like a big deal with small datasets but once you start having tens of thousands of rows in a dataset, adding a new feature can really slow things down.</li>
<li>Overfitting: Adding more features can lead to overfitting, meaning your model might be very accurate on the data it was trained on but cannot generalize well to unseen data in the future. Your model will learn from the noise in the data instead of the signal.</li>
<li>Interpretability: Adding more features makes it harder to explain the model‚Äôs predictions. If you can‚Äôt explain your forecast to non-technical business partners, then the forecast may not be used by anyone. I‚Äôve seen this countless times in my work. An accurate model doesn‚Äôt help anyone if the end user ultimately wants to know how the prediction was created. More on that in <a href="https://mftokic.github.io/posts/2023-02-11-three-levels-of-ml-adoption/">this post</a>.</li>
</ol>
</section>
<section id="feature-selection" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection">Feature Selection</h3>
<p>One way to simplify your data before model fitting is to implement a feature selection process. It‚Äôs called selection but it‚Äôs more like removal, where we drop any features that do not contribute to a model that can generalize well to new data. Here are a few techniques for feature selection.</p>
<ol type="1">
<li>Domain Expertise: Remove features that don‚Äôt make sense to you as a human. For example, the annual rain fall in Iceland might be perfectly correlated to Coca Cola sales in South America, but it doesn‚Äôt pass our smell test of being a factor that impacts the business. When in doubt take it out.</li>
<li>Correlation: If a feature has a strong correlation to the target variable (what we want to forecast) then we keep it in, but only after it passes our domain knowledge smell test.</li>
<li>Model Specific: Some models, like certain flavors of linear regression, have built in feature selection or feature importance. We can use that info to remove features and can then retrain on any kind of ML model.</li>
</ol>
<p>There are many other methods for feature selection, but are out of the scope of this post. The ones called out above are a good starting point.</p>
</section>
<section id="simple-models" class="level3">
<h3 class="anchored" data-anchor-id="simple-models">Simple Models</h3>
<p>Simplifying our data is helpful, but sometimes simplifying our models is even better. When starting a new forecast project, you might feel tempted to go out and build an advanced deep learning model, using all of the latest bells and whistles. That model may show promising results, but often a simpler model like linear regression can get the same or even better results. Models like linear regression are faster to train and have better model interpretability.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/image2.png" class="img-fluid"></p>
<p>We can even go one level deeper and not use any features at all. Univariate statistical models like ARIMA or exponential smoothing are classic time series forecasting models that only need one column of data, the historical values of your target variable. That‚Äôs what makes them univariate (one variable). They have built in feature engineering under the hood that allows them to learn from historical trends and seasonality in the data, so no additional work is needed to create features. Often in time series forecasting competitions a large team of deep learning researchers can just barely beat a single person team who uses simple models like ARIMA or random forest models. More on that in a future post.</p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>My forecasting package, <a href="https://microsoft.github.io/finnts/index.html">finnts</a>, has built in feature selection and other techniques to ensure simple models are built in ways that produce accurate forecasts. Check out the package and see for yourself.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Ultimately, the goal of any forecasting model is to provide clear, accurate, and quick results. Simpler models often meet these criteria better than complex ones because they‚Äôre easier to understand, faster to run, and just as accurate. By focusing on simplicity and minimizing inputs, we ensure that our forecasts are not only effective but also user-friendly. This approach doesn‚Äôt just save time; it makes the insights gained from the data accessible to everyone involved in the decision-making process. Simplicity, therefore, isn‚Äôt just a principle; it‚Äôs a practical strategy for better forecasting.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/</guid>
  <pubDate>Fri, 03 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: The Magic Is In The Feature Engineering</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-05-01-time-series-features/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the sixth principle of a good time series forecast, the magic is in the feature engineering. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/"><strong>The Magic Is In The Feature Engineering</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="turning-data-into-insight" class="level3">
<h3 class="anchored" data-anchor-id="turning-data-into-insight">Turning Data Into Insight</h3>
<p>A machine learning (ML) model is only as good as the data it‚Äôs fed. The process of transforming data, to make it easier for a model to learn from that data, is called feature engineering. It‚Äôs a technical term that is actually very simple in nature, really just data transformations. In the world of time series forecasting, feature engineering can make or break a good forecast.</p>
<p>Creating high quality features is a combination of strong domain expertise and data transformation skills. We have already covered how domain expertise impacts a forecast in a <a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">previous post</a>, so this post will cover how simple data transformations can drastically improve the accuracy of a machine learning forecast. Check out each category of time series feature engineering below to learn more.</p>
</section>
<section id="date-features" class="level3">
<h3 class="anchored" data-anchor-id="date-features">Date Features</h3>
<p>The most common type of feature engineering for time series is around dates. Date features allow us to capture seasonality patterns in our data. Think of seasonality as repeating peaks and valleys in our data. For example, our business might make most of its revenue in Q4 every year, with a subsequent dip in sales in Q1.</p>
<p>Let‚Äôs use the example time series below to illustrate each type of feature engineering.</p>
<table class="caption-top table">
<caption>Fake Time Series Data</caption>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Consumer Sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>100,000</td>
<td>68</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>110,000</td>
<td>67</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>120,000</td>
<td>65</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>115,000</td>
<td>70</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>130,000</td>
<td>72</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>125,000</td>
<td>73</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>135,000</td>
<td>74</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>140,000</td>
<td>75</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>130,000</td>
<td>70</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>145,000</td>
<td>72</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>150,000</td>
<td>71</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>160,000</td>
<td>75</td>
</tr>
</tbody>
</table>
<p>In this time series we would like to forecast monthly sales. We also have information about consumer sentiment that we can use to help forecast sales. A multivariate machine learning model cannot easily use the date column as is, so we have to do some data transformations (aka feature engineering) to make it easier for a model to understand how date information can help predict sales. Let‚Äôs go through a few examples of new features we can create from the date column. It‚Äôs important to note that after we create these new features it‚Äôs a good idea to remove the original date column before training a ML model.</p>
<p>Since the data is monthly there are a lot of simple features we can use. We can pull out the specific month, quarter, and even year into their own columns to use as features. If our data was at a daily level, we can even go deeper and get features related to day of the week, day of year, week of month, etc.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Date</th>
<th>Month</th>
<th>Quarter</th>
<th>Year</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>January</td>
<td>Q1</td>
<td>2023</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>February</td>
<td>Q1</td>
<td>2023</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>March</td>
<td>Q1</td>
<td>2023</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>April</td>
<td>Q2</td>
<td>2023</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>May</td>
<td>Q2</td>
<td>2023</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>June</td>
<td>Q2</td>
<td>2023</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>July</td>
<td>Q3</td>
<td>2023</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>August</td>
<td>Q3</td>
<td>2023</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>September</td>
<td>Q3</td>
<td>2023</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>October</td>
<td>Q4</td>
<td>2023</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>November</td>
<td>Q4</td>
<td>2023</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>December</td>
<td>Q4</td>
<td>2023</td>
</tr>
</tbody>
</table>
<p>That seems pretty straight forward right? Let‚Äôs keep squeezing our date fruit for more juice and see what other kinds of features we can create. Since this is a time series, adding some order of time can be helpful. This can be something as simple as an index starting at 1 (or even convert your date to a seconds format). This helps establish the proper order of our data and makes is easier for a model to pick up growing or declining trends over time. There is also slight differences in how many days there are from month to month, so we can add that too. If you don‚Äôt think that‚Äôs important then you have never been stung by the harsh mistress that is leap year. There have been multiple times where finance exec‚Äôs have dismissed forecasts for the quarter that includes February, where in the end we didn‚Äôt account for the fact that it was a leap year or we are one year removed from one. You can even take this one step further and add the number of business days for each month.</p>
<table class="caption-top table">
<caption>Adding a time index and other day related features</caption>
<thead>
<tr class="header">
<th>Date</th>
<th>Index</th>
<th>Days in Month</th>
<th>Business Days</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>1</td>
<td>31</td>
<td>22</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>2</td>
<td>28</td>
<td>20</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>3</td>
<td>31</td>
<td>23</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>4</td>
<td>30</td>
<td>20</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>5</td>
<td>31</td>
<td>23</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>6</td>
<td>30</td>
<td>22</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>7</td>
<td>31</td>
<td>21</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>8</td>
<td>31</td>
<td>23</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>9</td>
<td>30</td>
<td>21</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>10</td>
<td>31</td>
<td>22</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>11</td>
<td>30</td>
<td>22</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>12</td>
<td>31</td>
<td>21</td>
</tr>
</tbody>
</table>
<p>To get the final drop of juice out of the date column, we can also add Fourier series features. A Fourier series feature in time series forecasting is a component that captures seasonal patterns using sine and cosine functions to model periodic cycles in the data. In a nutshell they are just recurring peaks and valleys that can occur at various date grains like monthly or daily. These features can help capture more complex seasonality in your data. The chart below shows some standard Fourier series at the monthly and quarterly grain.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/chart1.png" class="img-fluid"></p>
</section>
<section id="lag-features" class="level3">
<h3 class="anchored" data-anchor-id="lag-features">Lag Features</h3>
<p>Time series forecasting is all about learning from the past to forecast the future. In order to learn about the past we have to create lags on our data. Often what we‚Äôre trying to forecast today is correlated to what happened in the past. This is a concept known as autocorrelation. For our monthly forecast example, a 3 month lag may be highly correlated to sales with a 0 month lag (or sales today). Consumer sentiment can also be correlated with sales, but this time a lag of 6 might have higher correlation, since there is most likely a long delay between customer purchase patters and how it affects our company‚Äôs product. Lags can be created for any amount, depending on your domain knowledge of the business and results from more exploratory data analysis (deep dive for a different day).</p>
<table class="caption-top table">
<caption>Adding lag features</caption>
<colgroup>
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 22%">
<col style="width: 21%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Consumer Sentiment</th>
<th>Sales 3-Month Lag</th>
<th>Sentiment 6-Month Lag</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>100,000</td>
<td>68</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>110,000</td>
<td>67</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>120,000</td>
<td>65</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>115,000</td>
<td>70</td>
<td>100,000</td>
<td></td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>130,000</td>
<td>72</td>
<td>110,000</td>
<td></td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>125,000</td>
<td>73</td>
<td>120,000</td>
<td></td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>135,000</td>
<td>74</td>
<td>115,000</td>
<td>68</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>140,000</td>
<td>75</td>
<td>130,000</td>
<td>67</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>130,000</td>
<td>70</td>
<td>125,000</td>
<td>65</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>145,000</td>
<td>72</td>
<td>135,000</td>
<td>70</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>150,000</td>
<td>71</td>
<td>140,000</td>
<td>72</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>160,000</td>
<td>75</td>
<td>130,000</td>
<td>73</td>
</tr>
</tbody>
</table>
<p>Last thing I‚Äôll say here is that you can also create leading features, especially for features that you know with 100% certainty ahead of time. For example, customers knowing of a new product launch in the future will definitely change how they purchase similar products you sell for the periods leading up to the launch. Someone may hold off on buying a new iPhone until the latest one gets released in a few months. Same goes for cars and many other products.</p>
</section>
<section id="rolling-window-features" class="level3">
<h3 class="anchored" data-anchor-id="rolling-window-features">Rolling Window Features</h3>
<p>Often using pure historical lags is not enough. The historical data of our target variable (what we want to forecast) can be very noisy, making it hard for a model to learn the proper trends and seasonality. One way to handle this is through rolling window transformations.</p>
<p>Rolling window features in time series forecasting help smooth out data, reduce noise, and capture essential trends and cycles by averaging or computing other statistics over a specified period. For a monthly forecast we can create rolling window features of averages, min/max, and other statistical calculations.</p>
<dl>
<dt><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/chart2.png" class="img-fluid"></dt>
<dd>
<p>Rolling Window Averages aka Moving Average</p>
</dd>
</dl>
<p>It‚Äôs best to calculate rolling window features based on your existing lag features. That way there is no <a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/#data-leakage">data leakage</a> during initial model training. See below for example of creating a 3 month rolling window average of the 3 month sales lag.</p>
<table class="caption-top table">
<caption>Rolling 3 month average applied to the 3 month sales lag</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 16%">
<col style="width: 27%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Sales 3-Month Lag</th>
<th>3-Month Rolling Avg</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>100,000</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>110,000</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>120,000</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>115,000</td>
<td>100,000</td>
<td></td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>130,000</td>
<td>110,000</td>
<td></td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>125,000</td>
<td>120,000</td>
<td>110,000</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>135,000</td>
<td>115,000</td>
<td>115,000</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>140,000</td>
<td>130,000</td>
<td>121,667</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>130,000</td>
<td>125,000</td>
<td>123,333</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>145,000</td>
<td>135,000</td>
<td>130,000</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>150,000</td>
<td>140,000</td>
<td>133,333</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>160,000</td>
<td>130,000</td>
<td>135,000</td>
</tr>
</tbody>
</table>
</section>
<section id="polynomial-features" class="level3">
<h3 class="anchored" data-anchor-id="polynomial-features">Polynomial Features</h3>
<p>The final type of feature engineering I‚Äôd like to discuss are polynomial transformations. Sometimes there is a non-linear relationship between your initial feature and the target variable. Some models, like ones that use decision trees, can handle this kind of relationship while others like linear regression cannot. To fix this we can transform the data via polynomials like squaring, cubing, and even taking the log of the initial feature.</p>
<p>Let‚Äôs take our example monthly sales data and add some spice to it. This time creating an exponential relationship between consumer sentiment and sales.</p>
<table class="caption-top table">
<caption>Updated sales data with an exponential relationship with consumer sentiment</caption>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Consumer Sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>1,309,000</td>
<td>68</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>1,204,000</td>
<td>67</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>1,000,000</td>
<td>65</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>1,525,000</td>
<td>70</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>1,849,000</td>
<td>72</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>1,964,000</td>
<td>73</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>2,121,000</td>
<td>74</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>2,500,000</td>
<td>75</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>1,525,000</td>
<td>70</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>1,849,000</td>
<td>72</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>1,764,000</td>
<td>71</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>2,500,000</td>
<td>75</td>
</tr>
<tr class="odd">
<td>January 2024</td>
<td>2,890,000</td>
<td>76</td>
</tr>
<tr class="even">
<td>February 2024</td>
<td>3,361,000</td>
<td>78</td>
</tr>
<tr class="odd">
<td>March 2024</td>
<td>3,844,000</td>
<td>79</td>
</tr>
<tr class="even">
<td>April 2024</td>
<td>4,641,000</td>
<td>81</td>
</tr>
</tbody>
</table>
<p>When graphing the data, see how the increase in consumer sentiment has an exponential effect on sales?</p>
<p><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/chart3.png" class="img-fluid"></p>
<p>To account for this, we can square the values of consumer sentiment and create a new feature to use. This new feature will make it easier for models like linear regression to capture these kinds of non-linear relationships.</p>
<table class="caption-top table">
<caption>New polynomial feature added</caption>
<colgroup>
<col style="width: 21%">
<col style="width: 15%">
<col style="width: 25%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Consumer Sentiment</th>
<th>Consumer Sentiment Squared</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>1,309,000</td>
<td>68</td>
<td>4,624</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>1,204,000</td>
<td>67</td>
<td>4,489</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>1,000,000</td>
<td>65</td>
<td>4,225</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>1,525,000</td>
<td>70</td>
<td>4,900</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>1,849,000</td>
<td>72</td>
<td>5,184</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>1,964,000</td>
<td>73</td>
<td>5,329</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>2,121,000</td>
<td>74</td>
<td>5,476</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>2,500,000</td>
<td>75</td>
<td>5,625</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>1,525,000</td>
<td>70</td>
<td>4,900</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>1,849,000</td>
<td>72</td>
<td>5,184</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>1,764,000</td>
<td>71</td>
<td>5,041</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>2,500,000</td>
<td>75</td>
<td>5,625</td>
</tr>
<tr class="odd">
<td>January 2024</td>
<td>2,890,000</td>
<td>76</td>
<td>5,776</td>
</tr>
<tr class="even">
<td>February 2024</td>
<td>3,361,000</td>
<td>78</td>
<td>6,084</td>
</tr>
<tr class="odd">
<td>March 2024</td>
<td>3,844,000</td>
<td>79</td>
<td>6,241</td>
</tr>
<tr class="even">
<td>April 2024</td>
<td>4,641,000</td>
<td>81</td>
<td>6,561</td>
</tr>
</tbody>
</table>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>Sometimes too much of a good thing can be a bad thing. Adding a lot of new features can increase the chance that a model overfits. Overfitting in machine learning occurs when a model learns to capture noise or random fluctuations in the training data, leading to poor generalization and high performance on training data but low performance on unseen data. The best way to prevent this kind of overfitting is to limit the number of features used to train a model. This will be discussed in greater detail in another post in this series.</p>
<p>Did you notice that when creating lags and rolling window features we had a lot of missing data at the start of the time series for those new features? This can be a problem. Some ML models do not like missing data, so we need to deal with those missing values. An easy way is to just drop the initial rows in the time series that have blank values for the new lags and rolling window features. This can work well if you have a lot of historical data. Dropping data can hurt model performance though, and if you don‚Äôt have a lot of data to start with it becomes a less favorable option. You could also replace the missing values, either by using a simple model to impute the value or just use the closest available value in the time series to ‚Äúfill in‚Äù the missing values. Both of these missing value replacement approaches have their own pros and cons but could be a better strategy then just simply dropping rows with missing values.</p>
<table class="caption-top table">
<caption>Filling in missing values with their closest available value</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 16%">
<col style="width: 27%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Sales ($)</th>
<th>Sales 3-Month Lag</th>
<th>3-Month Rolling Avg</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2023</td>
<td>100,000</td>
<td>100,000</td>
<td>110,000</td>
</tr>
<tr class="even">
<td>February 2023</td>
<td>110,000</td>
<td>100,000</td>
<td>110,000</td>
</tr>
<tr class="odd">
<td>March 2023</td>
<td>120,000</td>
<td>100,000</td>
<td>110,000</td>
</tr>
<tr class="even">
<td>April 2023</td>
<td>115,000</td>
<td>100,000</td>
<td>110,000</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>130,000</td>
<td>110,000</td>
<td>110,000</td>
</tr>
<tr class="even">
<td>June 2023</td>
<td>125,000</td>
<td>120,000</td>
<td>110,000</td>
</tr>
<tr class="odd">
<td>July 2023</td>
<td>135,000</td>
<td>115,000</td>
<td>115,000</td>
</tr>
<tr class="even">
<td>August 2023</td>
<td>140,000</td>
<td>130,000</td>
<td>121,667</td>
</tr>
<tr class="odd">
<td>September 2023</td>
<td>130,000</td>
<td>125,000</td>
<td>123,333</td>
</tr>
<tr class="even">
<td>October 2023</td>
<td>145,000</td>
<td>135,000</td>
<td>130,000</td>
</tr>
<tr class="odd">
<td>November 2023</td>
<td>150,000</td>
<td>140,000</td>
<td>133,333</td>
</tr>
<tr class="even">
<td>December 2023</td>
<td>160,000</td>
<td>130,000</td>
<td>135,000</td>
</tr>
</tbody>
</table>
</section>
<section id="other-pre-processing" class="level3">
<h3 class="anchored" data-anchor-id="other-pre-processing">Other Pre-Processing</h3>
<p>One thing I wanted to add that technically isn‚Äôt considered feature engineering are other data pre-processing methods. These are things you apply before you start your feature engineering process. They are specific to time series forecasting and can greatly improve forecast accuracy. Here are two pre-processing methods you should know about.</p>
<p>First is making your data stationary. This is a time series technical term that pretty much means removing the trend component of your data, where the time series has a constant mean and standard deviation. We can make a time series stationary by the process of differencing. This involves taking the difference between each date observation and using that as the new time series to train models with. Check out the example below. See how the upward trend gets removed when we simply use the difference between months instead of the original monthly values? Some machine learning models, like ones that rely on decision trees, cannot extrapolate trends. So differencing the data removes any trend pattern, making it a lot easier for these models to produce high quality forecasts.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/chart4.png" class="img-fluid"></p>
<p>Another pre-processing technique is a box-cox transformation. This helps remove any exponentially increasing trends by applying various types of power transformations. For example, taking the log of your time series. Removing non-linear trends can make it a lot easier for a model to create accurate forecasts. See the example below of a time series with a non-linear trend. We can then apply a box-cox transformation and then difference the data. See how nice the final time series looks? It will be way easier for a ML model to learn the patterns in the final transformed time series.</p>
<p><img src="https://mftokic.github.io/posts/2024-05-01-time-series-features/chart5.png" class="img-fluid"></p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>There‚Äôs a lot to unpack on feature engineering for time series forecasting. Thankfully my package, <a href="https://microsoft.github.io/finnts/index.html">finnts</a>, can automatically handle all of the feature engineering for you. It does everything I called out in this post plus more. Check it out and see just how easy ML forecasting can be.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Feature engineering is the backbone of successful time series forecasting, allowing models to uncover hidden patterns and relationships within the data, ultimately leading to more accurate predictions. By transforming raw data into meaningful features like date-related attributes, lag features, rolling window statistics, and polynomial transformations, we equip machine learning models with the necessary insights to make informed forecasts. However, it‚Äôs crucial to strike a balance between adding informative features and avoiding overfitting, as too many features can lead to poor generalization on unseen data. With careful consideration and the right techniques, feature engineering becomes a powerful tool in the arsenal of any data scientist or analyst aiming to unlock the predictive potential of time series data.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-05-01-time-series-features/</guid>
  <pubDate>Wed, 01 May 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-05-01-time-series-features/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Order Is Important</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-23-time-series-order/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-23-time-series-order/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the fifth principle of a good time series forecast, order is important. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Higher Grain Higher Accuracy</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/"><strong>Order Is Important</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="baking-cakes-over-making-smoothies" class="level3">
<h3 class="anchored" data-anchor-id="baking-cakes-over-making-smoothies">Baking Cakes Over Making Smoothies</h3>
<p>Machine learning (ML) is a lot like cooking. You have various ingredients and can combine them together in clever ways to make for a tasty dish. Most machine learning approaches like classification (predicting an outcome) and regression (predicting a number) can follow a similar process to making a smoothie. We can take some data (fruits and veggies) and blend it all together inside of our model blender.</p>
<p>Time series forecasting is a whole other beast. It still technically falls under the regression family tree but has to be handled very differently. Forecasting is more like baking a cake, where the order in which you do things is very important. For example, you cannot switch when you add the eggs and when you add the frosting. If you do you will certainly not be invited back to your nephew‚Äôs birthday party next year. In order to bake something tasty please follow the below guidance.</p>
</section>
<section id="time-series-training" class="level3">
<h3 class="anchored" data-anchor-id="time-series-training">Time Series Training</h3>
<p>Training any sort of machine learning model often requires two separate historical data sets. One that is used to train the initial model, then another that is set aside to create predictions based on the initial model. We can then see how accurate the predictions were on the test data set. This ensures that our new ML model can generalize well to new and unseen data, making sure our model doesn‚Äôt overfit to the training data.</p>
<p>Common ML approaches like classification and regression don‚Äôt need a lot of sophistication when splitting up the historical data between a training set and a testing set. Often it will be split randomly. This is similar to making a smoothie. You can randomly throw in bananas, apples, spinach, and blueberries. All without having to think about the order of when you do it.</p>
<p>Take the below housing data. This is a traditional regression problem. Let‚Äôs use the total square feet and number of bedrooms to predict how much the house will cost. We can randomly split 80% of the data to train the model, then hold out 20% of the data to test how accurate the model is. Randomly splitting the data ensures we get a healthy mix of different data in each split.</p>
<table class="caption-top table">
<caption>Example fake housing data for a regression model</caption>
<thead>
<tr class="header">
<th>Square_Feet</th>
<th>Bedrooms</th>
<th>Total_Cost</th>
<th>Split</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3774</td>
<td>2</td>
<td>822732</td>
<td>Training</td>
</tr>
<tr class="even">
<td>1460</td>
<td>1</td>
<td>245280</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>1894</td>
<td>4</td>
<td>602292</td>
<td>Training</td>
</tr>
<tr class="even">
<td>1730</td>
<td>4</td>
<td>550140</td>
<td>Training</td>
</tr>
<tr class="odd">
<td><strong>1695</strong></td>
<td><strong>4</strong></td>
<td><strong>539010</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="even">
<td>3692</td>
<td>5</td>
<td>1358656</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>2238</td>
<td>1</td>
<td>375984</td>
<td>Training</td>
</tr>
<tr class="even">
<td>2769</td>
<td>5</td>
<td>1018992</td>
<td>Training</td>
</tr>
<tr class="odd">
<td><strong>1066</strong></td>
<td><strong>5</strong></td>
<td><strong>392288</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="even">
<td>1838</td>
<td>1</td>
<td>308784</td>
<td>Training</td>
</tr>
</tbody>
</table>
<p>A time series has a built in order to it. It‚Äôs said right there in the name, time. Ignoring the order based on time can have disastrous consequences, resulting in your final future forecast not being accurate. Just like baking a cake, we need to make sure how we train a model is done in the right order. When splitting a historical time series into a training set and a testing set, splitting not at random but based on time is the proper way to go. Using the oldest data as the training set and the newest data as the testing set makes sure we respect the order of our data based on time. The example table below is a made up time series of the price of one specific house. In reality we would need a lot more data to train a good time series model but just be cool for a minute and go with me on this one. The split column now has the test data set at the very end instead of randomly split across time. We can now use <a href="https://developers.google.com/machine-learning/crash-course/framing/ml-terminology">features</a> like interest rates and gdp growth to help us forecast the price of this house over time. The first 9 months of data will train the model, and the final 3 months will be used to test the model‚Äôs accuracy.</p>
<table class="caption-top table">
<caption>Example fake time series for the price of a specific house</caption>
<colgroup>
<col style="width: 24%">
<col style="width: 23%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Interest_Rate</th>
<th>GDP_Growth</th>
<th>Total_Cost</th>
<th>Split</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2002</td>
<td>3.43635</td>
<td>1.58111</td>
<td>315052</td>
<td>Training</td>
</tr>
<tr class="even">
<td>February 2002</td>
<td>4.87679</td>
<td>0.03085</td>
<td>314723</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>March 2002</td>
<td>4.32998</td>
<td>-0.04544</td>
<td>312854</td>
<td>Training</td>
</tr>
<tr class="even">
<td>April 2002</td>
<td>3.99665</td>
<td>-0.04149</td>
<td>311865</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>May 2002</td>
<td>2.89005</td>
<td>0.26061</td>
<td>309452</td>
<td>Training</td>
</tr>
<tr class="even">
<td>June 2002</td>
<td>2.88999</td>
<td>0.81189</td>
<td>311106</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>July 2002</td>
<td>2.64521</td>
<td>0.57986</td>
<td>309675</td>
<td>Training</td>
</tr>
<tr class="even">
<td>August 2002</td>
<td>4.66544</td>
<td>0.22807</td>
<td>314681</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>September 2002</td>
<td>4.00279</td>
<td>1.02963</td>
<td>315097</td>
<td>Training</td>
</tr>
<tr class="even">
<td><strong>October 2002</strong></td>
<td><strong>4.27018</strong></td>
<td><strong>-0.15127</strong></td>
<td><strong>312357</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="odd">
<td><strong>November 2002</strong></td>
<td><strong>2.55146</strong></td>
<td><strong>0.23036</strong></td>
<td><strong>308345</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="even">
<td><strong>December 2002</strong></td>
<td><strong>4.92477</strong></td>
<td><strong>0.41591</strong></td>
<td><strong>316022</strong></td>
<td><strong>Testing</strong></td>
</tr>
</tbody>
</table>
</section>
<section id="data-leakage" class="level3">
<h3 class="anchored" data-anchor-id="data-leakage">Data Leakage</h3>
<p>Whenever time is involved in machine learning, the probability of shooting yourself in the foot rises. This has to do with the concept of <a href="https://machinelearningmastery.com/data-leakage-machine-learning/">data leakage</a>. Data leakage occurs when information from outside the training dataset is used to create the model, leading it to make overly optimistic predictions. It can also happen when we train with data that may not be available in the future when we need to create new predictions.</p>
<p>In time series forecasting we have already discussed one component of data leakage, related to splitting the data correctly based on time. Take the below table, instead of splitting properly by time the data is now split randomly. Our model can now ‚Äúsee ahead in time‚Äù when training, and in effect cheat when being evaluated on the testing splits. For example, for the test observation in July 2002 the model can learn from data on either side of that month. Figuring out previous and future trends and seasonality. This makes it easy to predict what the housing cost in July should be, since it has information before and after that month. With this approach our test accuracy will be a lot better than in the previous example where the splits are based on time. Future forecast performance will suffer though, since we have now trained and chosen a model that may only be good at figuring out how to extrapolate between two points, instead of trying to create predictions on unseen data in the future.</p>
<table class="caption-top table">
<caption>Incorrect train and test splits</caption>
<colgroup>
<col style="width: 24%">
<col style="width: 23%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>Date</th>
<th>Interest_Rate</th>
<th>GDP_Growth</th>
<th>Total_Cost</th>
<th>Split</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>January 2002</td>
<td>3.43635</td>
<td>1.58111</td>
<td>315052</td>
<td>Training</td>
</tr>
<tr class="even">
<td><strong>February 2002</strong></td>
<td><strong>4.87679</strong></td>
<td><strong>0.03085</strong></td>
<td><strong>314723</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="odd">
<td>March 2002</td>
<td>4.32998</td>
<td>-0.04544</td>
<td>312854</td>
<td>Training</td>
</tr>
<tr class="even">
<td>April 2002</td>
<td>3.99665</td>
<td>-0.04149</td>
<td>311865</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>May 2002</td>
<td>2.89005</td>
<td>0.26061</td>
<td>309452</td>
<td>Training</td>
</tr>
<tr class="even">
<td>June 2002</td>
<td>2.88999</td>
<td>0.81189</td>
<td>311106</td>
<td>Training</td>
</tr>
<tr class="odd">
<td><strong>July 2002</strong></td>
<td><strong>2.64521</strong></td>
<td><strong>0.57986</strong></td>
<td><strong>309675</strong></td>
<td><strong>Testing</strong></td>
</tr>
<tr class="even">
<td>August 2002</td>
<td>4.66544</td>
<td>0.22807</td>
<td>314681</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>September 2002</td>
<td>4.00279</td>
<td>1.02963</td>
<td>315097</td>
<td>Training</td>
</tr>
<tr class="even">
<td>October 2002</td>
<td>4.27018</td>
<td>-0.15127</td>
<td>312357</td>
<td>Training</td>
</tr>
<tr class="odd">
<td>November 2002</td>
<td>2.55146</td>
<td>0.23036</td>
<td>308345</td>
<td>Training</td>
</tr>
<tr class="even">
<td><strong>December 2002</strong></td>
<td><strong>4.92477</strong></td>
<td><strong>0.41591</strong></td>
<td><strong>316022</strong></td>
<td><strong>Testing</strong></td>
</tr>
</tbody>
</table>
<p>Ok, so we know not to split data randomly when training. Another thing to watch out for is how features are used in the model. In our time series housing example, we can use date information (month, quarter, etc) along with our macro features like interest rates and GDP growth. Let‚Äôs say we follow the right approach, split the data based on time, and see that we get good results on the test data. We can now take our model into production and try to create a forecast for the future. But wait, what do we do with the future feature values of interest rate and GDP growth? This is another potential data leakage issue, where data used to train the model is not available to create new predictions in the future. You might be thinking, no problem we can just create a forecast of future interest rates and gdp growth right? Wrong. If you can produce accurate interest rate and GDP growth forecasts, then you shouldn‚Äôt be reading this post. You should instead be sitting on your own private island, watching the return on your flagship hedge fund skyrocket. See where I‚Äôm going here? If you cannot perfectly predict future values of the feature you want to use, then you should consider not using that feature. You might be able to know with 100% certainty when a holiday or special event will take place, but not even expert economists can perfectly predict interest rate fluctuations. Instead you can look at using feature lags. Where instead of using real time interest rates or GDP growth you can instead use lags of them. For example, using a 3 or 12 month lag of each feature. Using lags in this example can help prevent <a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/#reversal">compounding errors</a> in your forecast.</p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>Looking for a way to to never worry about the order of your time series data again? Have no fear because <a href="https://microsoft.github.io/finnts/index.html">finnts</a> is here! Ok enough with the used car salesman talk. The finnts package is something myself and other outstanding team members have built to automate all of the tedious aspects related to time series forecasting. The package can automatically handle the proper splits of your data and has build in data leakage prevention. Check out the package to learn for yourself how easy forecasting can be.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Remember, order is important in forecasting. Make sure you don‚Äôt mix up your data when training models, and keep a look out for data leakage. Do this right and you just might get invited back to your nephew‚Äôs birthday party next year.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-04-23-time-series-order/</guid>
  <pubDate>Tue, 23 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-23-time-series-order/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Weekend Reads (4/19/24)</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-19-weekend-reads/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-19-weekend-reads/image.png" class="img-fluid"></p>
<section id="articles" class="level2">
<h2 class="anchored" data-anchor-id="articles">Articles</h2>
<ul>
<li><a href="https://hbr.org/2023/12/use-strategic-thinking-to-create-the-life-you-want">Use Strategic Thinking to Create the Life You Want</a></li>
<li><a href="https://www.maximiliankiener.com/digitalprojects/time/">How Time Flies</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/">Time Series First Principles: Higher Grain, Higher Accuracy</a></li>
</ul>
</section>
<section id="videos" class="level2">
<h2 class="anchored" data-anchor-id="videos">Videos</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=5N4UFl9G00A">Dave Chappelle - Unforgiven</a></li>
<li><a href="https://www.youtube.com/watch?v=-9ROlCeB5FQ">Balaji on AI Gods</a></li>
<li><a href="https://www.youtube.com/watch?v=SEnuWRLMI88">Building Laser Focus</a></li>
<li><a href="https://www.youtube.com/watch?v=5qGItht6U0E">Overrated and Underrated Habits</a></li>
<li><a href="https://www.youtube.com/watch?v=FPFqB1P9BIo">Lessons Learned on Writing</a></li>
</ul>
</section>
<section id="sites" class="level2">
<h2 class="anchored" data-anchor-id="sites">Sites</h2>
<ul>
<li><a href="https://suno.com/">Make Music with AI</a></li>
</ul>
</section>
<section id="tweets" class="level2">
<h2 class="anchored" data-anchor-id="tweets">Tweets</h2>
<ul>
<li><a href="https://twitter.com/trungtphan/status/1780794237426323918?s=46&amp;t=8Xa2BngQ9d359SJzFrCFMA">Building an Engineering Dream Team</a></li>
<li><a href="https://twitter.com/tunguz/status/1779532581274570991?s=46&amp;t=8Xa2BngQ9d359SJzFrCFMA">Experiences &gt; Things</a></li>
</ul>


</section>

 ]]></description>
  <category>weekend-reads</category>
  <guid>https://mftokic.github.io/posts/2024-04-19-weekend-reads/</guid>
  <pubDate>Fri, 19 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-19-weekend-reads/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Time Series First Principles: Higher Grain Higher Accuracy</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-18-time-series-grain/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/image.png" class="img-fluid"></p>
<section id="time-series-first-principles-series" class="level3">
<h3 class="anchored" data-anchor-id="time-series-first-principles-series">Time Series First Principles Series</h3>
<p>This post dives into the fourth principle of a good time series forecast, the higher the grain the higher the accuracy. Check out the <a href="https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/">initial post</a> in this series to get a high level view of each principle.</p>
<ol type="1">
<li><a href="https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/">Domain Expertise</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Garbage In Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">The Future Is Similar To The Past</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-18-time-series-grain/"><strong>Higher Grain Higher Accuracy</strong></a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-23-time-series-order/">Order Is Important</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-01-time-series-features/">The Magic Is In The Feature Engineering</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/">Simple Models Are Better Models</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-07-time-series-capture-uncertainty/">Capture Uncertainty</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-28-time-series-model-avg/">Model Combinations Are King</a></li>
<li><a href="https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/">Deep Learning Last</a></li>
</ol>
</section>
<section id="clear-skies" class="level3">
<h3 class="anchored" data-anchor-id="clear-skies">Clear Skies</h3>
<p>When planes take off from the ground they climb high into the sky. During that 5-10 minute period passengers have to stay seated with their seatbelt fastened. It‚Äôs only after the plane reaches 10,000 feet people can start to get up and move around the plane. Eventually the plane can reach an altitude of 40,000 feet. To compare, the peak of Mount Everest is 29,000 feet off the ground. Planes go up that high because it‚Äôs easier to fly the plane and more efficient. If planes flew a few feet off the ground it would be a lot bumpier ride, having to deal with changing weather and turbulence.</p>
<p>Forecasting is similar to flying a plane. Training a machine learning (ML) model at a higher grain of data is akin to a plane climbing in altitude. There is less turbulence (noise) in the data and your forecast has a better chance of being more accurate. You can either climb in altitude at the individual time series grain, or by the date grain. Let‚Äôs discuss each of them along with other methods.</p>
</section>
<section id="time-series-grain" class="level3">
<h3 class="anchored" data-anchor-id="time-series-grain">Time Series Grain</h3>
<p>Think of a higher time series grain as an aggregation of your original data. For example you might have product sales across a bunch of cities, where each city is a time series. Individual cities might have hard to model trend and seasonality, but when combined at a total country-level, it can be easier to model. Take the example charts below. Each city might be noisy but climbing in altitude up to the county level makes it easier to spot trends and seasonality.</p>
<p><img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/chart2.png" class="img-fluid"> <img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/chart1.png" class="img-fluid"></p>
</section>
<section id="date-grain" class="level3">
<h3 class="anchored" data-anchor-id="date-grain">Date Grain</h3>
<p>In a perfect world we would be able to forecast our businesses down to the day, or even minute, across the next 10 years. This is sadly not the case. The more granular you try to forecast at the date grain, the noisier the data is going to be, and the harder it will be to create accurate forecasts. Unless there is an absolute need to forecast at a certain level, I almost always recommend a higher date grain. Take the example charts below. See how aggregating daily data to a monthly date grain level makes it easier to spot trends and seasonality.</p>
<p><img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/chart3.png" class="img-fluid"> <img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/chart4.png" class="img-fluid"></p>
<p>In the finance org, the most common date grain to forecast at is month. This gives a healthy balance of being able to forecast long periods of time while also being able to update your forecast with new historical data every few weeks.</p>
<p>Here is another important point to call out. The longer your forecast horizon, the higher the date grain you should forecast at. Here are some recommendations based on your forecast horizon (how many periods you want to forecast). For example, if you‚Äôre trying to forecast out the next 6 months at the daily grain, you might get better results if you aggregate up to the monthly grain and forecast by month instead.</p>
<ul>
<li>Daily Grain: 1-90 day forecast horizon</li>
<li>Weekly Grain: 1-12 week forecast horizon</li>
<li>Monthly Grain: 1-18 month forecast horizon</li>
<li>Quarterly Grain: 1-8 quarter forecast horizon</li>
<li>Yearly Grain: more than 1 year forecast horizon</li>
</ul>
</section>
<section id="hierarchical-forecasting" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-forecasting">Hierarchical Forecasting</h3>
<p>A potential ‚Äúbest of both worlds‚Äù solution to the data grain issue is to use a hierarchical forecast. This is where you can train models at different grains of the data, then use a statistical process to reconcile each forecast together so they are in sync. Our forecasting godfather, Rob Hyndman, has done a lot of great work in this space. Here is a <a href="https://otexts.com/fpp3/hierarchical.html">chapter from his book</a> on hierarchical forecasting.</p>
<p>Let‚Äôs go back to our time series grain example. Using a hierarchical forecast you could train models and create forecasts for each city, then do the same at the total country-level, then finally do the same at a total world wide level across all countries. This is a standard hierarchical approach shown in the chart below. This hierarchical process blends a ‚Äúbottoms up‚Äù forecast of creating predictions at the lowest level by city, with a ‚Äútops down‚Äù forecast of creating predictions at the highest global level. A statistical process is then used to make the ‚Äútops down‚Äù forecast equal the ‚Äúbottoms up‚Äù forecast, optimizing for accuracy at all levels of the hierarchy.</p>
<p><img src="https://mftokic.github.io/posts/2024-04-18-time-series-grain/chart5.png" class="img-fluid"></p>
<p>The same idea can be applied at the date grain too. Where you can forecast at the daily level, weekly level, and monthly level. Then use a reconciliation process to get the final forecast at the daily level that is also accurate when summing up by month. This can work well if a monthly forecast is more accurate, but the final forecast needs to be at a daily level.</p>
</section>
<section id="allocations" class="level3">
<h3 class="anchored" data-anchor-id="allocations">Allocations</h3>
<p>Another option is to take a forecast at a higher grain and allocate it down to a lower grain using simple allocation logic. This process can replace the more complicated hierarchical forecasting discussed earlier. Simple allocations can be done in two ways.</p>
<p>The first is to take historical values and create a percent split to apply to the final forecast. For example we can create a forecast at the country-level, then split that out by city. The split percent by city (allocation percent) can be calculated based on how much each city was the percent of total country over the last few years. This can be broken down by period. So you can get a specific percent split for each month on average in the past. This approach helps maintain historical seasonality across each time series (each city). See the charts below for an example of using two historical years of monthly data to create the final allocation percentages.</p>
<table class="caption-top table">
<caption>Historical splits by city</caption>
<thead>
<tr class="header">
<th>Month</th>
<th>City A %</th>
<th>City B %</th>
<th>City C %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jan 2021</td>
<td>30.87%</td>
<td>30.08%</td>
<td>39.05%</td>
</tr>
<tr class="even">
<td>Feb 2021</td>
<td>28.11%</td>
<td>23.90%</td>
<td>47.99%</td>
</tr>
<tr class="odd">
<td>Mar 2021</td>
<td>23.77%</td>
<td>47.43%</td>
<td>28.81%</td>
</tr>
<tr class="even">
<td>Apr 2021</td>
<td>31.26%</td>
<td>18.42%</td>
<td>50.32%</td>
</tr>
<tr class="odd">
<td>May 2021</td>
<td>39.50%</td>
<td>34.08%</td>
<td>26.42%</td>
</tr>
<tr class="even">
<td>Jun 2021</td>
<td>50.58%</td>
<td>30.98%</td>
<td>18.44%</td>
</tr>
<tr class="odd">
<td>Jan 2022</td>
<td>36.33%</td>
<td>31.79%</td>
<td>31.88%</td>
</tr>
<tr class="even">
<td>Feb 2022</td>
<td>19.79%</td>
<td>40.74%</td>
<td>39.47%</td>
</tr>
<tr class="odd">
<td>Mar 2022</td>
<td>19.95%</td>
<td>28.99%</td>
<td>51.06%</td>
</tr>
<tr class="even">
<td>Apr 2022</td>
<td>20.37%</td>
<td>26.76%</td>
<td>52.87%</td>
</tr>
<tr class="odd">
<td>May 2022</td>
<td>41.37%</td>
<td>41.53%</td>
<td>17.10%</td>
</tr>
<tr class="even">
<td>Jun 2022</td>
<td>43.86%</td>
<td>41.10%</td>
<td>15.04%</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<caption>Average of city split by month</caption>
<thead>
<tr class="header">
<th>Month</th>
<th>City A %</th>
<th>City B %</th>
<th>City C %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jan</td>
<td>33.60%</td>
<td>30.93%</td>
<td>35.46%</td>
</tr>
<tr class="even">
<td>Feb</td>
<td>23.95%</td>
<td>32.32%</td>
<td>43.73%</td>
</tr>
<tr class="odd">
<td>Mar</td>
<td>21.86%</td>
<td>38.21%</td>
<td>39.93%</td>
</tr>
<tr class="even">
<td>Apr</td>
<td>25.82%</td>
<td>22.59%</td>
<td>51.60%</td>
</tr>
<tr class="odd">
<td>May</td>
<td>40.43%</td>
<td>37.81%</td>
<td>21.76%</td>
</tr>
<tr class="even">
<td>Jun</td>
<td>47.22%</td>
<td>36.04%</td>
<td>16.74%</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<caption>Final forecast using the city splits</caption>
<colgroup>
<col style="width: 8%">
<col style="width: 16%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Month</th>
<th style="text-align: right;">Country Forecast</th>
<th style="text-align: right;">City A %</th>
<th style="text-align: right;">City B %</th>
<th style="text-align: right;">City C %</th>
<th style="text-align: right;">City A Forecast</th>
<th style="text-align: right;">City B Forecast</th>
<th style="text-align: right;">City C Forecast</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Jan 2023</td>
<td style="text-align: right;">15000</td>
<td style="text-align: right;">33.60</td>
<td style="text-align: right;">30.93</td>
<td style="text-align: right;">35.46</td>
<td style="text-align: right;">5040</td>
<td style="text-align: right;">4639.5</td>
<td style="text-align: right;">5319</td>
</tr>
<tr class="even">
<td style="text-align: left;">Feb 2023</td>
<td style="text-align: right;">15200</td>
<td style="text-align: right;">23.95</td>
<td style="text-align: right;">32.32</td>
<td style="text-align: right;">43.73</td>
<td style="text-align: right;">3640.4</td>
<td style="text-align: right;">4912.64</td>
<td style="text-align: right;">6646.96</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mar 2023</td>
<td style="text-align: right;">15400</td>
<td style="text-align: right;">21.86</td>
<td style="text-align: right;">38.21</td>
<td style="text-align: right;">39.93</td>
<td style="text-align: right;">3366.44</td>
<td style="text-align: right;">5884.34</td>
<td style="text-align: right;">6149.22</td>
</tr>
<tr class="even">
<td style="text-align: left;">Apr 2023</td>
<td style="text-align: right;">15600</td>
<td style="text-align: right;">25.82</td>
<td style="text-align: right;">22.59</td>
<td style="text-align: right;">51.60</td>
<td style="text-align: right;">4027.92</td>
<td style="text-align: right;">3524.04</td>
<td style="text-align: right;">8049.6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">May 2023</td>
<td style="text-align: right;">15800</td>
<td style="text-align: right;">40.43</td>
<td style="text-align: right;">37.81</td>
<td style="text-align: right;">21.76</td>
<td style="text-align: right;">6387.94</td>
<td style="text-align: right;">5973.98</td>
<td style="text-align: right;">3438.08</td>
</tr>
<tr class="even">
<td style="text-align: left;">Jun 2023</td>
<td style="text-align: right;">16000</td>
<td style="text-align: right;">47.22</td>
<td style="text-align: right;">36.04</td>
<td style="text-align: right;">16.74</td>
<td style="text-align: right;">7555.2</td>
<td style="text-align: right;">5766.4</td>
<td style="text-align: right;">2678.4</td>
</tr>
</tbody>
</table>
<p>The second approach is to use a future forecast to create the allocation splits. For example we can create future forecasts at the country-level and also at the city-level. Then we can create the split percent for each city by taking the city forecast and summing it up to the country-level, then taking the percent split for each city. These splits can then be applied to the final country-level forecast to get the final forecast by city. This approach uses the more robust country-level forecast, while still trying to capture future changing trends and seasonality by city.</p>
<table class="caption-top table">
<caption>Initial forecast, where country and each city are forecasted separately</caption>
<colgroup>
<col style="width: 12%">
<col style="width: 22%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Month</th>
<th>Country Forecast</th>
<th>City A Forecast</th>
<th>City B Forecast</th>
<th>City C Forecast</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jan 2023</td>
<td>10,000</td>
<td>4,000</td>
<td>3,500</td>
<td>2,000</td>
</tr>
<tr class="even">
<td>Feb 2023</td>
<td>10,500</td>
<td>4,200</td>
<td>3,000</td>
<td>3,300</td>
</tr>
<tr class="odd">
<td>Mar 2023</td>
<td>11,000</td>
<td>4,500</td>
<td>3,200</td>
<td>3,300</td>
</tr>
<tr class="even">
<td>Apr 2023</td>
<td>11,500</td>
<td>4,800</td>
<td>3,400</td>
<td>3,300</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>12,000</td>
<td>5,000</td>
<td>3,500</td>
<td>3,500</td>
</tr>
<tr class="even">
<td>Jun 2023</td>
<td>12,500</td>
<td>5,200</td>
<td>3,800</td>
<td>3,500</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<caption>Calculating the percent splits by city</caption>
<thead>
<tr class="header">
<th>Month</th>
<th>City A %</th>
<th>City B %</th>
<th>City C %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jan 2023</td>
<td>42.11%</td>
<td>36.84%</td>
<td>21.05%</td>
</tr>
<tr class="even">
<td>Feb 2023</td>
<td>40.00%</td>
<td>28.57%</td>
<td>31.43%</td>
</tr>
<tr class="odd">
<td>Mar 2023</td>
<td>40.91%</td>
<td>29.09%</td>
<td>30.00%</td>
</tr>
<tr class="even">
<td>Apr 2023</td>
<td>41.74%</td>
<td>29.57%</td>
<td>28.70%</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>41.67%</td>
<td>29.17%</td>
<td>29.17%</td>
</tr>
<tr class="even">
<td>Jun 2023</td>
<td>41.60%</td>
<td>30.40%</td>
<td>28.00%</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<caption>Final forecast after applying the city splits to the country-level forecast</caption>
<colgroup>
<col style="width: 12%">
<col style="width: 29%">
<col style="width: 29%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Month</th>
<th>City A Final Forecast</th>
<th>City B Final Forecast</th>
<th>City C Final Forecast</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Jan 2023</td>
<td>4,211</td>
<td>3,684</td>
<td>2,105</td>
</tr>
<tr class="even">
<td>Feb 2023</td>
<td>4,200</td>
<td>3,000</td>
<td>3,300</td>
</tr>
<tr class="odd">
<td>Mar 2023</td>
<td>4,500</td>
<td>3,200</td>
<td>3,300</td>
</tr>
<tr class="even">
<td>Apr 2023</td>
<td>4,800</td>
<td>3,400</td>
<td>3,300</td>
</tr>
<tr class="odd">
<td>May 2023</td>
<td>5,000</td>
<td>3,500</td>
<td>3,500</td>
</tr>
<tr class="even">
<td>Jun 2023</td>
<td>5,200</td>
<td>3,800</td>
<td>3,500</td>
</tr>
</tbody>
</table>
</section>
<section id="reversal" class="level3">
<h3 class="anchored" data-anchor-id="reversal">Reversal</h3>
<p>A more granular forecast can sometimes be more accurate, especially if the more detailed grain uncovers more stable trends and seasonality that can be modeled. Take for example a product whose sales are impacted by Chinese New Year. That holiday doesn‚Äôt happen on the same day every year, and it can even happen in different months. Sometimes in January, and sometimes in February. Since it happens over multiple days the split between the two months can change drastically from year to year. Creating a forecast at the daily level, adding information around when Chinese New Year is happening, could result in a more accurate forecast. You could also take the approach of a monthly forecast, and have a numeric feature that lists how many days of Chinese New Year falls within each month.</p>
<p>If your initial data at the higher grain is noisy or has low forecast accuracy, consider asking the domain expert if there could be more insightful trends and seasonality at a lower grain.</p>
</section>
<section id="finnts" class="level3">
<h3 class="anchored" data-anchor-id="finnts">finnts</h3>
<p>Hierarchical forecasting is a tricky business, thankfully my open-source package <a href="https://microsoft.github.io/finnts/index.html">finnts</a> can automatically do hierarchical forecasting. The package can even use external regressors (features) in the hierarchical approach! Today finnts supports hierarchical forecasting at the time series grain. Hopefully one day we will implement hierarchical forecasting at the date grain, stay tuned. This is the same package I use internally at my job, allowing my company to replace hundreds of billions of manual forecasts with machine learning. Check out the package and see for yourself.</p>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>Just as pilots navigate to higher altitudes to find smoother skies and better efficiency, so too must we elevate our approach to data granularity in forecasting when needed. By stepping back from the minutiae of daily or city-level data and ascending to monthly or country-level aggregations, we enable our models to capture more coherent patterns and deliver forecasts with improved precision. This strategic shift‚Äîfrom a granular view to a broader perspective‚Äîis not just about avoiding turbulence; it‚Äôs about leveraging stability to enhance predictability.</p>
<p>However, the real magic often lies in blending these approaches through hierarchical forecasting. This method combines the detailed insights available at lower levels with the clarity and simplicity of higher-level forecasts, ensuring both depth and breadth in our predictive capabilities. As we continue to refine our techniques and tools, like the finnts package, we are paving the way for a future where complex, multi-tiered forecasting is as streamlined as a flight cruising at 40,000 feet.</p>
<p>In your journey through data, remember that the right altitude can make all the difference. Rising above the noise can provide not just clearer views, but also far-reaching insights. So, buckle up‚Äîwe‚Äôre about to take forecasting to new heights.</p>


</section>

 ]]></description>
  <category>time-series</category>
  <category>machine-learning</category>
  <category>finance</category>
  <guid>https://mftokic.github.io/posts/2024-04-18-time-series-grain/</guid>
  <pubDate>Thu, 18 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-18-time-series-grain/image.png" medium="image" type="image/png" height="82" width="144"/>
</item>
<item>
  <title>Weekend Reads (4/12/24)</title>
  <dc:creator>Mike Tokic</dc:creator>
  <link>https://mftokic.github.io/posts/2024-04-12-weekend-reads/</link>
  <description><![CDATA[ 





<p><img src="https://mftokic.github.io/posts/2024-04-12-weekend-reads/image.png" class="img-fluid"></p>
<section id="articles" class="level2">
<h2 class="anchored" data-anchor-id="articles">Articles</h2>
<ul>
<li><a href="https://tim.blog/wp-content/uploads/2020/01/17-Questions-That-Changed-My-Life.pdf">17 Important Questions</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-08-time-series-garbage/">Time Series First Principles: Garbage In, Garbage Out</a></li>
<li><a href="https://mftokic.github.io/posts/2024-04-11-time-series-past-future/">Time Series First Principles: The Future Is Similar To The Past</a></li>
</ul>
</section>
<section id="videos" class="level2">
<h2 class="anchored" data-anchor-id="videos">Videos</h2>
<ul>
<li><a href="https://youtu.be/Z2BnqYArwaw?si=tfQXzl1Zu4Zfom9R">Telling Good Stories</a></li>
<li><a href="https://youtu.be/_ZJpU43NA0c?si=-kIn8vjU9-PYWaE2">Power of Being a Contrarian</a></li>
<li><a href="https://youtu.be/sgVDljNavSc?si=YnSlsK_JgUqlv6Lp">Making Friends as an Adult</a></li>
</ul>
</section>
<section id="podcasts" class="level2">
<h2 class="anchored" data-anchor-id="podcasts">Podcasts</h2>
<ul>
<li><a href="https://open.spotify.com/episode/1Ab1UcjpAvbhAhUslDt0kA?si=oGRuirSFQaWt1G6z_gawaw">Protocols to Improve Your Sleep, Huberman Lab</a></li>
</ul>
</section>
<section id="tweets" class="level2">
<h2 class="anchored" data-anchor-id="tweets">Tweets</h2>
<ul>
<li><a href="https://x.com/jasonfried/status/1775918262259536230">Power of Motivation</a></li>
<li><a href="https://x.com/JamesLucasIT/status/1777026561947963709">Our Planet Rocks</a></li>
<li><a href="https://x.com/DeepLearningAI/status/1777345409007952098">Shipping on Fridays</a></li>
<li><a href="https://x.com/Alkibiades_/status/1777418763316466080">Cool Idea Around Building the Pyramids</a></li>
</ul>
</section>
<section id="books" class="level2">
<h2 class="anchored" data-anchor-id="books">Books</h2>
<ul>
<li><a href="https://www.amazon.com/Sapiens-Humankind-Yuval-Noah-Harari-ebook/dp/B00ICN066A/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;dib_tag=se&amp;dib=eyJ2IjoiMSJ9.04mm2YQe9BvKUhAU-tuaKKTp_ekz9KX-YILKoI3_bfIEBt49_HpfeyFUJJlvIwSWAIPWrL_6wT9ArXYinHBdd26BvpLuJiO8L-PDmeH6Bp72MBNaBM3BIkDJczpjw__nnhyQBkyPBqqAxj5-FZHbC_wDc7NP_EpLiA253DUGUSSl1faJjZ8ThMS_HxUzPgC0WHQZRl5HkHwYJMTQgcFc-mGrBXCLrlR63IS6_1cyxs4.80oPRRInfnlKdy_MYM4yAdkI9w0YEtNugcGm04aOaDM&amp;qid=1712934638&amp;sr=8-1">Sapiens by Yuval Noah Harari</a></li>
</ul>


</section>

 ]]></description>
  <category>weekend-reads</category>
  <guid>https://mftokic.github.io/posts/2024-04-12-weekend-reads/</guid>
  <pubDate>Fri, 12 Apr 2024 07:00:00 GMT</pubDate>
  <media:content url="https://mftokic.github.io/posts/2024-04-12-weekend-reads/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
</channel>
</rss>
