[
  {
    "objectID": "start_here.html",
    "href": "start_here.html",
    "title": "Learn How I Think",
    "section": "",
    "text": "My Journey in Microsoft‚Äôs Finance Rotation Program\nFirst Principles of Time Series Forecasting\nPersonal User Manual\nThree Levels of Machine Learning Adoption in Finance\nPower"
  },
  {
    "objectID": "start_here.html#binge-my-best-stuff",
    "href": "start_here.html#binge-my-best-stuff",
    "title": "Learn How I Think",
    "section": "",
    "text": "My Journey in Microsoft‚Äôs Finance Rotation Program\nFirst Principles of Time Series Forecasting\nPersonal User Manual\nThree Levels of Machine Learning Adoption in Finance\nPower"
  },
  {
    "objectID": "posts/2024-06-28-life-circles/index.html",
    "href": "posts/2024-06-28-life-circles/index.html",
    "title": "Inner Circles of Life",
    "section": "",
    "text": "One of my siblings recently got married and had a baby. As they were living this new life, myself and other family members realized something. They now had less time for us. Instead of coming around to the standard family events like they used to, they now had to ‚Äúsqueeze us in‚Äù between other things going on with their life. This become the most apparent around Christmas time. My sibling and their family now only stopped by for a few hours around the Christmas holiday, and on Christmas day this meant only seeing them for 2-3 hours. In previous years we would have been together nonstop. This broke my Mom‚Äôs heart. She could no longer be with all of her kids 24/7 during holidays and major life events.\nIt was during this Christmas that I realized something. My sibling had to deprioritize us for their spouse and eventually their new child. And that‚Äôs perfectly ok. They now had a new top priority in life, and myself and other family and friends are now lower down on the list. A new inner circle of priorities formed for them, and myself and others were no longer in it. We had to make due with this new reality and understand it could get worse in the future. This kind of deprioritization also happens with friends too. It just gets hard to prioritize people in your life as new people come into it.\nThe list of life priorities is something I like the call the ‚Äúinner circles of life‚Äù. Sounds fancy but it‚Äôs just a list of what you truly prioritize and make time for in life. Your inner most cirlce is your top prioritiy. Then circles form around and outward. With each new circle, your priorities of things in that circle drops. Let‚Äôs see how that changes as we get older.\n\n\nAge 0-1\n\nMom\n\nWhen you‚Äôre born, the only person that exists is your mother. Dad, who‚Äôs Dad? What‚Äôs a Dad? You have no idea. The only person you recognize and bond with is your Mom. No one else comes close.\n\n\nAge 1-6\n\nMom\nDad\nGrandparents, Aunt/Uncle\n\nOk, now Dad comes into the picture as the second circle of your life, but Mom still holds the inner most circle. You also now get to know there are other people who love you unconditionally. These people are called Grandparents and they‚Äôre awesome. All they do is give you hugs and tasty food, life is great when they are around. Aunts and Uncles come into the picture to. They kind of look like Mom and Dad but smell different and always seem to get out of having to change your diaper. Lucky them.\n\n\nAge 7-12\n\nParents\nFriends\nSiblings\nRelatives\n\nYour parents still keep the top spot, but now they are more like a combined unit. Not just separate people but a singular force. Now they are telling you to do chores and keep an eye on your brothers and sisters. Who the heck are they? These people look like me but have different interests and personalities. And hey, they‚Äôre mean! We don‚Äôt get along that well. We always fight. So they are definitely farther down on the list. Same goes with Grandparents and other relatives. We still enjoy seeing them, but we‚Äôd rather hang out more with these people who go to school with you. People your own age who have the same interests as you. Who you see every day at school. People who give you their pudding cup just because you looked hungry. These people are your friends, and they are the best. They now take a higher spot on the list. Life starts to revolve more around your friends and less around your other family members.\n\n\nAge 13-25\n\nFriends\nRomantic Partners\nSiblings\nParents\nRelatives\n\nThis is what I have come to call the ‚Äúdark decade‚Äù. A time where you kinda suck as a person. You might hate your parents. You might hate school. You might just hate the world. Don‚Äôt worry that‚Äôs just your emo phase, it‚Äôll pass. Friends become your top priority in life. School might be on that list too but it most likely won‚Äôt come until your college years, so I‚Äôve left it off the list for now. There are now others who might have initially looked like friends. But who you now see in a different light. They smell nice. They have shiny hair. They seem cool. You‚Äôd like to get to know them more. Maybe even kiss them right on the mouth. Yes, these are people I call romantic partners. Hopefully they do not occupy the top spot on the list. You know the saying, bros before ____ right? Wrong. At times a girlfriend, boyfriend, or someone you admire (who might not know you even exist) could take the top spot. You might even go to a specific college on the other side of the country for them. This dark decade is where mistakes happen. Where you fail a lot. Do dumb things. Thankfully all of this dumb stuff happens when you‚Äôre at a school of some sort, so the mistakes are temporary. These mistakes are things you learn from and grow into a better person (hopefully).\n\n\nAge 22-26\n\nJob\nRomantic Partners\nFriends\nParents\nSiblings\nRelatives\n\nNow you‚Äôre in the real world. And you need money to live. Work has now become your top priority. Don‚Äôt believe me? How many of your friends took jobs in different cities after college graduation? Did you break up with your romantic partner because you both were headed to different cities to start your careers? Yup, it happens. It‚Äôs ok to have your job be the top priority. You need to establish yourself at this stage in life. Build a career that‚Äôs going somewhere. You also might be in a serious relationship with someone who smells nice and has shiny hair. Lucky you. This person might even move in with you. Your roommate used to be your best friend. Now you‚Äôve kicked them out for a different kind of best friend, one you may want to spend every day of the rest of your life with. Friends are still high up on the list, but they might live in a different city now. You take trips to visit them, but you only have so many vacation days off work. You also need to balance that with time to see your parents and other family members. Now you have too many life balls in the air to juggle, so some might get dropped. When was the last time you called your Grandparents? Call them now.\n\n\nAge 25-35\n\nSpouse\nJob\nParents\nSiblings\nFriends\nRelatives\n\nBy now you might have married your roommate who smells nice. Where you live and sometimes changing jobs are based on this other person. They are now the center of your world. Having a job and good friends are still high up on the list, but those fall by the wayside compared to your new spouse. You are now out of the ‚Äúdark decade‚Äù, so naturally your parents and siblings become fun again. You genuinely enjoy hanging out with them. And miss them when they‚Äôre not around. Now that you‚Äôre starting to see your family more, and your job responsibilities are heating up, all of a sudden you can‚Äôt see your friends +3x a week. Some of your friends might move away, back to their or their spouse‚Äôs hometown. It happens, and it kinda sucks. But that‚Äôs life. They‚Äôre also dealing with their own priorities just like you.\n\n\nAge 28-38\n\nKids\nSpouse\nJob\nParents\nSiblings\nFriends\nRelatives\n\nThe most beautiful thing in the world happens. A baby comes into your life. Everything else is meaningless. The only the thing that matters is making sure this child is happy and healthy. Your kids become your inner most circle. Everything else gets bumped down the list of priorities. You now realize you need baby sitters, because eventually you might have to return to work. This is where parents and siblings come in. Now you‚Äôre closer than ever with them. Friends visit you, but that weekly poker game or all night weekend party is now out of the question. You have someone to feed and someone to love. Life is beautiful. Work falls on the list too. Late night and weekend working sessions become harder. Now you have to tradeoff time at work with time with your child. This becomes a hard choice that has been argued thousands of times by smart people. The answer is hard. Thankfully by now you have built up some career capital. Meaning you can use your seniority and expertise at your company to guard your time more. Only work on the biggest impact items instead of the grunt work you did at the start of your career. They say you can have everything in life, just not all at once. Choices have to be made. Just know the tradeoffs of each one. Make sure you define your own definition of success in life. Which can be truly anything. No one has the right answer. No one has it all figured out.\n\n\nAge 35+\n\nKids\nSpouse\nParents\nSiblings\nFriends\nJob\nRelatives\n\nHopefully by your mid to late thirties you are able to find a nice smelling person. Maybe even raise some rugrats. By this time you have also built up a lot of career capital. Maybe this means you can now do your job how and when you‚Äôd like. Maybe your job is still demanding most of your time. Good news, things will only get worse. More people will ask for your time. Pull you in a thousand directions. Ask you to do more. Then more. Then once you get all of that work done, your reward is more work. Congrats! Maybe you tell yourself you‚Äôll retire early. That way you can then have more time to spend with your family. Like I said before, I don‚Äôt know the right answer. I don‚Äôt think anyone does. So again I‚Äôll say that life is all about priorities and tradeoffs. How you define success in life could be different than someone else. And that‚Äôs ok. I think in a perfect world your family and friends are still high on the list as you get older. You can spend more time with them, and maybe less time on that job. Or maybe your job fulfills you immensely. You know the work contributes to making the world better. So working more is a worthwhile tradeoff. Do Presidents of nations feel bad that they cannot spend time with their family every day? Maybe they do, maybe not. For me I don‚Äôt think retiring to a beach for the rest of my life is any fun. I‚Äôd like to be like Charlie Munger, working into his 90s. I assume he wasn‚Äôt working nights and weekends in his 90s. Instead he still worked, but also made time for other relationships in his life. Again, there‚Äôs no right answer.\n\n\nFinal Thoughts\nWhen we‚Äôre on our deathbed. I don‚Äôt think any of us will say ‚ÄúI wish I worked harder‚Äù. The quality of our life boils down to the quality of our relationships. The quality of your relationship with your kids, parents, siblings, friends, and extended family. Relationships at work can serve a purpose too, but it‚Äôs hard for a job to replace these other types of relationships. You can have everything in life, just not all at once. Everything comes with a tradeoff. In the end, define what success looks like to you and have zero f#### for anyone else who tells you how to live your life. Prioritize accordingly."
  },
  {
    "objectID": "posts/2024-06-12-msft-ml-fcst-journey-1/index.html",
    "href": "posts/2024-06-12-msft-ml-fcst-journey-1/index.html",
    "title": "Microsoft Finance ML Forecasting Journey: Part One",
    "section": "",
    "text": "This is a multipart series:\n\nPart One\nPart Two\n\nEver wonder how Microsoft Finance got started with machine learning? It didn‚Äôt just happen overnight. It started small and grew from calculated steps. In this post and a few others I want to tell the journey of how we got started. Gather round children! It‚Äôs story time.\n\nParadigm Shift\nIn the summer of 2015 AI and machine learning (ML) weren‚Äôt terms you‚Äôd hear every day. Maybe you‚Äôd hear the word ‚Äúbig data‚Äù being thrown around business circles but no one had a clue what it meant. There was a lot of data being captured about our world. Somehow we could ‚Äúmine‚Äù the data to get some value out of it. No one really knew.\nEarlier that year, something interesting happened at Microsoft. A new product called Azure Machine Learning was officially released. The service allowed anyone to start mining their data up in the cloud. You could train models and serve them through APIs. It was basically magic. Unfortunately in finance, those words meant nothing. To a Microsoft finance worker the term ‚Äútrain a model‚Äù meant training the new employee on building excel models. Everything was done by hand and with care. Especially forecasting our financial statements. The CFO of Microsoft, Amy Hood, thought differently. What if we could use the new product to improve some of the manual work we did in finance? Could we have these models be trained to forecast our business? It was a tough question. No one in finance at the time was really qualified to answer it. She had to go ask the expert.\n\n\nGetting The Ball Rolling\nAmy went to the legend himself. The head of Microsoft‚Äôs cloud, Scott Guthrie. King of the cloud and wearing red polo shirts. She wanted to see if Scott‚Äôs engineering team could help finance build machine learning models. Allowing finance to forecast the business. Thankfully Scott said yes and lent a few data scientists to help the finance org get off the ground with ML.\nThe big ticket item was forecasting revenue. Instead of starting small with one specific area we started very high level. Amy wanted a quarterly global revenue forecast by each of Microsoft‚Äôs major products. This forecast could be used internally to compare against the manual forecasts. Which are created by sales finance and product finance teams. The ML forecast could either confirm or contradict these bottoms up forecasts made by humans. Allowing finance to either adjust their forecasts. Or make sure they know why they are different than ML.\nThe results were strong. The ML forecast was around 1%-2% off on average, compared to the manual human forecast error of 2%-4%.\n\n\nKeeping The Ball Rolling\nThe game officially changed. The finance team could now just rely 100% on ML going forward right? Not so fast! Who would keep training these models? What if we wanted to forecast at a more granular level? Scott‚Äôs data scientists couldn‚Äôt help forever. To fix this Amy had to hire some data science talent. People who knew what they were doing. Like the engineers on Scott‚Äôs team.\nHiring your first data scientist is a hard thing to do. Creating a career path for them in a non-technical team like finance makes it harder. As a first step, a team of vendor data scientists were hired. This was enough help to take the work done by Scott‚Äôs team and keep it going. Even expand it to other areas. The hope was to eventually turn a vendor data scientist team into a team of full time employees.\n\n\nLessons Learned\nGoing from zero ML work to your first forecast solution takes hard work and perseverance. Here are a few lessons Microsoft finance learned when starting out.\n\nBorrow -&gt; Rent -&gt; Buy\nInitially data scientists were borrowed from other teams at the company. Then they were rented from outside companies as vendors. Then finally once a strong data science practice was established after a few years, full time employees were hired. Many were vendors who turned into full time employees. This process was slow, but allowed finance the time to make sure a data science practice and career path could be built.\n\n\nWhat‚Äôs the biggest opportunity?\nThe biggest opportunity to forecast with ML was revenue. We could have spread ourselves thin and tried to do the entire income statement. But we knew revenue was the hardest to forecast. So that‚Äôs where we started first.\n\n\nStart at the top, work your way down\nStarting first with worldwide revenue allowed finance to get good results without getting too deep into the weeds first. If we wanted to get an accurate daily forecast down to the sku level, that would have taken forever. Instead we started big and then eventually worked our way down. This process may not initially replace the manual forecast work being done. But it starts to get others in finance comfortable using ML in the decision making process. After finance leaders got used to seeing these ML forecasts, we could then start working on more granular forecasts that could replace more manual work.\n\n\n\nFinal Thoughts\nOk now you know how the ML ball got rolling in Microsoft finance. Before reading this article you might have thought we had this amazing ML kick-off with millions invested in the space. We definitely did not. Instead we started small in areas that had the highest ROI and worked our way from there. If your company is just starting out on your ML journey, I suggest you do the same. Small, incremental change can compound into enormous impact over the long run. That‚Äôs the kind of change that lasts."
  },
  {
    "objectID": "posts/2024-06-04-how-to-master-storytelling/index.html",
    "href": "posts/2024-06-04-how-to-master-storytelling/index.html",
    "title": "How To Master Storytelling",
    "section": "",
    "text": "Our dumb caveman brains can‚Äôt remember everything. There is too much information passing through our heads each day. The only thing that sticks are stories. Either ones we tell ourselves or ones we hear from others. Don‚Äôt believe me? Let‚Äôs play a game. Tell me what you had for lunch last week on Tuesday. Now tell me the plot to the first Star War‚Äôs movie. Ha gotcha. Stories stick. More than anything else in this world. Whoever can tell the best stories has the power to do great things in our world.\nShaan Puri is an entrepreneur and content creator. Known for his work in the tech industry and his popular podcast ‚ÄúMy First Million.‚Äù A few months ago he was on the popular ‚ÄúHow I Write‚Äù podcast. While on the show he blew my mind with tons of great ideas around storytelling. I learned a lot and wanted to share the best ideas here.\n\nAaron Sorkin‚Äôs 30 Second Masterclass\n\n‚ÄúI worship at the alter of intention and obstacle.‚Äù\n‚Äî Aaron Sorkin\n\nAaron Sorkin is a famous screenwriter. Creating hits like the TV show ‚ÄúThe West Wing‚Äù or movies like ‚ÄúThe Social Network‚Äù. His says that every story needs to have a clear intention and obstacle.\nIn any story, the main character has a have a clear intention. What do they want? Why do they want it? After that you need an obstacle. Who or what is trying to stop them from getting their intention? It‚Äôs fundamental to every story, but doesn‚Äôt have to be life and death. Every movie you have ever seen has this. If it didn‚Äôt have a strong intention or obstacle in the first 10 minutes, you probably hated the movie. For example, all Harry Potter wants is to live a normal life with a loving family and close friends. Since the day he was a born a dark wizard is trying to kill him. Intention (living), and obstacle (trying to kill him).\n\n\nHooks vs Frames\nEveryone has seen Twitter/X threads that start with ‚ÄúThe unbelievable story of XYZ person doing XYZ thing‚Äù. It‚Äôs clickbait and makes me cringe every time I see it. These are tactics that try to hook a reader into continuing to engage in the content. Instead of creating hooks, Shaan recommends creating the right frame for a story. Hooks are about the words you‚Äôre going to write. Frames are about the idea. And how you‚Äôre going to connect many ideas together to make it relevant to an audience.\nCheck out the following tweets about the popular audio app Clubhouse.\n\nThis first tweet is by someone who founded a similar audio app before Clubhouse. He should be very knowledgeable on the subject. The tweet gained little traction because he told no story. It was dry and full of technical industry facts and jargon.\n\nNow take a similar tweet by Shaan. In his tweet he told a story. He put in the frame of ‚Äúevery one thinks X, but I think Y, and here‚Äôs how I think it‚Äôs going to go down‚Äù. It had millions of views. The thread is a mini screenplay and masterclass on how the right frame can make an idea turn into a powerful story.\nAnother powerful example is Dave Chappelle trying to get the rights back to his famous sketch show. He didn‚Äôt complain to Viacom (owners of Comedy Central which initially hosted the show). Instead he told a story with a powerful framing. It wasn‚Äôt a funny story. But one that made his fans start boycotting the show on streaming platforms. Hoping to get paid for his original hard work. This was all due to a powerful story with the right framing. Check out this video to hear the full story.\n\n\nTell 100 Stories\nIf you don‚Äôt know who Mr.¬†Beast is than you live under a rock with no internet connection. People always ask how they can be like him. His advice is simple. Make 100 videos. Each time do one thing better than the last video. He says it‚Äôs the perfect advice. Because either no one actually goes through with making 100 videos. Or the people that do never reach out to him again because once they make 100 videos they figured out how to make them successful.\nTo tell great stories you need to tell hundreds of stories, maybe thousands. You need to get intelligent reps in. With each rep you get a little better. These gains can compound a powerful skill over time.\n\n\nMake A Story Go Viral\nThere are companies out in the world whose sole purpose is to make content go viral. Now that‚Äôs a wild job. They know the only way something goes viral is how many people share the content. People will normally share content if it creates an emotional reaction. Shaan calls these emoji reactions. Things like ü§£üòçüòéü§îüòÆü§¨. When the company is creating a piece of content, they start with the emotion they want the content to create. Then they work back from that emotion to create the content. The north star is always the emotion the want the audience to feel. If their first draft doesn‚Äôt create that emotion, they will adjust until they get it right.\n\n\nStoryworthy Book\nMatthew Dicks wrote the book on storytelling. No he actually did. He wrote a book called ‚ÄúStoryworthy‚Äù. I‚Äôm currently reading it and love the book, future post coming soon. Shaan calls out a few kew ideas from the book in the podcast.\nFirst is every story needs stakes. You need to make clear what‚Äôs at stake if the main character doesn‚Äôt get what they want (their intention). These stakes don‚Äôt have to be life or death though. Stakes come from the emotion. Something closer to regular life has the biggest impact because other people can see themselves in the story. Telling a heartwarming story about learning life lessions from your kids at the dinner table can be more powerful than a shark attack story.\nNo one cares about your vacation or crazy party story. If you have something that‚Äôs generally interesting about your vacation, then only keep the parts that are relevant to the story. If you got pickpocketed at the Vatican, but were able to chase down the bad guy and get your wallet back, then just tell that part of the story. It may not matter that you were on the Amolfi coast the previous day or you were even at the Vatican. No one cares. Just keep your story as long as it is interesting. Not a second longer.\nAt its core, a story is a five second moment of change. Everything comes up to one moment where a thing or person is transformed. If the story doesn‚Äôt have change, then it‚Äôs just an anecdote. A sequence of events. Not a real story. Stories start with the world one way, and end with it another way. Shaan says a good example is every romantic comedy you‚Äôve ever seen. Whatever the main character is, they are 100% going to be the opposite at the end of the movie. Is the woman a fast paced lawyer who never made time for love? By the end of the movie she will have a romantic partner and will take more time away from work. Is the guy a ladies man who will never settle down? By the end of the movie he will fall for someone who will make him rethink everything and start hearing wedding bells. It‚Äôs always about change.\n\n\nAdding Humor in Storytelling\nHumor is the sauce but not the meal in most stories. All humor is just surprise. If you see the punch line coming it‚Äôs not very funny. Try to add humor into your storytelling. Just don‚Äôt make it the whole story. Leave that up to the comedians.\n\n\nBinge Bank\nIf someone wanted to learn more about you, what would they do? Maybe they‚Äôre a recruiter trying to offer you the perfect job. Maybe they‚Äôre an entrepreneur trying to find their next co-founder. Or maybe they‚Äôre a cute girl you‚Äôre about to have dinner with. Chances are the only things they can find on you is your social media presence, and hopefully not any mugshots.\nShaan recommends creating a binge bank for yourself. Think of it as a bank of content that someone can go down the rabbit hole on you. Blogs, videos, newsclippings. Hopefully a collection of stories. Whatever content that gives someone all access pass to how your brain works and who you are as a person. By the end a person‚Äôs opinion of you should drastically change. This binge bank can become more powerful than a resume, because it shows your true self. While also showcasing how you communicate.\nAfter hearing this I started to curate my own binge bank. I added a Start Here section to my personal site. On this page I have links to posts that best describe me and my capabilities. Right now it‚Äôs small but one day I plan to grow it to the binge banks of people like Ryan Holiday, Tim Ferriss, or Mark Manson.\n\n\nGrowing An Audience\nShaan has a friend with a cool rule about being interesting. If you tell someone something interesting, they will say ‚Äúwow that‚Äôs interesting‚Äù. If you tell someone two interesting things, the will say ‚Äúthose are interesting‚Äù. If you tell someone three interesting things, they will say ‚Äúok, now you are interesting‚Äù. To build a following on the internet or at your job, you need to demonstrate your insight constantly. Instead of doing this three times you may have to do this 100 times to get people to come back and pay attention.\nAnother powerful point that Shaan learned from a branding expert is people will follow you to the ends of the earth if you can give them a feeling more consistently than anyone else. Why do people listen to podcasts from comedians? It‚Äôs not because they have groundbreaking insight about current events or they perform their standup routine. It‚Äôs because when people listen they feel like ‚Äúone of the guys‚Äù who like to hang out with their friends and crack jokes at one another. How people feel after consuming your story is the end goal.\n\n\nClosing Thoughts\nStorytelling is more art than science. It takes reps to get it right. If you can tell good stories, you can do almost anything in this world. Get out there and start telling better stories today. Just not ones about your vacation."
  },
  {
    "objectID": "posts/2024-05-28-time-series-model-avg/index.html",
    "href": "posts/2024-05-28-time-series-model-avg/index.html",
    "title": "Time Series First Principles: Model Combinations Are King",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the ninth principle of a good time series forecast, model combinations are king. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nWisdom of the Crowds\nIn 1906, famed statistician Francis Galton went to a county fair for some fun. While there he came upon a competition to guess the weight of an ox. Eight hundred people entered the competition but the guesses were all over the place, some too high, some too low. Francis was a big numbers guy, so he took all of the guesses home with him and crunched the data. He found out that the average of all the guesses was only one pound away from the actual weight of the ox, which weighed 1,198 pounds. That‚Äôs an error of less than 0.08%. What he stumbled upon that day is now know as the wisdom of the crowds.\nThe concept of wisdom of the crowds states that the collective wisdom of a group of individuals is usually more accurate than that of a single expert. When guessing the weight of the ox, the overestimates and underestimates of regular people cancelled each other out. Creating an average prediction that was more accurate and any single person‚Äôs estimate.\nThis principle is important in machine learning forecasting. Usually it‚Äôs not one single model that performs the best, but instead a combination of multiple models. Let‚Äôs take a look at how we can combine models into more accurate forecasts.\n\n\nTypes of Model Combinations\nThere are many different ways individual model forecasts can be combined to create more accurate forecasts. For today we‚Äôll cover the most common approaches. If you‚Äôd like to dive deeper I recommend this amazing paper by our forecasting Godfather Rob Hyndman.\n\nSimple Average: As simple as it sounds. Just take the forecasts from individual models and average them together.\nEnsemble Models: Feed the individual model forecasts as features into a machine learning model, and have the model come up with the correct weighted combination. This is also known as ‚Äúmodel stacking‚Äù.\nHierarchical Reconciliation: This involves forecasting at different aggregations of the data set based on its inherent hierarchies, then reconciling the down to the lowest level (bottoms up) using a statistical process. For example forecasting by city, country, continent, and global level then reconciling each forecast down to the city level. This reconciliation can be thought as combining different forecasts together to create something more accurate. This approach has more nuances, and will be covered in another post.\n\n\n\nModel Combination Example\nLet‚Äôs walk through a simple example around how combining the predictions of more than one model can outperform any single model. Below is an example monthly time series. We will try to back test the last 12 months of the historical data.\n\nTo keep things simple we can just run a few models to get the back testing results for the last year of the data. We‚Äôll use various univariate time series models. Ignore the types of models used. Instead, let‚Äôs just see how each model did on it‚Äôs own. Learn more about accuracy metrics in a previous post.\n\n\nAccuracy by Single Model\n\n\nModel\nMAPE\nMAE\nRMSE\n\n\n\n\narima\n1.97\n3.76\n4.68\n\n\ncroston\n10.18\n19.54\n20.01\n\n\nnnetar\n9.77\n18.37\n26.00\n\n\nstlm-ets\n1.92\n3.68\n4.59\n\n\ntbats\n1.86\n3.51\n4.05\n\n\ntheta\n2.46\n4.71\n5.52\n\n\n\nIt looks like the tbats model performs the best across the board with stlm-ets and arima not far behind. What if we averaged the three of them together? Let‚Äôs see how the results change.\n\n\nAccuracy for Average Model\n\n\nModel\nMAPE\nMAE\nRMSE\n\n\n\n\narima_stlm-ets_tbats\n1.84\n3.51\n3.99\n\n\n\nEven better results! See how creating simple model averages can improve the results? Averaging the results can help smooth out any under or over forecasts, creating more accurate models.\nSimple model averages are often the quickest way to improved forecast accuracy. Another way is to create an ensemble model that can create the weights on its own. Let‚Äôs feed the predictions from each model into a linear regression model and have it determine the optimal weights.\n\n\nAccuracy for Ensemble Model\n\n\nModel\nMAPE\nMAE\nRMSE\n\n\n\n\nEnsemble\n1.81\n3.40\n3.65\n\n\n\nAlight more accurate results! By feeding each individual model forecast into a final ensemble model, we were able to get a more accurate forecast.\n\n\nReversal\nWhen trying to combine models, there is always a risk of overfitting. Meaning the combination approach (like simple average or ensemble) could have great accuracy on the back test data but not generalize well to new unseen data in our future forecast. To prevent that we can make sure to back test on enough historical data to prove our combination approach works well for more than just a period or two. We can also have separate validation and test splits in the back testing to see how combinations made on one data set can generalize well when tested on the other.\nPrediction intervals are harder to create. Simply combining the 80% and 95% prediction intervals of multiple models together is not going to fully capture the uncertainty of forecasts created by the new model combination. So we would need to re-create the intervals based on the results of the new combined model.\nSimilar to prediction intervals, combining models can also make it harder to interpret them. Instead of just understanding one model and its predictions, we now have to understand how multiple models work and are combined to get the final forecast.\n\n\nfinnts\nModel combinations can be hard to do effectively. Thankfully my forecasting package, finnts, is here to help! It automatically handles every kind of model combination method listed in this post. Check out the package and see just how easy forecasting can be.\n\n\nFinal Thoughts\nJust like the county fair crowd nailed the ox‚Äôs weight, combining multiple models in time series forecasting yields more accurate predictions by balancing out individual errors. When you‚Äôre forecasting, remember to embrace the collective wisdom of models for better results!"
  },
  {
    "objectID": "posts/2024-05-07-time-series-capture-uncertainty/index.html",
    "href": "posts/2024-05-07-time-series-capture-uncertainty/index.html",
    "title": "Time Series First Principles: Capture Uncertainty",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the eighth principle of a good time series forecast, capture uncertainty. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nBuilding Trust\nWould you give your retirement savings to a hedge fund manager because they asked nicely? Probably not. Instead, you would like to do your research about them. Ask them how well they performed in the market historically, and also see how they expect the future markets to unravel in the near term. If their answer to those questions are, ‚ÄúI don‚Äôt have a historical track record‚Äù and ‚ÄúI have no clue what the future holds‚Äù then you are probably not going to give them one penny of your hard earned money. The same holds true for using a time series forecast created by machine learning (ML) models. In order to build trust with the end user of the forecast, you need to show them how a similar forecast would have performed historically and also quantify some aspect about the future. Let‚Äôs dive into each one.\n\n\nPast Uncertainty\nBefore a ML model can be used to forecast the future, we need to see how it has handled the past. This is called back testing, where we see how a model performed historically. This can give us a good proxy around how it could perform in the future.\nBack testing at its core is all about training a model on a portion of your historical data set (training data), then using the trained model on another portion of the historical data (testing data). This can be as simple as using the first 80% of your historical data to train a model, and use the last 20% for testing. Check out a previous post to learn more about why the order of that train/test split is important.\nThere are also more advanced methods of doing this, like time series cross-validation. This involves many rounds of training a model and then creating a prediction on the testing data. Time series cross-validation can be used to tune model hyperparameters (inputs a model cannot learn from but must be given by a human) but is especially useful for model back testing. Check out the chart below that shows how we can effectively back test using a time series cross-validation approach. Each pass has its own train and test split, and the testing splits can overlap from one pass to another.\n\n\n\nSource: Uber Engineering\n\n\nIn order to capture how accurate the back testing is, we need to calculate a metric that summarizes the model‚Äôs performance on the testing data splits. There are countless metrics we can use, each with their own pros and cons. That kind of discussion is out of scope for this post but let‚Äôs highlight a few common ones you could use in determining how accurate a model is during back testing.\n\nMean Absolute Error (MAE)\n\nDescription: MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It‚Äôs calculated as the average of the absolute differences between forecasts and actual observations.\nStrengths: MAE is straightforward and easy to interpret as it directly represents average error magnitude.\nWeaknesses: MAE treats all errors with the same weight, thus large errors have the same influence as small ones, which might not be optimal for all applications.\n\nRoot Mean Squared Error (RMSE)\n\nDescription: RMSE is the square root of the mean of the squared errors. It measures the average magnitude of the error, with the squaring giving higher weight to larger errors.\nStrengths: RMSE is sensitive to outliers and provides a measure of how large errors are when they occur, which can be crucial for many practical applications.\nWeaknesses: Like MSE, RMSE can be heavily influenced by outliers and large errors, possibly leading to overestimations of the typical error if the error distribution is skewed.\n\nMean Absolute Percentage Error (MAPE)\n\nDescription: MAPE expresses accuracy as a percentage, and it measures the size of the error in percentage terms. It is calculated as the average of the absolute errors divided by the actual values, expressed as a percentage.\nStrengths: MAPE is scale-independent and provides a clear interpretation in terms of percentage errors, making it easy to communicate.\nWeaknesses: MAPE can be highly skewed when dealing with values close to zero, and it disproportionately penalizes underestimations compared to overestimations.\n\n\n\n\nFuture Uncertainty\nNow that we‚Äôve quantified how well our model works historically, we can just give the future forecast to our end user right? Not so fast. Our model might say that next month our company‚Äôs product will make $100, but if that‚Äôs all the info we provide to the end user of that forecast that‚Äôs not a good way to build trust. Instead we need to show how confident we are in that $100 forecast. How likely are we to hit that number? That‚Äôs where prediction intervals come in.\nPrediction intervals help quantify the future uncertainty in our model‚Äôs forecast. They are statistical ranges, typically based on the forecast error, used to indicate the likelihood that the future value of a time series will fall within a specified range at a certain confidence level. Common ranges for a prediction interval are 80% and 95%. For example, the future forecast may be $100 but have a 95% prediction interval of $75 and $125. This means that there is a 95% likelihood that the future value will fall between $75 and $125. The tighter the range, the less uncertainty there is in the forecast. Below is an example forecast with 80% and 95% prediction intervals.\n\n\n\nReversal\nThe back testing process can only ever be a proxy of what kind of results to expect on the future forecast. It follows the assumption that the future will be similar to the past. Sometimes this is not the case, and future results may be worse than historical back testing performance.\nWhile prediction intervals help quantify uncertainty, they also do not do a perfect job. There may be times where the future forecast will fall outside of the ranges. It‚Äôs not the end of the world when it does, but instead shows that the future is often different than what happened in the past. This is where strong domain knowledge comes in to understand what‚Äôs truly an outlier and what‚Äôs a new fundamental factor in your business going forward. For example, a new product launch in the future is hard to quantify with a prediction interval, but once it happens we can learn from that information and try to capture it the next time we train our model.\n\n\nfinnts\nBack testing and prediction intervals is tough work. Thankfully my forecasting package, finnts, takes care of both of these for you. You can even customize the back testing process to fit your needs. Check out the package and see just how easy forecasting can be.\n\n\nFinal Thoughts\nCapturing uncertainty in time series forecasting is essential for creating robust forecasts that stakeholders can rely on. Utilizing back testing and prediction intervals not only strengthens the credibility of forecasts but also provides users with a clearer perspective on potential risks and variations. In the end these approaches help build trust with the forecast end user. The more trust we can build, the more likely the ML forecast will be used."
  },
  {
    "objectID": "posts/2024-05-01-time-series-features/index.html",
    "href": "posts/2024-05-01-time-series-features/index.html",
    "title": "Time Series First Principles: The Magic Is In The Feature Engineering",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the sixth principle of a good time series forecast, the magic is in the feature engineering. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nTurning Data Into Insight\nA machine learning (ML) model is only as good as the data it‚Äôs fed. The process of transforming data, to make it easier for a model to learn from that data, is called feature engineering. It‚Äôs a technical term that is actually very simple in nature, really just data transformations. In the world of time series forecasting, feature engineering can make or break a good forecast.\nCreating high quality features is a combination of strong domain expertise and data transformation skills. We have already covered how domain expertise impacts a forecast in a previous post, so this post will cover how simple data transformations can drastically improve the accuracy of a machine learning forecast. Check out each category of time series feature engineering below to learn more.\n\n\nDate Features\nThe most common type of feature engineering for time series is around dates. Date features allow us to capture seasonality patterns in our data. Think of seasonality as repeating peaks and valleys in our data. For example, our business might make most of its revenue in Q4 every year, with a subsequent dip in sales in Q1.\nLet‚Äôs use the example time series below to illustrate each type of feature engineering.\n\nFake Time Series Data\n\n\nDate\nSales ($)\nConsumer Sentiment\n\n\n\n\nJanuary 2023\n100,000\n68\n\n\nFebruary 2023\n110,000\n67\n\n\nMarch 2023\n120,000\n65\n\n\nApril 2023\n115,000\n70\n\n\nMay 2023\n130,000\n72\n\n\nJune 2023\n125,000\n73\n\n\nJuly 2023\n135,000\n74\n\n\nAugust 2023\n140,000\n75\n\n\nSeptember 2023\n130,000\n70\n\n\nOctober 2023\n145,000\n72\n\n\nNovember 2023\n150,000\n71\n\n\nDecember 2023\n160,000\n75\n\n\n\nIn this time series we would like to forecast monthly sales. We also have information about consumer sentiment that we can use to help forecast sales. A multivariate machine learning model cannot easily use the date column as is, so we have to do some data transformations (aka feature engineering) to make it easier for a model to understand how date information can help predict sales. Let‚Äôs go through a few examples of new features we can create from the date column. It‚Äôs important to note that after we create these new features it‚Äôs a good idea to remove the original date column before training a ML model.\nSince the data is monthly there are a lot of simple features we can use. We can pull out the specific month, quarter, and even year into their own columns to use as features. If our data was at a daily level, we can even go deeper and get features related to day of the week, day of year, week of month, etc.\n\n\n\nDate\nMonth\nQuarter\nYear\n\n\n\n\nJanuary 2023\nJanuary\nQ1\n2023\n\n\nFebruary 2023\nFebruary\nQ1\n2023\n\n\nMarch 2023\nMarch\nQ1\n2023\n\n\nApril 2023\nApril\nQ2\n2023\n\n\nMay 2023\nMay\nQ2\n2023\n\n\nJune 2023\nJune\nQ2\n2023\n\n\nJuly 2023\nJuly\nQ3\n2023\n\n\nAugust 2023\nAugust\nQ3\n2023\n\n\nSeptember 2023\nSeptember\nQ3\n2023\n\n\nOctober 2023\nOctober\nQ4\n2023\n\n\nNovember 2023\nNovember\nQ4\n2023\n\n\nDecember 2023\nDecember\nQ4\n2023\n\n\n\nThat seems pretty straight forward right? Let‚Äôs keep squeezing our date fruit for more juice and see what other kinds of features we can create. Since this is a time series, adding some order of time can be helpful. This can be something as simple as an index starting at 1 (or even convert your date to a seconds format). This helps establish the proper order of our data and makes is easier for a model to pick up growing or declining trends over time. There is also slight differences in how many days there are from month to month, so we can add that too. If you don‚Äôt think that‚Äôs important then you have never been stung by the harsh mistress that is leap year. There have been multiple times where finance exec‚Äôs have dismissed forecasts for the quarter that includes February, where in the end we didn‚Äôt account for the fact that it was a leap year or we are one year removed from one. You can even take this one step further and add the number of business days for each month.\n\nAdding a time index and other day related features\n\n\nDate\nIndex\nDays in Month\nBusiness Days\n\n\n\n\nJanuary 2023\n1\n31\n22\n\n\nFebruary 2023\n2\n28\n20\n\n\nMarch 2023\n3\n31\n23\n\n\nApril 2023\n4\n30\n20\n\n\nMay 2023\n5\n31\n23\n\n\nJune 2023\n6\n30\n22\n\n\nJuly 2023\n7\n31\n21\n\n\nAugust 2023\n8\n31\n23\n\n\nSeptember 2023\n9\n30\n21\n\n\nOctober 2023\n10\n31\n22\n\n\nNovember 2023\n11\n30\n22\n\n\nDecember 2023\n12\n31\n21\n\n\n\nTo get the final drop of juice out of the date column, we can also add Fourier series features. A Fourier series feature in time series forecasting is a component that captures seasonal patterns using sine and cosine functions to model periodic cycles in the data. In a nutshell they are just recurring peaks and valleys that can occur at various date grains like monthly or daily. These features can help capture more complex seasonality in your data. The chart below shows some standard Fourier series at the monthly and quarterly grain.\n\n\n\nLag Features\nTime series forecasting is all about learning from the past to forecast the future. In order to learn about the past we have to create lags on our data. Often what we‚Äôre trying to forecast today is correlated to what happened in the past. This is a concept known as autocorrelation. For our monthly forecast example, a 3 month lag may be highly correlated to sales with a 0 month lag (or sales today). Consumer sentiment can also be correlated with sales, but this time a lag of 6 might have higher correlation, since there is most likely a long delay between customer purchase patters and how it affects our company‚Äôs product. Lags can be created for any amount, depending on your domain knowledge of the business and results from more exploratory data analysis (deep dive for a different day).\n\nAdding lag features\n\n\n\n\n\n\n\n\n\nDate\nSales ($)\nConsumer Sentiment\nSales 3-Month Lag\nSentiment 6-Month Lag\n\n\n\n\nJanuary 2023\n100,000\n68\n\n\n\n\nFebruary 2023\n110,000\n67\n\n\n\n\nMarch 2023\n120,000\n65\n\n\n\n\nApril 2023\n115,000\n70\n100,000\n\n\n\nMay 2023\n130,000\n72\n110,000\n\n\n\nJune 2023\n125,000\n73\n120,000\n\n\n\nJuly 2023\n135,000\n74\n115,000\n68\n\n\nAugust 2023\n140,000\n75\n130,000\n67\n\n\nSeptember 2023\n130,000\n70\n125,000\n65\n\n\nOctober 2023\n145,000\n72\n135,000\n70\n\n\nNovember 2023\n150,000\n71\n140,000\n72\n\n\nDecember 2023\n160,000\n75\n130,000\n73\n\n\n\nLast thing I‚Äôll say here is that you can also create leading features, especially for features that you know with 100% certainty ahead of time. For example, customers knowing of a new product launch in the future will definitely change how they purchase similar products you sell for the periods leading up to the launch. Someone may hold off on buying a new iPhone until the latest one gets released in a few months. Same goes for cars and many other products.\n\n\nRolling Window Features\nOften using pure historical lags is not enough. The historical data of our target variable (what we want to forecast) can be very noisy, making it hard for a model to learn the proper trends and seasonality. One way to handle this is through rolling window transformations.\nRolling window features in time series forecasting help smooth out data, reduce noise, and capture essential trends and cycles by averaging or computing other statistics over a specified period. For a monthly forecast we can create rolling window features of averages, min/max, and other statistical calculations.\n\n\n\nRolling Window Averages aka Moving Average\n\n\nIt‚Äôs best to calculate rolling window features based on your existing lag features. That way there is no data leakage during initial model training. See below for example of creating a 3 month rolling window average of the 3 month sales lag.\n\nRolling 3 month average applied to the 3 month sales lag\n\n\n\n\n\n\n\n\nDate\nSales ($)\nSales 3-Month Lag\n3-Month Rolling Avg\n\n\n\n\nJanuary 2023\n100,000\n\n\n\n\nFebruary 2023\n110,000\n\n\n\n\nMarch 2023\n120,000\n\n\n\n\nApril 2023\n115,000\n100,000\n\n\n\nMay 2023\n130,000\n110,000\n\n\n\nJune 2023\n125,000\n120,000\n110,000\n\n\nJuly 2023\n135,000\n115,000\n115,000\n\n\nAugust 2023\n140,000\n130,000\n121,667\n\n\nSeptember 2023\n130,000\n125,000\n123,333\n\n\nOctober 2023\n145,000\n135,000\n130,000\n\n\nNovember 2023\n150,000\n140,000\n133,333\n\n\nDecember 2023\n160,000\n130,000\n135,000\n\n\n\n\n\nPolynomial Features\nThe final type of feature engineering I‚Äôd like to discuss are polynomial transformations. Sometimes there is a non-linear relationship between your initial feature and the target variable. Some models, like ones that use decision trees, can handle this kind of relationship while others like linear regression cannot. To fix this we can transform the data via polynomials like squaring, cubing, and even taking the log of the initial feature.\nLet‚Äôs take our example monthly sales data and add some spice to it. This time creating an exponential relationship between consumer sentiment and sales.\n\nUpdated sales data with an exponential relationship with consumer sentiment\n\n\nDate\nSales ($)\nConsumer Sentiment\n\n\n\n\nJanuary 2023\n1,309,000\n68\n\n\nFebruary 2023\n1,204,000\n67\n\n\nMarch 2023\n1,000,000\n65\n\n\nApril 2023\n1,525,000\n70\n\n\nMay 2023\n1,849,000\n72\n\n\nJune 2023\n1,964,000\n73\n\n\nJuly 2023\n2,121,000\n74\n\n\nAugust 2023\n2,500,000\n75\n\n\nSeptember 2023\n1,525,000\n70\n\n\nOctober 2023\n1,849,000\n72\n\n\nNovember 2023\n1,764,000\n71\n\n\nDecember 2023\n2,500,000\n75\n\n\nJanuary 2024\n2,890,000\n76\n\n\nFebruary 2024\n3,361,000\n78\n\n\nMarch 2024\n3,844,000\n79\n\n\nApril 2024\n4,641,000\n81\n\n\n\nWhen graphing the data, see how the increase in consumer sentiment has an exponential effect on sales?\n\nTo account for this, we can square the values of consumer sentiment and create a new feature to use. This new feature will make it easier for models like linear regression to capture these kinds of non-linear relationships.\n\nNew polynomial feature added\n\n\n\n\n\n\n\n\nDate\nSales ($)\nConsumer Sentiment\nConsumer Sentiment Squared\n\n\n\n\nJanuary 2023\n1,309,000\n68\n4,624\n\n\nFebruary 2023\n1,204,000\n67\n4,489\n\n\nMarch 2023\n1,000,000\n65\n4,225\n\n\nApril 2023\n1,525,000\n70\n4,900\n\n\nMay 2023\n1,849,000\n72\n5,184\n\n\nJune 2023\n1,964,000\n73\n5,329\n\n\nJuly 2023\n2,121,000\n74\n5,476\n\n\nAugust 2023\n2,500,000\n75\n5,625\n\n\nSeptember 2023\n1,525,000\n70\n4,900\n\n\nOctober 2023\n1,849,000\n72\n5,184\n\n\nNovember 2023\n1,764,000\n71\n5,041\n\n\nDecember 2023\n2,500,000\n75\n5,625\n\n\nJanuary 2024\n2,890,000\n76\n5,776\n\n\nFebruary 2024\n3,361,000\n78\n6,084\n\n\nMarch 2024\n3,844,000\n79\n6,241\n\n\nApril 2024\n4,641,000\n81\n6,561\n\n\n\n\n\nReversal\nSometimes too much of a good thing can be a bad thing. Adding a lot of new features can increase the chance that a model overfits. Overfitting in machine learning occurs when a model learns to capture noise or random fluctuations in the training data, leading to poor generalization and high performance on training data but low performance on unseen data. The best way to prevent this kind of overfitting is to limit the number of features used to train a model. This will be discussed in greater detail in another post in this series.\nDid you notice that when creating lags and rolling window features we had a lot of missing data at the start of the time series for those new features? This can be a problem. Some ML models do not like missing data, so we need to deal with those missing values. An easy way is to just drop the initial rows in the time series that have blank values for the new lags and rolling window features. This can work well if you have a lot of historical data. Dropping data can hurt model performance though, and if you don‚Äôt have a lot of data to start with it becomes a less favorable option. You could also replace the missing values, either by using a simple model to impute the value or just use the closest available value in the time series to ‚Äúfill in‚Äù the missing values. Both of these missing value replacement approaches have their own pros and cons but could be a better strategy then just simply dropping rows with missing values.\n\nFilling in missing values with their closest available value\n\n\n\n\n\n\n\n\nDate\nSales ($)\nSales 3-Month Lag\n3-Month Rolling Avg\n\n\n\n\nJanuary 2023\n100,000\n100,000\n110,000\n\n\nFebruary 2023\n110,000\n100,000\n110,000\n\n\nMarch 2023\n120,000\n100,000\n110,000\n\n\nApril 2023\n115,000\n100,000\n110,000\n\n\nMay 2023\n130,000\n110,000\n110,000\n\n\nJune 2023\n125,000\n120,000\n110,000\n\n\nJuly 2023\n135,000\n115,000\n115,000\n\n\nAugust 2023\n140,000\n130,000\n121,667\n\n\nSeptember 2023\n130,000\n125,000\n123,333\n\n\nOctober 2023\n145,000\n135,000\n130,000\n\n\nNovember 2023\n150,000\n140,000\n133,333\n\n\nDecember 2023\n160,000\n130,000\n135,000\n\n\n\n\n\nOther Pre-Processing\nOne thing I wanted to add that technically isn‚Äôt considered feature engineering are other data pre-processing methods. These are things you apply before you start your feature engineering process. They are specific to time series forecasting and can greatly improve forecast accuracy. Here are two pre-processing methods you should know about.\nFirst is making your data stationary. This is a time series technical term that pretty much means removing the trend component of your data, where the time series has a constant mean and standard deviation. We can make a time series stationary by the process of differencing. This involves taking the difference between each date observation and using that as the new time series to train models with. Check out the example below. See how the upward trend gets removed when we simply use the difference between months instead of the original monthly values? Some machine learning models, like ones that rely on decision trees, cannot extrapolate trends. So differencing the data removes any trend pattern, making it a lot easier for these models to produce high quality forecasts.\n\nAnother pre-processing technique is a box-cox transformation. This helps remove any exponentially increasing trends by applying various types of power transformations. For example, taking the log of your time series. Removing non-linear trends can make it a lot easier for a model to create accurate forecasts. See the example below of a time series with a non-linear trend. We can then apply a box-cox transformation and then difference the data. See how nice the final time series looks? It will be way easier for a ML model to learn the patterns in the final transformed time series.\n\n\n\nfinnts\nThere‚Äôs a lot to unpack on feature engineering for time series forecasting. Thankfully my package, finnts, can automatically handle all of the feature engineering for you. It does everything I called out in this post plus more. Check it out and see just how easy ML forecasting can be.\n\n\nFinal Thoughts\nFeature engineering is the backbone of successful time series forecasting, allowing models to uncover hidden patterns and relationships within the data, ultimately leading to more accurate predictions. By transforming raw data into meaningful features like date-related attributes, lag features, rolling window statistics, and polynomial transformations, we equip machine learning models with the necessary insights to make informed forecasts. However, it‚Äôs crucial to strike a balance between adding informative features and avoiding overfitting, as too many features can lead to poor generalization on unseen data. With careful consideration and the right techniques, feature engineering becomes a powerful tool in the arsenal of any data scientist or analyst aiming to unlock the predictive potential of time series data."
  },
  {
    "objectID": "posts/2024-04-19-weekend-reads/index.html#articles",
    "href": "posts/2024-04-19-weekend-reads/index.html#articles",
    "title": "Weekend Reads (4/19/24)",
    "section": "Articles",
    "text": "Articles\n\nUse Strategic Thinking to Create the Life You Want\nHow Time Flies\nTime Series First Principles: Higher Grain, Higher Accuracy"
  },
  {
    "objectID": "posts/2024-04-19-weekend-reads/index.html#videos",
    "href": "posts/2024-04-19-weekend-reads/index.html#videos",
    "title": "Weekend Reads (4/19/24)",
    "section": "Videos",
    "text": "Videos\n\nDave Chappelle - Unforgiven\nBalaji on AI Gods\nBuilding Laser Focus\nOverrated and Underrated Habits\nLessons Learned on Writing"
  },
  {
    "objectID": "posts/2024-04-19-weekend-reads/index.html#sites",
    "href": "posts/2024-04-19-weekend-reads/index.html#sites",
    "title": "Weekend Reads (4/19/24)",
    "section": "Sites",
    "text": "Sites\n\nMake Music with AI"
  },
  {
    "objectID": "posts/2024-04-19-weekend-reads/index.html#tweets",
    "href": "posts/2024-04-19-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (4/19/24)",
    "section": "Tweets",
    "text": "Tweets\n\nBuilding an Engineering Dream Team\nExperiences &gt; Things"
  },
  {
    "objectID": "posts/2024-04-12-weekend-reads/index.html#articles",
    "href": "posts/2024-04-12-weekend-reads/index.html#articles",
    "title": "Weekend Reads (4/12/24)",
    "section": "Articles",
    "text": "Articles\n\n17 Important Questions\nTime Series First Principles: Garbage In, Garbage Out\nTime Series First Principles: The Future Is Similar To The Past"
  },
  {
    "objectID": "posts/2024-04-12-weekend-reads/index.html#videos",
    "href": "posts/2024-04-12-weekend-reads/index.html#videos",
    "title": "Weekend Reads (4/12/24)",
    "section": "Videos",
    "text": "Videos\n\nTelling Good Stories\nPower of Being a Contrarian\nMaking Friends as an Adult"
  },
  {
    "objectID": "posts/2024-04-12-weekend-reads/index.html#podcasts",
    "href": "posts/2024-04-12-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (4/12/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nProtocols to Improve Your Sleep, Huberman Lab"
  },
  {
    "objectID": "posts/2024-04-12-weekend-reads/index.html#tweets",
    "href": "posts/2024-04-12-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (4/12/24)",
    "section": "Tweets",
    "text": "Tweets\n\nPower of Motivation\nOur Planet Rocks\nShipping on Fridays\nCool Idea Around Building the Pyramids"
  },
  {
    "objectID": "posts/2024-04-12-weekend-reads/index.html#books",
    "href": "posts/2024-04-12-weekend-reads/index.html#books",
    "title": "Weekend Reads (4/12/24)",
    "section": "Books",
    "text": "Books\n\nSapiens by Yuval Noah Harari"
  },
  {
    "objectID": "posts/2024-04-08-time-series-garbage/index.html",
    "href": "posts/2024-04-08-time-series-garbage/index.html",
    "title": "Time Series First Principles: Garbage In, Garbage Out",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the second principle of a good time series forecast, garbage in garbage out. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nYou‚Äôre Not a Wizard, Harry\nA common I see people in finance make when trying to use machine learning (ML) is around approaching it like a magic wand. Thinking as long as they bring in some data and throw it over the fence to a ML process, a perfect forecast will come back to them. All shiny and clean. ML should be able to find all of the patterns in the data and do things we humans can‚Äôt fathom right? No, wrong. ML is not a cure all thing. Having a good ML forecast starts with having quality historical data for a model to learn from. Without good data, you won‚Äôt get a good forecast. It‚Äôs as simple as that. Let‚Äôs dive into ways data can be stinky and how we can sanitize it before training models. To help illustrate each point we‚Äôll use an example of a monthly sales forecast.\n\n\nAmount of Historical Data\nIdeally you want to get as much historical data as possible. If we want to forecast the next quarter of sales, it‚Äôs a bad idea to only use the last 12 months of historical data to train a model. Usually I try to get 5+ years of historical data before training any model. This allows for enough year over year observations for a model to learn from. For monthly forecasts, I won‚Äôt even start a forecast project if there is less than 3 years of historical data.\nHaving sufficient historical data creates the opportunity to have sufficient model back testing. Where we can see how models performed over the last few months of the data. We can then use that back testing accuracy as a proxy for what to expect for the future forecast. The less historical data you have, the less you can back test.\nThe more data you have, the longer you can forecast into the future. If I only have 3 years of historical data, it‚Äôs a bad idea to try and forecast the next 2 years. A good heuristic is to cap your forecast horizon (periods you want to forecast out) to less than 50% of the historical data. So if you want to forecast out the next 3 years of monthly sales, you need at least 6 years of historical data. When in doubt always get more data.\nWhat if I have tons of historical monthly sales data, but a cool new feature I want to use around sales pipeline is only available for the last 2 years? Most of the time, stick with using more historical data even if it‚Äôs at the expense of using highly correlated features but less historical data. Feel free to try both approaches, but often the one with the most historical data wins.\n\n\nTrend and Seasonality\nMost time series can be broken into three pieces. First is trend, is your data going up or down over time. Second is seasonality, are there peaks and valleys in your data that happen at the same time each year. Finally is the ‚Äúerror‚Äù or ‚Äúresidual‚Äù component, which is anything left over after accounting for trend and seasonality. Think of it as noise in your data. This approach of breaking down a time series into separate pieces is called time series decomposition.\nHaving recurring trends and seasonality in your historical data make things 100% easier to forecast in the future. If your data has trends that change month to month and seasonal patterns that evolve over time, your data is basically all noise. A noisy dataset is a bad dataset, one that can‚Äôt be modelled effectively by any ML model. Take a look at the below time series, each broken out by trend, seasonality, and residual. Now tell me which one would be easier to train a model on? Which would produce a high quality future forecast?\n\nYou can deal with noisy data like this in a few ways. The first is to just change the grain of the data. For example, if this was forecasting a specific product SKU, maybe instead sum it up to a higher level like product category. That way more stable trends and seasonality might appear. You can also try to add features (variables to your training data) to try to teach a model why the seasonality and trends in the data are messy. For example the main COVID years from 2020-2022 really throw a wrench in any trends or seasonal patterns in most data sets. So adding information to a data set that tells a model that there was a special one-off situation for specific periods can help a model learn the right kinds of relationships in the data and generalize well to new unseen data going forward.\n\n\nMissing Data\nMissing data is the silent killer in forecasting. If you don‚Äôt specifically look for it you might never know it‚Äôs the reason your forecasts perform poorly. Missing data is important because many models (either statistical models or ML models) often need all sequential date observations of the historical data to train a model. Even one period of missing data can throw off an entire model and lead to poor performance.\nOften times financial systems will not have tons of missing data. It‚Äôs important to know if the data that is missing should mean treated as actually missing or seen as a true zero value. For example, if we have product sales missing for a specific month, should we classify that value as truly missing or just hardcode that value to zero? Make sure you clarify that with whoever owns the data.\nIf the missing should be zero then that‚Äôs a quick fix, but if it‚Äôs truly missing then you now have another problem on your hands around what to do. Simply replacing the missing value with zero can throw off any trend or seasonality patters like we discussed earlier. Common ML advice is to replace missing feature data with the median or mean value of that feature, but this is terrible advice for time series forecasting. Usually the best approach is to use some sort of simple statistical model that can understand the trends and patterns of data around the missing value and impute what the value should be. This will keep existing trends and seasonality patterns in the data, meaning your future forecast will be more robust.\n\n\nOutliers\nAn outlier in time series forecasting is an atypical data point that significantly deviates from the overall pattern of the data. They can occur multiple times in a historical time series or just be a one off for a particular period. Either way, their presence can greatly impact how a model learns from the data.\nOutlier detection in time series forecasting often involves statistical methods, anomaly detection algorithms, or visual inspection to identify data points that significantly deviate from the typical patterns of the series. Techniques include setting thresholds based on standard deviations, using moving averages to smooth the series and highlight anomalies, applying machine learning models like isolation forests, or utilizing robust decomposition methods (like STL) to separate the series into components and identify outliers in the residuals.\nTake a look at the chart below. See how just one large value towards the end of the time series completely changes the trend and seasonality. A model might take this data and produce a huge forecast going forward, since the trend changed drastically based on the outlier. It might also have a huge spike for that specific period next year, since it learned that seasonality recently changed.\n\nThere are a few ways we can handle the presence of outliers. First we can leave it alone, and let it‚Äôs presence impact our future forecast. Maybe after talking with the business domain expert they say that there is a foundational change in the business (new product launch, tax change) that means we expect to see similar values in the future. Second we can add some more information to our data to explain what happened in that period and if we expect it to happen again in the future. If the outlier was caused by a new product launch, we can label that as a feature in the data and also tell the model if we expect any product launches in the future. A model will then learn of these one off patterns and adjust the forecast as needed. The final method is to remove the outlier altogether. Once removed, we can treat it like a missing value and replace it with a value more in line with recent trends and seasonality. If it‚Äôs truly a one off thing that will never happen again then removing it is sometimes the best approach. The choice you make always depends on the context of what caused the outlier and how we expect similar things to happen going forward.\n\n\nReversal\nSometimes having more historical data is a bad thing, and can hurt model performance. For example having 30 years of historical data could produce a good forecast, but do the trends and patterns of the business 20 years ago still apply to the business today? Often in fast changing industries this is not the case, so sometimes deliberately shortening your data is the right idea. Five years from now we may want to exclude data from pre-2023 to remove all impacts of COVID. How your customers purchased your services in 2019 is most likely very different than how they will buy them in 2024. Gee, thanks COVID.\n\n\nAutomatic Data Cleaning with finnts\nThankfully there is a solution to most of these problems. My package, finnts, helps solve a lot of the data sanitizing needed to produce a high quality forecast. It can handle outliers and missing values automatically for you. It abstracts away all of these hard topics and makes it easy to get up and running with a forecast in one line of code. Check it out.\n\n\nFinal Thoughts\nMessy data will always lead to a messy forecast. ML models can‚Äôt save you from bad data. There‚Äôs no magic wand to cure common data problems. What you can do though is make sure your data has solid historical trends/seasonality, no missing data, and good approach to handling outliers. With these taken care of, you‚Äôre own your way to building a high quality forecast."
  },
  {
    "objectID": "posts/2024-04-02-time-series-domain-expertise/index.html",
    "href": "posts/2024-04-02-time-series-domain-expertise/index.html",
    "title": "Time Series First Principles: Domain Expertise",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the first principle of a good time series forecast, domain expertise. Check out the initial post to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nIntroduction\nAny data scientist worth their salt can create a time series forecast for you. They can pull some data, train some machine learning (ML) models, and give you a forecast. All with you out of the loop. If that‚Äôs the case at your company, run! This is a big red flag. While that can sometimes yield good results, often the most important ingredient is missing, which is strong domain expertise about what you‚Äôre trying to forecast. This is where strong understanding of the business and market forces come into play. You know, the stuff that finance people excel at. Pairing robust ML models with strong domain expertise about the area being forecasted always yields the most accurate forecast. It also increases trust in that forecast, since the humans using that forecast know the model took into account important factors that influence the business. In this post we‚Äôll use a hypothetical example of a company‚Äôs real estate spending to showcase the importance of domain expertise.\n\n\nTranslating Domain Expertise Into Features\nHow does domain expertise change how a ML model is created? This can manifest in many forms. The most common is changing the kind of data used in training a model. Variables that a model learns from are called ‚Äúfeatures‚Äù. Let‚Äôs apply this to our real estate spend forecast example. In the last few years, COVID and the work from home revolution have changed how people come into work. This changes how many people drink coffee, use the copier, and even which buildings stay in operation for a company. Simply pulling historical building expense data and training a model could get you ok results, but to get to peak performance you need domain expertise around what actually moves the needle for building expenses. Example features could be the square footage of a building, how many people actually badge into that building each month, even the periods where COVID was at its worse and a work from home mandate was in effect. All of these things are custom knowledge, most likely kept inside the heads of the finance workers who oversee the real estate space within a finance org.\n\n\nIteration is Key\nThrowing all of your ideas as features into a model from the start is usually not a good idea. Instead having multiple rounds of iteration is key. In the real estate example, it‚Äôs best to start out with no external features. Just use historical spend to forecast future spend. Starting with this simpler approach can sometimes get you 90% of the accuracy you need, maybe even 100% if there are stable trends and seasonality that carries into the future. Run this first to see what the initial accuracy is, and if it doesn‚Äôt meet your requirements that when we can refine by adding new data.\nOnce you have the baseline, you can look deeper into the accuracy results to see where the forecast is performing poorly. This is where domain knowledge kicks in. Poor initial forecast performance can be fixed by asking the domain expert if there is a difference between what the model knows and what a human knows. If there is a gap, can that be quantified as data to teach a model? This kind of insight can be added into a model with easy to find numeric data, or even as binary yes or no values (1 or 0) to denote when a specific one off event happened. This iterative process is where the magic happens.\nFor the real estate forecast, maybe there was a period where expenses jumped sharply in one month and stayed at that new level for the rest of the year. This will be hard for a ML model to understand or even anticipate, but the domain expert of the real estate space knows that in that specific month there were two new building openings. So the expenses of course jumped up a significant degree and stayed like that going forward. Knowing this, we can get historical square footage information and add it into our model. We can even incorporate future buildings that might be removed or added going forward. This will help a model understand how changes in total buildings impact spend.\nSo we added total square footage to our model and the results improved compared to our initial baseline of no external features. But it didn‚Äôt move the needle that much. Even though our company might be adding more buildings, in recent years the spend may not have a perfect correlation with added square footage. Knowing this, the domain expert recommends using anonymous badge in data to see who is actually coming into work. Pre-covid this data may not have been useful, since most buildings were always at max capacity with everyone coming to work each day. Now in a post-covid world this has changed forever. Some teams might only be in their assigned building 2-3 days a week. Or maybe they never returned in person, deciding instead to buy ranches in Wyoming with fast WiFi. Combining the square footage and badge in data into the model yielded fantastic results, much better than the initial baseline.\nAfter reviewing the improved results with the domain expert, the future forecast still seems a little low compared to the domain experts expectations. The domain expert has one last idea, trying to teach the model how COVID impacted spending. This can be quantified as a binary variable, where in all rows of the data we add a 1 if COVID was impacting the world, and 0 when it wasn‚Äôt. This means from early 2020 - early 2022 we have values of 1 and every period before and after we give a value of 0. A model can now understand that what happened over those two years was mostly a one off situation that is not expected going forward. After the ML model is trained with this new insight the back testing now looks great and the future forecast matches the expectations of the domain expert.\n\n\nReversal\nGetting high quality data to use as features in a model is always a good idea. There are times though where the amount of historical feature data might be lacking. For example, we may not be able to get more than 3 years of historical square footage data for our real estate expense forecast, even though we can get 5 years of historical spend data. What should we do? We can shorten the historical spend data to the last 3 years to match the square footage data, but having less data can sometimes degrade model performance. So in some cases choosing to use the full 5 years of historical spend without the square footage data is the best approach that yields the best accuracy.\nWhen facing this dilemma, try both approaches and see how accuracy is effected. I‚Äôve seen many times that using more historical data of what you‚Äôre trying to forecast is often more accurate than shortening that data to combine it with external features.\n\n\nFinal Thoughts\nStarting any ML process without a business domain expert in the room is always a bad mistake. They are the cheat code in the video game that gets you to level 20 in half the time. Involving them early and often while also adopting a quick iteration approach can create a world class forecast that is trusted by the ultimate end users, which often are the domain experts themselves. At the end of the day most ML forecasts come down to trust by the end user. That‚Äôs why domain expertise is the first principle in building quality time series forecasts."
  },
  {
    "objectID": "posts/2024-03-22-weekend-reads/index.html#articles",
    "href": "posts/2024-03-22-weekend-reads/index.html#articles",
    "title": "Weekend Reads (3/22/24)",
    "section": "Articles",
    "text": "Articles\n\nDHH On Coding AI Agents\nWhen Confidence Backfires"
  },
  {
    "objectID": "posts/2024-03-22-weekend-reads/index.html#videos",
    "href": "posts/2024-03-22-weekend-reads/index.html#videos",
    "title": "Weekend Reads (3/22/24)",
    "section": "Videos",
    "text": "Videos\n\nSahil Bloom on Building Life Systems\nSam Altman on Lex Friedman\nJensen Huang at Stanford GSB\nTikTok Ban, AI, and more on All In"
  },
  {
    "objectID": "posts/2024-03-22-weekend-reads/index.html#podcasts",
    "href": "posts/2024-03-22-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (3/22/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nMorgan Housel on Modern Wisdom"
  },
  {
    "objectID": "posts/2024-03-22-weekend-reads/index.html#tweets",
    "href": "posts/2024-03-22-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (3/22/24)",
    "section": "Tweets",
    "text": "Tweets\n\nGrateful that Kansas won their first round game\nHow Momentum Rules Your Life\nCognitive Biases"
  },
  {
    "objectID": "posts/2024-03-22-weekend-reads/index.html#books",
    "href": "posts/2024-03-22-weekend-reads/index.html#books",
    "title": "Weekend Reads (3/22/24)",
    "section": "Books",
    "text": "Books\n\nThe Numbers Game by Chris Anderson"
  },
  {
    "objectID": "posts/2024-03-13-power-pursuits/index.html",
    "href": "posts/2024-03-13-power-pursuits/index.html",
    "title": "Thoughts on Power",
    "section": "",
    "text": "A few years ago I was at dinner with a few friends. Out of the blue one of them asked me ‚Äúwhat is the meaning of life?‚Äù. Without thinking I blurted out ‚Äúlove‚Äù. Thinking that in the end, everything comes down to love. It sounded cheesy and cliche coming out of my mouth. Over the years though, I continued to ponder the question and now think more than ever that it‚Äôs the right answer.\nWhat most people want in life can mostly be boiled down to the pursuit and gaining of power. Some types of power are good for you, but the most common ones always leave you unfulfilled. You know what does fulfil you though? Love. It fills you up better than power ever could. Let‚Äôs review how most things we chase in life are just hidden forms of power.\n\nTypes of Unfulfilling Power\nStarting with unfulfilling power. These are things that can be attained in ok ways, but doing them just for the pursuit of having it is a good way to waste your life.\n\nMoney: The most popular form of power and game we play in society. Granted you want to get to a certain level of money to have a good life, but after that the more money you have the less fulfilled it makes you. Mo money, mo problems.\nStatus: This is power over how people in society look at you. There are many ways to get status. Going to an elite college or working at a prestigious job are ways to gain status. You also gain status around how you spend your money. Living in a fancy house in a fancy neighborhood punches a ticket to a different level of status than someone living in a trailer park. It‚Äôs easy for people to see what you own, and most people own certain things to convey status. Does a Rolex tell more accurate time than your phone? No but it shows that you are the type of person who can spend $10,000 minimum on an accessory you don‚Äôt really need. Increases in status normally feed your ego, which is not good for anyone.\nBeauty: Having power over other‚Äôs attention and sexual desires. This is a weird one because it can either be gained naturally (just being born) or through external means (botox, plastic surgery). If you hit the genetic lottery, congrats, but the external route is a slippery slope that usually doesn‚Äôt end well.\nFame: Having power over people‚Äôs attention. Will Smith says the process of becoming famous is fun, but maintaining fame is just ok, and losing fame is hell. You don‚Äôt know how important having a private life is until you become a famous person. Can Will Smith casually go to a local park with his family on a Sunday? Nope. His life is forever inconvenienced by his fame. How much would he pay to have a quiet afternoon on a park bench with no interruptions? It might be more than you think.\nLegacy: Having power after you‚Äôre dead. How many people donate tens of millions to a university anonymously? Almost none. Most donate and in return get their name on a building. Maybe they‚Äôre stewards of higher education but maybe they just want their name to be remembered after they‚Äôre dead. Who cares if people remember your name 100 years from now, you will be dead. I repeat, you will be dead. Has your grandpa ever talked to you about his grandpa while growing up? Probably not. People are forgotten and that‚Äôs ok.\nLeadership Roles: Having power over people‚Äôs careers. Most people think the further they progress in their careers, the more likely they will end up managing a large team or company. While that‚Äôs true for a lot of jobs, it may not be everything you expected. Maybe instead of the management promotion you actually just wanted to work on higher impact projects and have more autonomy over how you get your work done. Think about a band like the Rolling Stones. They are very good at their job, but it‚Äôs not like after their second album their record label said ‚Äúyou have been doing such a good job that we think you should be promoted and start overlooking other bands‚Äù. That would be crazy. Instead they were given every opportunity to write more songs and perform to bigger audiences. I think knowledge work will move more towards that in the future. Corporate rock-stars will forgo the management path and truly become the best in the world at something. With scalable technology the best person in the world at a task can literally do it for everyone else in the world.\n\n\n\nTypes of Fulfilling Power\nI‚Äôm not saying that we should all shave our head, sell our possessions, and live as a monk. It‚Äôs ok to do things in life that increase your power, but I recommend trying to increase types of power that can actually fulfill you instead of leaving you wanting more. There are types of power that compound as you continue to increase them. These are true super powers that should be prioritized in life. Here are the types of power worth pursuing.\n\nFreedom: Having power over how you spend your time. Why does America rock? People will say the freedom they have, which is another way to say they can truly do whatever they want. Having control over your time is the best power you can have.\nHealth: Having power over your own body. You might have a million thoughts and worries bounce around your head in a typical day, but when you‚Äôre sick or injured you only think about one thing. Improve your sleep, diet, and exercise and see if your life doesn‚Äôt improve by an order of magnitude.\nLearning: How does that old saying go again? Oh yeah, knowledge is power. Some would argue it‚Äôs potential power, waiting to be applied. Noticed I listed this as learning and not knowledge. Having knowledge is great but there is never a reason to stop acquiring knowledge throughout life. This power compounds as you learn more. Lean into the eighth wonder of the world and keep learning.\nReputation: Having power over other‚Äôs perceptions of your character. Warren Buffet can do billion dollar deals over the phone, no contract needed, because of his stellar reputation he built over a lifetime. A good reputation can increase your luck surface area in life. A good reputation takes time to build, but can be gone in an instant.\n\nRelationships: Having power over other people‚Äôs time. Loving friends, an amazing spouse, and children running around your house are some of the most rewarding things in life. Not having strong relationships is literally the equivalent to smoking when it comes to lifespan. I would choose these relationships wisely though. Once when I was in college my business class went to visit a local bank in Kansas City. The bank‚Äôs founder, a hunched over man in his 80s, came out to speak to us students. When asked for advice for living a good life, his response was simple. He said, ‚Äúbe around good people‚Äù. The older I get the more this advice makes more sense. You truly are the average of the five people you hang out with most. Choose wisely.\n\n\n\nLove Over Power\nLove what you do, love who you spend your time with, and most importantly love yourself. The most important things in life boil down to love. Truly a first principle when it comes to living a good life. Call me a hippie but it‚Äôs the truth. Stay away from the most popular desires in life that ultimately are unfulfilling power, double down on power that does fulfill you, and most importantly optimize for more love in your life."
  },
  {
    "objectID": "posts/2024-02-19-frp-journey/index.html",
    "href": "posts/2024-02-19-frp-journey/index.html",
    "title": "Thoughts on My Journey in Microsoft‚Äôs Finance Rotation Program",
    "section": "",
    "text": "On new years day of 2015, I got an email from a Microsoft recruiter to schedule a phone screen for Microsoft‚Äôs Finance Rotation Program (FRP). A week after the phone screen, I was reading the book The Alchemist during a study break and started to think about my personal legend. I thought Microsoft would be a good opportunity and hoped for another interview. That‚Äôs right when my phone rang, it was the recruiter informing me that I was about to be chosen for the final super day in February. The weekend before the big interview in Redmond, my brother got me tickets to see the Oklahoma City Thunder take on the LA Clippers, a present for my 21st birthday. I told him I couldn‚Äôt go, because I needed time to prepare for the biggest interview of my life. Laughing, he said I had nothing to worry about, I would get the job, and we can use this weekend to celebrate the new job. His confidence in me, and the uncanny coincidence with the Alchemist book, were the ‚Äúomens I‚Äôd follow‚Äù into my own personal legend. Thankfully the interview went well, I got the offer, and the rest is history.\nThe Finance Rotation Program at Microsoft is a two year program where recent college graduates complete four rotations across various groups in finance, each lasting six months. It‚Äôs a great opportunity to learn about different parts of Microsoft‚Äôs business, while also being a world class ‚Äútry it before you buy it‚Äù experience for finding your optimal career path. You get to find what your destined to do, and Microsoft gets a well rounded finance professional that is tuned to find the exact job they want long term. A true win-win.\nI often get asked to talk about my experience from would be FRP‚Äôs still in school and existing FRP‚Äôs in the program. My career path in the FRP is nontraditional in nature, but is hopefully one anyone with enough high agency can do as well. I write this to those high agency folks. People who want to do interesting things in the world and help others. What follows is my path in the FRP and what I recommend for today‚Äôs FRP‚Äôs."
  },
  {
    "objectID": "posts/2024-02-19-frp-journey/index.html#rotations",
    "href": "posts/2024-02-19-frp-journey/index.html#rotations",
    "title": "Thoughts on My Journey in Microsoft‚Äôs Finance Rotation Program",
    "section": "Rotations",
    "text": "Rotations\n\nInternship: Venture Integration\nI came into the FRP as an intern in the summer of 2015, the same summer Microsoft launched Windows 10. It was a fun time to join finance at Microsoft. Satya Nadella was only installed as CEO a few years back, same as Amy Hood the CFO. The stock price was in the 40s and no one thought Microsoft was doing anything cool or exciting. Safe to say most students at my college had MacBooks instead of Surfaces.\nThe Venture Integration team was responsible for helping acquired companies integrate successfully into Microsoft. While Corporate Development teams work on closing the deal, the Venture Integration team does everything else. From doing due diligence on the companies financial statements, digging through the products source code, all the way to determining if the acquired CEO would move to Redmond. It was a cool space to be in. Code names were used for each deal, and the deal flow was strong. Often we started a new deal every two weeks.\nMy role on the team didn‚Äôt involve running deals, although I did get to help with modeling the integration costs for one deal. I was tasked with running the quarterly scorecard process. This involved a lot of cat herding. For each recently closed deal, I would track down metrics to see if we were on track for a successful integration into Microsoft. This was more project management and less about financial modeling or analysis. The cool part was on the last day of my internship I completed the scorecard powerpoint and personally sent it to the CFO for review, who would then share it with the board of directors. So, in a way, I got to work on something that Bill Gate‚Äôs got to see. Most likely it was buried in the appendix of the board materials but I know my guy Bill loves to read! So I‚Äôm sure he saw it and was blown away by my excellence. At least that‚Äôs what I tell myself.\nThe life changing moment for me happened towards the end of the summer. The tech team responsible for reviewing the code of acquired companies hosted a brown bag lunch session about a new service called ‚ÄúAzure Machine Learning Studio‚Äù. They demoed a drag and drop tool that could build a machine learning model to predict if an event was going to happen or not. With a few clicks of their mouse they had a working model up and running in a self-serve UI. Only a few lines of code were written, with most of the magic happening in predefined lego blocks they pieced together. My mind was blown. It seemed like a magic trick to me, and I had to figure out how they did it. In school I always loved building three statement financial models to value a companies stock price. The biggest assumption we had to make was around future revenue growth, and that number was always made up. You could say xyz company‚Äôs revenue will continue to grow 10% because of xyz reason, but in the end it was a guess. The fact that you could build a machine learning model to create a more accurate prediction of the future was astounding to me, and I had to learn more.\nWithin a week after that presentation, all of the FRP interns got to meet with the CFO. A lot of questions were asked. I‚Äôm not even sure who asked it but a question around AI was brought up. Amy said that AI and machine learning will become very important in finance one day, and it will be an important skill for everyone to know. There are few moments of pure clarity in a lifetime where everything comes together and makes perfect sense. This was one of those moments for me. I had to figure out this AI thing. I had absolutely no clue how to start. I was only a finance major with zero technical skills. What I lacked in hard skills I made up for with enthusiasm to go figure it out any way possible.\nThe internship ended and thankfully I got the return offer. I remember the head of the FRP looking at my manager during the final review meeting. She said, ‚Äúshould you tell him or should I tell him‚Äù. My heart dropped, I thought I messed up and wasn‚Äôt getting the offer. Thankfully that confusion was cleared up real fast and I had a return offer in my hand. How can I say no to an opportunity to come back to Microsoft? Within a week back at school I accepted the offer and had total peace of mind going into my final year of undergrad.\n\n\nFirst Rotation: Windows COGS\nMy first rotation was a classic rhythm of business (ROB) rotation in the world of Windows, Microsoft‚Äôs oldest business. It was a great place to earn my sea legs in traditional FP&A work. I was responsible for owning the royalty portfolio for ‚Äúcodec‚Äù payments. Basically there is special software needed to read/write disks that are inserted into an optical drive on a computer. That software comes from companies like Dolbe, and Microsoft had to pay them to license the software on Windows OS. Wow, that makes me sound old. Working on royalties for optical drives on computers. Safe to say no one even thinks about optical drives anymore.\nIt wasn‚Äôt the prettiest work, but it helped me learn the basics of closing the books each month and forecasting the future each quarter. I had to submit journal entries and make manual adjustments to our payments when needed. Once you got the hang of pulling the data and making journal entries, the work wasn‚Äôt all that hard. So most of my time toward the end of the rotation was spent trying to automate every single part of the ROB process. What was initially a complex refresh process handed to me at the start of my rotation was transformed into a simple click to refresh excel that did all of the heavy lifting for you. It was my pride and joy.\nMy manager gave me a powerful piece of feedback towards the end of the rotation. He said that I liked building things. Some people like to start things, others like to keep a process going or optimize it. I was someone who liked to build new things. That idea stuck with me. I eventually became the guy on the team who could build any complex workflow as a model in excel. For example building out the new forecast consolidation file that all team members could add their component forecasts into. While it was fun to build this kind of stuff, it wasn‚Äôt AI work. For that I had to continue on my data hero‚Äôs journey.\n\n\nSecond Rotation: Worldwide Commercial Solutions Finance\nFor my second rotation I wanted to leave the world of ROB behind and get my hands dirty with data. I went to a team that did two things. First was analytics around discounts and customer renewals across Microsoft‚Äôs commercial business, and the second was around supporting the financing of customer purchases. Back in the day a customer could finance a large software purchase so they didn‚Äôt have to pay for most of it up front. We would partner with a bank to finance the payment or even do the financing ourselves through our Treasury department.\nThis rotation was all around analytics, which was exactly what I needed. I was able to analyze patterns around the type of discounts we give to customers, and how those discounts compound as a customer continues to renew their software purchase contracts. Another big ticket analytics item was around tracking special azure financing deals. This was a key metric we reported directly to the CFO, so a lot of eye balls were on it. I was the guy to track down this data and create interesting ways to understand it.\nPower BI was recently released. No one on the team knew how to use it, but understood how powerful it could be when visualizing data. So I became the Power BI guy on the team. The one to figure out how to use it, then teach it to everyone else. Thankfully the previous FRP started them on the Power BI journey by building some initial dashboards. My job was to improve those dashboards, then get everyone else up to speed on how they could build their own. It was a solid experience in the world of analytics. One where no one could show you how to solve certain problems. You were on your own and had to figure everything out yourself.\nThe analytics experience was great, but still no AI. While on the team I tried to find ways we could apply AI and machine learning to specific areas but couldn‚Äôt find a good opportunity. At this point I started to learn more on my own through books and courses on the internet. I was flying blind, with no one to guide me to ensure I was headed in the right direction. That‚Äôs when the power of serendipity came and altered the course of my FRP career.\nAnother FRP analyst in my class got her last pick on her list of ranked rotations. Like choice number 32 out of 32 options. Right before she started on her team for the second rotation, there was a large re-org and she was able to now chose wherever she‚Äôd like to go in the broader org. A true choose your own adventure. She had an information systems background and found a team that was just starting to get into the world of machine learning. I heard about this and knew exactly where I wanted to go for my third rotation.\n\n\nThird Rotation: Finance Business Intelligence Services\nWorking for the FBI, what a fun thing to say. The FBI Services team was almost an in house consultancy team, who supported most folks in finance. If someone wanted a special dashboard, custom tool, or even machine learning model they could reach out to this team and get the help they needed. The team consisted mostly of program manager (PM) type roles for full time employees, and various technical roles for vendors.\nComing to this team was being at the right place at the right time. In the summer of 2015 (during my internship) our CFO reached out to the head of the cloud engineering team, asking him to help get some initial machine learning solutions off the ground. Eventually this work was transferred to the FBI Services team, who hired a team of data science vendors to keep the work going. Most of the ML solutions centered around time series forecasting, since that is easily the biggest bang for buck ML work in the corporate finance space. Almost everyone under the CFO is responsible for some sort of forecast, so the impact with ML is enormous.\nI came on the team as a PM who would partner with the data science vendors to build ML solutions. It was a dream come true. Finally I had people I can talk to about ML. Pick their brain. See how they think about ML problems. I also found a data science certification offered through Microsoft (on the EdX learning platform). It had a full learning track for python, no coding experience required. Thankfully as a Microsoft employee I could take the courses for free. While on the team I was able to make the learning part of my job, and even devote part of each day to taking the courses. It was a dream come true.\nThe big project on the team was around creating a centralized tool for our finance teams in the field, teams who support sales and sit all around the world, to use for their quarterly forecast of the commercial business. We were tasked with building a tool that could combine machine learning methods with existing sales pipeline based methods (like taking what‚Äôs in the sales pipeline and multiplying it by how many deals we have closed on average historically). One tool that everyone in the field could use instead of building their own manual excel model that eats up weeks of their time every forecast cycle.\nIt was a fun project to work on. The stakes were high, and no one had done anything like it before. It wasn‚Äôt just a machine learning project, but also a centralization and automation project. A crazy ven diagram of categories of technology. Over the course of my six month rotation we (and I say that lightly since I was the rookie on this project) were able to go from initial concept to working plugin in excel that could automatically calculate the forecast for the user, allow them to select what forecast methods they‚Äôd like to use, make any manual adjustments, and save that data back to a data cube. A herculean effort that had a few late nights toward the end of the project. It was a fantastic learning experience, with so many different opportunities to learn in one project. How to work with technical and non-technical people. Getting the most from ML models, and knowing their limitations. The change management required to get to people to want to use the new tool and move away from what they‚Äôve always done.\nI was also able to knock out the entire Microsoft certification in data science. I could now write python code to analyze data, train models, and create predictions. Everything that astounded me as an intern was now in my tool kit. I had data super powers and it felt awesome. The only problem is I had no idea what came next. Towards the end of the rotation I wanted to stay on the team and graduate early. Unfortunately the previous FRP was able to secure a full time spot on the team before I asked. Now I was in a tough spot. Knowing what I wanted to do, but not knowing where to do it. Thankfully serendipity came to my rescue.\nAt a FRP manager round table, my manager noted to the group that I was looking for a home for my fourth and final rotation. I wanted to work on machine learning but had no clue what team to rank highly. Another manager spoke up, saying they were just starting to dip their toes in the ML space and could have me on the team. That chance encounter lead me to ranking that rotation as my number one pick. Other FRP‚Äôs thought I was crazy. Usually that rotation went to a first year FRP, often as someone‚Äôs first rotation. I saw it as an opportunity to work on a technical team and carve out a path to working full time on ML. Safe to say after I joined the team every FRP who has worked on the team since has been a second year analyst.\n\n\nFourth Rotation: Office/Dynamics/Bing Business Intelligence\nMy final rotation was a BI team that supported finance teams in the Office, Dynamics, and Bing product spaces. Previous FRP‚Äôs worked on an executive scorecard that would get sent to senior leaders. I had my fill of scorecards while an intern so I was relieved that they were changing the rotation to allow me to work on machine learning problems. I felt like the luckiest guy in the world. I could actually write code to build machine learning models and get paid for it, all without having a degree in the subject.\nI worked on two main projects while on the team. One was around forecasting search traffic volume for our Bing business partners, and another was focused on trying to predict if a potential customer purchase in our sales pipeline was going to close or not. The search volume project was exciting because I could write code from scratch to create the forecasts. It was my first true ML project that I was responsible for coding. Building something from nothing, it was exhilarating. Just like my first rotation manager said.\nThe second project was a tough one. Being able to create a classification model to predict the likelihood of a deal in our customer sales pipeline closing was a tricky project. One where getting high quality historical data was tough to get. I knew this was something analytics teams in sales had to of tried to solve before, so I sent out the bat signal through a few distribution groups and got a hit. A data science team in India had done the exact work I needed and could send me the results of their models prediction. What should have taken months of work was now widdled down to a few weeks to pull the prediction data and display it in a Power BI. Leveraging the work of others proved a powerful lesson for me, and allowed me to finish my project that much faster.\nMy time on the team flew by and I was quickly approaching the end of my FRP tenure, meaning I needed to find a full time job ASAP. Most of my time quickly shifted to checking internal job boards and emailing managers in other BI teams, hoping they had a spot for me. I wanted this ML train to keep rolling. My current team said that they could potentially offer me a job, but I had to wait a little longer. This did little to remove any fear of not finding a job post FRP graduation. FRP‚Äôs were given some flexibility in finding a full time job. After graduating the program in September, Microsoft gave you about 4-6 weeks more to settle into a final job. You would still get paid, even if you didn‚Äôt have a team to call home. Being in this corporate limbo sounds nice in theory but in reality is awful. Like your days are numbered.\nThe waiting ended in a re-org of another BI team joining ours. So there was a hiring freeze in the meantime while both teams were being combined. After that, a job was offered, and the rest is history. I‚Äôm still on that team today. We‚Äôve been through a lot of re-orgs since 2018, but I still get to work on ML every single day. I consider myself lucky."
  },
  {
    "objectID": "posts/2024-02-19-frp-journey/index.html#advice",
    "href": "posts/2024-02-19-frp-journey/index.html#advice",
    "title": "Thoughts on My Journey in Microsoft‚Äôs Finance Rotation Program",
    "section": "Advice",
    "text": "Advice\nHere is my advice for anyone interested in the FRP, currently in the program, or FRP‚Äôs who are about to graduate and start the next phase of their career. I have to say that this advice may not stand the test of time, hiring practices constantly change. So take these next words with a grain of salt.\n\nAspiring FRP‚Äôs\nRecruiting for the FRP used to be done at target schools. Places like Texas, Penn, and the University of Washington. Kids from these schools normally had the best shot at getting an interview, because Microsoft would come onto campus and do interviews. Thankfully this started to change when I applied for the internship. Now Microsoft finds FRPs from any school in the country, even recently expanding to other parts of the world. This opens up the opportunity of joining the FRP to almost anyone in the world, but because of this it becomes harder to get recognized in a sea full of applications.\nBefore you even apply, you need to put yourself in a strong position by building a portfolio of experiences and skills that make you a perfect match for the FRP. Here is what I recommend.\n\nHigh Agency: Having good grades is a fantastic accomplishment, but nowadays everyone has good grades. It becomes more of a check box than something that differentiates yourself from others. Grades show that you can follow instructions, keep deadlines, and demonstrate some type of understanding of your study area. What companies want though are people who go out into the world and make things happen. People who are proactive instead of reactive, or better put have high agency. The best definition I‚Äôve heard of high agency is ‚Äúa person you would call to bail you out of a third world prison‚Äù. Someone who can do that can most likely figure out how to do any kind of job, and become indispensable to their company. The best way to showcase high agency to recruiters at Microsoft is to show how you spend your time outside of the classroom. If you have a 4.0 but only play video games when you‚Äôre not studying, your resume will most likely get thrown away. Don‚Äôt get me wrong, you could be a fantastic hire but a recruiter has no idea how you get along with others, your communication skills, or anything that‚Äôs not related to reading a textbook and taking a test to prove you read the textbook. The best way to showcase high agency is with leadership experience. Running your own business, being the vice president of your sorority, treasurer of your finance club, running your own charity, working a full time job to pay for school. These are all ways to demonstrate that you are someone who goes out and makes things happen. That you indeed have high agency.\nPrevious Work Experience: Having already done the job makes it easy to get a similar job. This can sometimes be a chicken or the egg situation, where needing a previous job to get your first job isn‚Äôt possible. That‚Äôs why I think you should start small. For me, it started the summer after 8th grade. I got a job in the concession stand of my local city pool. It wasn‚Äôt fun work, but lead to my second job working for the famous Jack Stack BBQ in the catering department. That job lead to multiple summers working at two different golf courses as a cart boy. All to say that it eventually lead to my first real business job working as an intern on Fridays during the school year at a financial advisor company. I didn‚Äôt do much, but I showed up every Friday ready to help. Having that experience gave me the opportunity to work in FP&A at HR&R Block in Kansas City as a legit intern. That experience then opened the door to work at Microsoft as an FRP intern. Small beginnings lead to massive results. Having a crap job at the beginning can lead to your dream job later down the line. Start this path as soon as you can, because hard work compounds.\nData Superpowers: In this modern age of AI, knowing how to work with data is critical. I‚Äôm not just talking excel skills. Being able to pull, manipulate, visualize, and tell stories with data is most of what you do in Finance at Microsoft. Having some programming or low code skills in data tools like python or Power BI can go a long way. This gets supercharged with tools like ChatGPT, where a little knowledge of coding can quickly make you on par with an entry level data scientist or software engineer. Imagine not learning email or Microsoft Office in the 90s. Not knowing email today is basically a firable offense. Using AI powered tools might have the same path in knowledge work. Get ahead and get data superpowers.\n\nYou might have all of those experiences and skills called out above, but that‚Äôs only half the battle. You still have to get noticed somehow. Either by a recruiter or someone else who works at the company. Here is what I recommend.\n\nConferences: Microsoft has pivoted away from on campus interviews and now does a lot of recruiting at conferences. This is how the first round of interviews get offered. These conferences are usually meetings of large student organizations like Association of Latino Professionals for America (ALPFA), Management Leadership for Tomorrow (MLT), Out for Undergrad (O4U), and National Association of Black Accountants (NABA).\nEmployee Referral: Getting your resume referred by a current or FRP alum can go a long way, but it‚Äôs no guarantee of an interview. The best way to get a referral is by actually knowing someone who works at the company, or having a shared interest with them. Blindly messaging Microsoft employees on LinkedIn could work, but you‚Äôd have better success finding someone who went to your same high school or served in the same branch of the military. Having a shared connection around a place or thing helps.\nIntern First: The best way to get a full time offer is to be an intern the summer before. 50-90% of full time FRP‚Äôs were an intern the previous summer. This is the path with the greatest opportunity of a job. Don‚Äôt go work at a bank in the summer, assuming you‚Äôll get a shot at Microsoft in the fall if you don‚Äôt like your bank job. Be in it to win it from the start.\n\n\n\nCurrent FRP‚Äôs\nFor folks currently in the FRP, I hope you‚Äôre having just as much fun as I did. Normally when working with FRP‚Äôs today I have to fight the urge to grab them around the shoulders and beg them to cherish being in the FRP (just like Billy Madison). In order to get the most out of the program you should do the following.\n\nAlways ask for what you want. The biggest unlock I learned in the FRP is to just be up front with what you want in the rotation. Telling your manager at the beginning of the rotation what you‚Äôd hope to get out of the next six months can unlock doors you never thought possible. In my fourth rotation, they literally changed the entire rotation because I simply asked if I could solely work on building ML solutions. Don‚Äôt forget I‚Äôm still working in that job six years later. The best approach is to ask for what you want before you rank your next rotation. Bear in mind you can‚Äôt just say something like ‚ÄúI want to work with the CFO directly‚Äù. I‚Äôve seen interns do this in the past, and it doesn‚Äôt end well for them. Know what you want, have some sort of skill or experience doing it, then ask for it.\nAlways see who else is working on something first. It‚Äôs always fun to build something from scratch. An important skill to learn in the FRP though is building on the work of others. This is a key component of how impact gets measured at the company. In my fourth rotation, I was able to have impact 10x as fast on the deal pipeline analysis project because I found another data scientist team who already had a deal pipeline conversion model I could leverage and start using immediately. If I didn‚Äôt ask I would have spent most of my time trying to build one, which may not have been useful at all. Always ask around before building something new.\nEmbrace serendipity. Your career path is never going to work out 100% exactly like you planned it, and you don‚Äôt want it to. You need to brace serendipity, keeping an open mind for making the most of new opportunities. The best example was the FRP in my class who got their dead last choice of rotation, but ended up working on that team full time post FRP. You never know where a road is going to take you. Don‚Äôt be afraid to explore different paths or see things through on an existing path.\n\n\n\nGraduating FRP‚Äôs\nMy last piece of advice comes to those who are ending their FRP tenure and now look on to the rest of their career. It can be a scary feeling, so here‚Äôs what I recommend.\n\nCast a wide net. Don‚Äôt just apply to one job, hoping you‚Äôre going to get it and end up with the perfect job. You need to apply to a crap ton of jobs. Don‚Äôt be picky. Even apply to jobs that don‚Äôt look exciting on paper, because that could change once you meet with the hiring manager. Serendipity could strike in your favor.\nThe best jobs go to people with high agency. Your perfect job is out there, but it may not be listed on the internal company job board. So have high agency and chase down opportunities that don‚Äôt even exist yet. This starts months before graduation. You need to network with teams you are interested in, meeting with potential hiring managers who could one day give you a job. For me, I went to every business intelligence team in finance. I emailed the director level leader, gave my short sales pitch saying I wanted do work on ML solutions for their team, and asked if we could chat to discuss any openings on their team. I‚Äôd even ask them if they knew any other team I should go talk to. Most didn‚Äôt offer me a job. Some continued to offer me interviews years after the FRP. You just never know. If you are talking to teams that aren‚Äôt hiring, you are basically in your own talent pool. Looking for opportunities that might materialize later once you graduate. If a job does open on that team, who is their first call? Probably the person who proactively reached out and made a connection well before a job was ever created. Use this to your advantage.\nJoin for the team, not the job. The job will eventually come. Being in the right position is important more than what you do. Most jobs after graduating may not be the most fun. They will be work, maybe even grueling. Someone needs to do the work and congrats it‚Äôs your turn to eat a plate of crap for the company. Going to the right team is definitely more important that getting the dream job you wanted on a crap team. For me, I took a job on my fourth rotations team. They said I could do ML work, but most of my time would be spent as a program manager (PM). I hated PM work, but knowing that part of my time could be doing ML work was all I needed to hear. Eventually my job morphed into 100% ML work after I was able to prove to my manager that I could have more impact on the ML side than the PM side of my job. Being on the right team or org allows you more opportunity to work on things you eventually want your career to morph into. The team I joined post FRP has now morphed into a 400+ person engineering team. One that‚Äôs quickly becoming the defacto engineering team in finance at Microsoft. Being on this team over the years has created much more opportunity compared to doing similar work on a different team. Thankfully I was well positioned, even though I didn‚Äôt have the ideal job initially. See what I mean? So don‚Äôt be too picky on the job, but more picky on the team."
  },
  {
    "objectID": "posts/2024-02-19-frp-journey/index.html#closing-thoughts",
    "href": "posts/2024-02-19-frp-journey/index.html#closing-thoughts",
    "title": "Thoughts on My Journey in Microsoft‚Äôs Finance Rotation Program",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nBeing an FRP teaches you a lot. How to ramp up quickly on new skills, being able to juggle many things at a time, effective communication and teamwork, and countless other skills. All things that make you a well rounded person both professionally and personally. Some of my favorite memories come from the summer internship or hanging out with my FRP coworkers in building 26 (before they tore it down). I think it‚Äôs a great idea for anyone who is interested in technology and business. If you‚Äôd like to apply to the program, please do so on the Microsoft career website. Whether you‚Äôre interviewing for the FRP or trying to land your post FRP role, I wish you good luck!"
  },
  {
    "objectID": "posts/2024-01-26-weekend-reads/index.html#articles",
    "href": "posts/2024-01-26-weekend-reads/index.html#articles",
    "title": "Weekend Reads (1/26/24)",
    "section": "Articles",
    "text": "Articles\n\nA Techno-Trad Take on Fertility\nThis is Your Reminder to Say No\nSigns and Portents: Hints about the next year of AI\nMicrosoft Releases Copilot Pro"
  },
  {
    "objectID": "posts/2024-01-26-weekend-reads/index.html#podcasts",
    "href": "posts/2024-01-26-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (1/26/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nSal Khan on the Future of Education\nPeter Attia on Huberman Lab\nMarkel CEO on The Knowledge Project"
  },
  {
    "objectID": "posts/2024-01-26-weekend-reads/index.html#tweets",
    "href": "posts/2024-01-26-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (1/26/24)",
    "section": "Tweets",
    "text": "Tweets\n\nPeacock Boosting Their App Downloads\nSurface Area in Your Life"
  },
  {
    "objectID": "posts/2024-01-26-weekend-reads/index.html#books",
    "href": "posts/2024-01-26-weekend-reads/index.html#books",
    "title": "Weekend Reads (1/26/24)",
    "section": "Books",
    "text": "Books\n\nAtomic Habits by James Clear"
  },
  {
    "objectID": "posts/2024-01-26-weekend-reads/index.html#products",
    "href": "posts/2024-01-26-weekend-reads/index.html#products",
    "title": "Weekend Reads (1/26/24)",
    "section": "Products",
    "text": "Products\n\nTuo Circadian Lightbulb"
  },
  {
    "objectID": "posts/2024-01-05-weekend-reads/index.html#articles",
    "href": "posts/2024-01-05-weekend-reads/index.html#articles",
    "title": "Weekend Reads (1/5/24)",
    "section": "Articles",
    "text": "Articles\n\nSahil Bloom: The Best Ideas of 2023\nSam Altman: What I Wish Someone Had Told Me"
  },
  {
    "objectID": "posts/2024-01-05-weekend-reads/index.html#videos",
    "href": "posts/2024-01-05-weekend-reads/index.html#videos",
    "title": "Weekend Reads (1/5/24)",
    "section": "Videos",
    "text": "Videos\n\nLast Lecture Series: How to Live an Asymmetric Life"
  },
  {
    "objectID": "posts/2024-01-05-weekend-reads/index.html#podcasts",
    "href": "posts/2024-01-05-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (1/5/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nAli Abdaal on Modern Wisdom\nBill Perkins on Modern Wisdom"
  },
  {
    "objectID": "posts/2024-01-05-weekend-reads/index.html#books",
    "href": "posts/2024-01-05-weekend-reads/index.html#books",
    "title": "Weekend Reads (1/5/24)",
    "section": "Books",
    "text": "Books\n\nThe Daily Laws by Robert Greene\nThe Daily Stoic by Ryan Holiday\nA Calendar of Wisdom by Leo Tolstoy"
  },
  {
    "objectID": "posts/2023-12-22-weekend-reads/index.html#articles",
    "href": "posts/2023-12-22-weekend-reads/index.html#articles",
    "title": "Weekend Reads (12/22/23)",
    "section": "Articles",
    "text": "Articles\n\n4 Types of Professional Time: Always make time for consumption and ideation time.\nMicrosoft Releases Phi-2: Small models are so hot right now.\nNo Meetings, No Deadlines, No Full Time Employees: One meeting a quarter at Gumroad. Yep, that‚Äôs the dream. One can only imagine."
  },
  {
    "objectID": "posts/2023-12-22-weekend-reads/index.html#videos",
    "href": "posts/2023-12-22-weekend-reads/index.html#videos",
    "title": "Weekend Reads (12/22/23)",
    "section": "Videos",
    "text": "Videos\n\nMaking LLMs Uncool Again: Hear thoughts from an AI legend, Jeremy Howard.\nA16Z AMA: Various topics tackled by the top dogs at Andreesen Horowitz.\nAngeList QnA with Naval: You can never go wrong hearing from Naval."
  },
  {
    "objectID": "posts/2023-12-22-weekend-reads/index.html#podcasts",
    "href": "posts/2023-12-22-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (12/22/23)",
    "section": "Podcasts",
    "text": "Podcasts\n\nGeorge Mack on Modern Wisdom: Powerful insights on the role memes play in society."
  },
  {
    "objectID": "posts/2023-12-22-weekend-reads/index.html#books",
    "href": "posts/2023-12-22-weekend-reads/index.html#books",
    "title": "Weekend Reads (12/22/23)",
    "section": "Books",
    "text": "Books\n\nAnthology of Naval Ravikant: Don‚Äôt think, just buy this book and re-read once a month.\nPoor Charlie‚Äôs Almanac: Timeless wisdom from Charlie Munger, may he rest in peace."
  },
  {
    "objectID": "posts/2023-12-09-weekend-reads/index.html#articles",
    "href": "posts/2023-12-09-weekend-reads/index.html#articles",
    "title": "Weekend Reads (12/9/23)",
    "section": "Articles",
    "text": "Articles\n\nHow AI agents will impact the world: great post from Bill Gates exploring agents and their new place in our world.\nLangchain expands collaboration with Microsoft: so cool to see that open-source LLM tech like langchain can coexist with tech giants like Microsoft."
  },
  {
    "objectID": "posts/2023-12-09-weekend-reads/index.html#videos",
    "href": "posts/2023-12-09-weekend-reads/index.html#videos",
    "title": "Weekend Reads (12/9/23)",
    "section": "Videos",
    "text": "Videos\n\nHow large language models work: perfect intro to LLMs from an AI legend\nAI Engineering Summit: interesting videos about a new discipline\n\nOpen questions about AI engineering\nClimbing the ladder of abstraction\nThe intelligent interface"
  },
  {
    "objectID": "posts/2023-12-09-weekend-reads/index.html#podcasts",
    "href": "posts/2023-12-09-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (12/9/23)",
    "section": "Podcasts",
    "text": "Podcasts\n\nDr.¬†Andrew Huberman on The Tim Ferris Show: Dr.¬†Huberman is a gift to humanity. So much great insight per minute of listening."
  },
  {
    "objectID": "posts/2023-12-09-weekend-reads/index.html#books",
    "href": "posts/2023-12-09-weekend-reads/index.html#books",
    "title": "Weekend Reads (12/9/23)",
    "section": "Books",
    "text": "Books\n\nCurrently Reading\n\nThe Anthonolgy of Balaji Srinivasan: Need to get excited about the future? Learn from one of the best in Balaji.\n\n\n\nRe-Reading\n\nTools of Titans: Re-reding this once a year is almost mandatory for me now. So much great insight that stands the test of time."
  },
  {
    "objectID": "posts/2023-10-19-aa-tam/index.html",
    "href": "posts/2023-10-19-aa-tam/index.html",
    "title": "Total Addressable Market for Advanced Analytics in Finance",
    "section": "",
    "text": "What would you say, you do here? I often have to explain the buzzword term ‚Äúadvanced analytics‚Äù and why exactly does a team like that exist in finance. Shouldn‚Äôt we just be called a business intelligence team or data team? The reason my advanced analytics team exists can be boiled down to one sentence. To reinvent how we use new technology to look forward, look backward, and make the next best decision in finance. This one phrase has a lot of surface area, so let‚Äôs peel back the onion and dive deeper.\nThe first two buckets cover most activities related to rhythm of business (RoB) work that is so crucial to finance, from budgeting and forecasting to the close process. This contains the bulk of the opportunity or addressable market. The final frontier is helping finance teams make the next best decision, which takes up every other activity outside of the traditional RoB activities.\n\nLooking Forward\nBy definition finance tries to make decisions about the future. What will out revenue look like? How is headcount going to change over the next 12 months? Are we going to actually hit our budget numbers for Q3? Most of looking forward is about some sort of time series forecasting. This is where I think the biggest bang for buck lies in the finance space. Most finance teams need some sort of prediction about the future, and most predictions are time related.\n\n\nLooking Backward\nAfter making predictions about the future, we have to measure what actually happened in the business. This is the ‚Äúclose process‚Äù you might hear finance pros discuss (sometimes with distaste). At the end of each month, each quarter, each fiscal year the financial numbers for that period need to lock. Journal entries need to complete, allocations have to be allocated, and every metaphorical ‚Äúi‚Äù is dotted and ‚Äút‚Äù is crossed. This is probably the most manual process in finance today that hasn‚Äôt changed much since the inception of spreadsheets and powerpoint. Human beings have to look at reports, see how the month closed and if that number was different then what was initially forecasted, and then go tell their CFO why we were off $100 or $1,000,000,000 that month. As we use algorithms to look into the future, we can use algorithms to look into the past. If ML can be adopted in the forecast process, then there is a good chance we can use it during close. This is a unique problem to solve though. Currently we can get decent interpretability of the future forecast, but understanding the error between the forecast and what actually happened is not an exact science. I also think it‚Äôs a problem most data scientists may not think about today since it might only be important in small niches like finance where explaining that forecast error is crucial and prevents adoption of ML solutions. Large language models I think can unlock this problem, where we can build agents that can peel back the layers of forecasts and run the variance analysis to create the explanations we need. This takes a very manual process and could eventually speed up the story gathering part of close from a few days to a few minutes. Very exciting.\n\n\nMaking the Next Best Decision\nThis is the final frontier for advanced analytics in finance. Outside of the financial RoB processes like forecast and close, finance people do so much other kinds of work that mostly revolve around making data informed decisions about what to do in the business. How should we price our new product? Will the new product hurt sales of our older products? What‚Äôs the lifetime value of our customer? Should we buy back shares or double down on R&D? All of these questions cannot fit into a standard time series forecasting bucket, or any standard ML bucket. Each question has its own nuance and needs to be approached in a different way. One way to solve this with technology it to add armies of data & AI professionals who can answer these non-standard questions on demand, but this method doesn‚Äôt scale. Finance is notorious for running lean and being reluctant to add large technical teams that don‚Äôt do standard BI work like data pipelines and reporting. So again this is a space that is ripe for disruption through large language models. Again it‚Äôs agents to the rescue. Agents that can piece together various tools that can find data, pull data, and then analyze that data to help a human make a decision faster and with more precision. We are still so early in this space.\n\n\nFinal Thoughts\nAdvanced analytics in finance still has so much promise yet to be realized. Expectations are high and skeptics are always there. Brick by brick we get closer to making these ideas become a reality. If you are working on machine learning in your finance org, I salute you! It‚Äôs often nerve racking work that doesn‚Äôt always pay off immediately. Stay at it my friends, and the future will be yours."
  },
  {
    "objectID": "posts/2023-02-11-three-levels-of-ml-adoption/index.html",
    "href": "posts/2023-02-11-three-levels-of-ml-adoption/index.html",
    "title": "Three Levels of Machine Learning Adoption in Finance",
    "section": "",
    "text": "Growing machine learning adoption in your finance org is tough. There are many levels to clear, with final bosses to beat. This happens most in aspects of finance that are already being done manually by humans, like forecasting (time series). Below are the three questions or ‚Äúlevels‚Äù you need to clear before any company can truly leverage machine learning to its full potential in finance for forecasting.\n\nIs the ML forecast as or more accurate than the current process?\nCan you explain the number generated by the black box?\nIf ML forecasted $100, but actuals came in at $110, how do we explain the forecast variance?\n\nEach level needs to be fully cleared before you can tackle the next. Let‚Äôs dive in.\n\nFirst Level\nThe first level is pretty straight forward, you know what the accuracy bar is for the existing process, and in most cases you can create a machine learning forecast that can beat it. Even if you reach a similar level of accuracy with the ML process, ML can run in 95% less time than the manual process. Even then that can be a win for your finance team.\n\n\nSecond Level\nNow you‚Äôve built a machine learning process with great results. This is often where you will face the most resistance from your finance team. They will say ‚Äúoh great the forecast is super accurate, but how do I know how it came up with its number?‚Äù. This is tougher than just creating an accurate forecast, since there is often a trade off between building the most accurate model and building the most interpretable one. Thankfully this is a hot area of research right now, with lots of great open source tools being released. The big key to interpretable forecasts for finance tasks is to understand the seasonality and trend of your forecast, and also how outside drivers (macro, internal KPIs) effect your forecast. Your CFO might not care that your model is mostly driven by historical growth rates over the last 2-3 years but telling her that a key driver in your model is the rise of interest rates and lowering consumer sentiment is a great way to tell a story around the forecast and get more people bought in.\n\n\nThird Level\nThis is the final boss. This in uncharted territory because no data scientist or researcher is thinking about how their models impact the FP&A process at a company. This is a process specific to finance and one that doesn‚Äôt have a clear answer. If algorithms can be used to look into the future, then they could also be used to look into the past. Knowing how to automatically reconcile what was initially forecasted versus what actually happened would be a game changer. Allowing you to potentially automate much of the traditional close process in your finance team. Being able to explain the initial ML forecast in level two will allow you to eventually see the factors that contribute to the forecast variance during close.\n\n\nFinal Thoughts\nI think about each one of these questions every single day. Open source tools that my team are working on, like Finn, are being actively improved to answer each one of these questions. If you‚Äôre interested in using these tools at your company or helping to make them even better, please reach out to me on LinkedIn."
  },
  {
    "objectID": "posts/2021-12-03-finance-aa-book/index.html",
    "href": "posts/2021-12-03-finance-aa-book/index.html",
    "title": "Free Book on Advanced Analytics in Corporate Finance",
    "section": "",
    "text": "I‚Äôm excited to announce the release of a new R package called finnts, aka Finn. The package helps simplify the process of creating time series forecasts with statistical and machine learning models. Finn automates the more tedious aspects of machine learning. Things like data cleaning, feature engineering, back testing, and model selection are all handled automatically by Finn while still being flexible to many forecasting scenarios.\nFinn is a perfect solution for people new to machine learning as well as seasoned pros looking for a scalable way to put many forecasts into production. Please take a look at the package site, try it out, and let me know what you think!"
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html",
    "href": "posts/2021-08-26-personal-user-manual/index.html",
    "title": "Personal User Manual",
    "section": "",
    "text": "Below are some basic operating instructions for myself. It details how I work best personally and with others. I got the idea from a great book, called How Google Works. In the book they mention some higher up at the company writing a ‚Äúhow to fix me if broken‚Äù post that I found very informative. A lot of times we have to learn how best to work with others the hard way, through trial and error to see what works. Sometimes your coworker might tell you how they prefer to work together, but I cringe thinking about a lot of employees who work through gritted teeth in ways that go against how they work best. So in the below sections I detail how I personally work best throughout the week with myself and others."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#work-hours",
    "href": "posts/2021-08-26-personal-user-manual/index.html#work-hours",
    "title": "Personal User Manual",
    "section": "Work Hours",
    "text": "Work Hours\nIdeally I like to work from 7:30am - 4pm PST. I‚Äôm based out on the West Coast (Seattle) and like to work earlier in the day. This allows me to have a lot of focused deep work time in the first few hours of the work day, before any meetings are scheduled.\nMy mornings are sacred. That‚Äôs when I get my best thinking done, and the code flies out of my little meat fingers. I try to have multiple 90 to 120 min blocks of uninterrupted time for deep work. This is crucial to me getting any substantial work done in a day. I guard this time with my life."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#how-to-contact-me",
    "href": "posts/2021-08-26-personal-user-manual/index.html#how-to-contact-me",
    "title": "Personal User Manual",
    "section": "How To Contact Me",
    "text": "How To Contact Me\nBecause of my love for morning deep work, I often have my email and teams app closed on my computer, and my phone is set on do not disturb. This means any urgent email or teams message may not get seen until the early afternoon. If there truly is an emergency and you need to get a hold of me, please call me through teams directly. That way I have to answer and break my deep work flow. Just make sure it‚Äôs a true emergency that cannot wait.\nUsually email and IM are the best ways to contact me. For email I try to respond within 1-3 days, and for IM within 24 hours (usually same day). Sending me an IM with just ‚Äúhey‚Äù is usually not my favorite message to receive. If you have a question or request, then say it right out of the gate when sending a message. This will help me prioritize what you‚Äôre asking over other things I‚Äôm working on that day.\nSomething I‚Äôm trying to find good solutions for is the dreaded ‚Äúdo you have a min for a quick call‚Äù? I know sometimes just asking a question out loud or sharing your screen is the quickest way to figure something out, but it can rob any flow or attention from the person being asked. So I only try to do this sparingly with others on my team and hope others do the same with me."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#dev-vs-meeting-days",
    "href": "posts/2021-08-26-personal-user-manual/index.html#dev-vs-meeting-days",
    "title": "Personal User Manual",
    "section": "Dev vs Meeting Days",
    "text": "Dev vs Meeting Days\nI love the idea from Paul Graham about a maker vs manager schedule. Basically it states that even a single meeting can throw off any momentum or flow one might have when building things. It could take more than an hour to get ramped up into a specific coding task, and one 30 minute meeting in the middle of the afternoon can totally derail and developer productivity after the meeting.\nThis has caused me to segment my days in two ways. First, on Monday/Wednesday/Friday I block out my calendar entirely. These are my dev days, where I only focus on writing code and deep thinking. Then my Tuesday/Thursdays are wide open for anyone to schedule meetings with me. I have found this to work very well. Usually when I meet a new person and hear the words ‚ÄúI‚Äôll set up some time to chat‚Äù, I immediately tell them that Tuesday/Thursday are usually the best days for me. It‚Äôs been hard to keep this kind of schedule, especially for meetings that have a lot of people attached to them where finding open calendar spots is next to impossible. There are cases where I make an exception and meet with people on my dev days, but this is only after asking the organizer if it‚Äôs imperative that I come to the meeting. I have learned that a lot of the times I can skip and catch up with folks later.\nSaying no to meeting requests has been a long journey for me, and I‚Äôm now getting to the point where I don‚Äôt feel like a total jerk declining meetings where I would just be a fly on the wall. What a weight off my shoulders."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#my-meeting-checklist",
    "href": "posts/2021-08-26-personal-user-manual/index.html#my-meeting-checklist",
    "title": "Personal User Manual",
    "section": "My Meeting Checklist",
    "text": "My Meeting Checklist\nI try to work against the default setting of having a meeting for everything. So here is my checklist of reasons I will attend a meeting, if a meeting doesn‚Äôt meet any point below, I have to decline it. Granted there are exceptions to the rule but most of the time I will say no.\nReasons I Will Join A Meeting\n\nPre-defined agenda send ahead of time\nMain discussion point is around making a decision or coordinating next steps on a work item (either with prep work from #5)\nA business partner is having issues with something I built for them, and we cannot fix via IM/email\nShowcasing a new solution, that requires a QnA or broader discussion\nSome sort of prep work was done ahead of time, usually in the form of a meeting pre-read document or email\n\nReasons I Will Not Join A Meeting\n\nNo agenda\nMain discussion point is a status update (should have been an email), sharing information (should have been an email) or brainstorming without any previous work done\nNo prep work was done ahead of time\nIf there are more than 7 people invited (more people don‚Äôt make a meeting better)\n\nIf the meeting falls more into the second list instead of the first, I will simply decline the meeting and ask for it to be transcribed via teams so I can catch up via Teams Copilot. If the meeting is about sharing information, I will promise to send a written write up of my update."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#asynchronous-by-default",
    "href": "posts/2021-08-26-personal-user-manual/index.html#asynchronous-by-default",
    "title": "Personal User Manual",
    "section": "Asynchronous by Default",
    "text": "Asynchronous by Default\nI love the idea of asynchronous communication. Less meetings, more documentation and long form writing. I think a lot of times people schedule meetings to think through something rather than reflecting on their own and coming up with a solution before sharing it with others. Scheduling a meeting to ‚Äúthink‚Äù about a problem or project is a huge waste of everyone who attended the meeting. It‚Äôs also selfish.\nHaving everyone come together to discuss something should not be the first option in my opinion. I think personal reflection and writing your thoughts down before sharing with others is much better. It helps clarify your thinking and prevents groupthink from seeping into decisions. Thinking about a problem, writing down possible solutions with thorough analysis, then sharing the results with team members asynchronously is a much better use of everyone‚Äôs time then just having a meeting where no preparation is done beforehand. You could still have a meeting to discuss the written document but usually I only like to do that if there are certain components you‚Äôd like to debate about what was written of if a final decision needs to be made.\nAs more teams realize the power of distributed teams, asynchronous communication and writing skills will become an even bigger superpower. Doing this well not only makes better use of everyone‚Äôs time, but allows for better decisions to be made. More people have the opportunity to voice their opinion, especially introverts. More people have the freedom to work when they want to work, preventing the dreaded 4pm Friday meeting üò™."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#best-times-to-meet",
    "href": "posts/2021-08-26-personal-user-manual/index.html#best-times-to-meet",
    "title": "Personal User Manual",
    "section": "Best Times to Meet",
    "text": "Best Times to Meet\nThe best times I like to meet are at the edges of existing components of my day. For example, I‚Äôd rather meet right after my lunch break around 1:00/1:30pm then having a meeting a 2:30pm. Having a meeting in the middle of the afternoon splits that part of the day in half, which can limit the amount of deep work I get get in a day since it takes a while to get in the zone or flow state. That‚Äôs ok because I have set up days (Tue/Thu) where I batch all my meetings together. So if I have a bunch of meetings, all with half hour gaps in between, I don‚Äôt worry because I know the next day I‚Äôll have a lot of unstructured time for deep work.\nWith that being said, normally I like meetings to start after 10:30am, take a break over the noon lunch hour, then end by 4pm. Thankfully I work on a team and company that respects those hours, but on occasion I have a 4pm meeting I just can‚Äôt avoid. Meeting with a CVP for example. Not the end of the world."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#how-to-fix-me-if-broken",
    "href": "posts/2021-08-26-personal-user-manual/index.html#how-to-fix-me-if-broken",
    "title": "Personal User Manual",
    "section": "How to Fix Me if Broken",
    "text": "How to Fix Me if Broken\nI had to do some hard thinking about this one. In the end I settled on a simple answer, just leave me alone to think. If I even get flustered or seem super stressed, normally time to myself is the cure. Either just some quiet reflection at my desk or a quick walk around the block.\nI‚Äôm an introvert so sometimes too many meetings and deadlines can get the best of me and stress me out too much, solitude is the brain reboot I need to come back down to normal."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#final-thoughts",
    "href": "posts/2021-08-26-personal-user-manual/index.html#final-thoughts",
    "title": "Personal User Manual",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI think more people should write about how they work best. Obviously not everyone will have the same working style as other team members, but knowing how someone honestly likes to get their work done helps find common ground and make compromises to get the best collaboration.\nI could even see HR products like Microsoft Viva coming into the mix soon with some type of employee bio or baseball card with high level stats on who they are and how they do their work. Aggregating this kind of info across teams could lead to some interesting insights. If everyone on the team likes to email and IM vs having a meeting to discuss, then maybe the boss should stop scheduling weekly in person update meetings. The same goes for a team that is mostly extroverted and likes to meet in person to work things through. If a new introvert who enjoys asynchronous work joins the team, maybe the team members could create ways to accommodate their working style to find the best of both worlds. Like recording a meeting and allowing them to contribute on their own time instead of joining synchronously. By doing this more teams will become more inclusive, and hopefully more productive. Which is a buzzword every management team loves to hear."
  },
  {
    "objectID": "posts/2021-08-19-why-courses-stink/index.html",
    "href": "posts/2021-08-19-why-courses-stink/index.html",
    "title": "Why Online Courses Stink",
    "section": "",
    "text": "Nowadays there are thousands of courses on the internet, teaching everything from cooking to machine learning with R. Being a self-taught coder myself I started off taking these online courses, but overtime have realized they may not be the best way of learning.\n\nCertifications Over Knowledge\nFor me, traditional schooling techniques do not ensure I master the material or learn anything that will live in my long term memory. What it does ensure is that I will do anything to ‚Äúcheck the box‚Äù of a certain class and try to get a good grade. Getting a good grade and actually learning the material are two separate things in these environments.\nGetting a good grade entails learning only what is required to pass some test to prove you learned. This is a dangerous game because there are various ways for students to learn as little as possible and still ace a test or exam. What do these tests prove? Mostly that you can regurgitate facts and equations on command. I think we have all been in a situation where we studied just enough to cover what‚Äôs going to be on the exam, then a week after the test all of that knowledge seeps out through our ears. Forever lost.\nThe same can be said about online courses that offer ‚Äúcertifications‚Äù. This creates the same attitude as regular classes you may have already taken in high school and college. Knowing just enough to get by and get the class credit, or in this case, a certification. The crazy thing about certifications though is that they don‚Äôt show how well you did in the course, only that you passed. That‚Äôs like seeing a college transcript with only pass/fail grades instead of a GPA.\n\n\nPrioritizing Just in Time Learning\nGetting out of the ‚Äúcertification learning mode‚Äù has been critical to my development in programming and data science. What I have realized is the importance of applying whatever I‚Äôm learning to real world projects as soon as possible. Here is the learning loop I follow.\n\nLearn enough foundational concepts to be dangerous\nImmediately apply the knowledge to a real world project\nRefer back to learning materials as needed to reinforce concepts\nGo further down the learning rabbit hole with advanced concepts to improve project\nRinse and repeat above steps to constantly reinforce topics and improve project\n\nThis kind of just in time learning has been amazing for me. It‚Äôs allowed me to quickly ramp up on the basics and reinforce them in my brain by working on things in my job that actually matter. Not just some fake project within a course, but something that actually effects my job where the stakes are higher. Then after I get the hang of the basics I go back to the learning well to learn more advanced concepts to then come back and apply within the same real world project.\n\n\nReversal\nThe downside to approaching skill acquisition like this is that there is no fancy diploma or certification you can hang on your wall or post on LinkedIn. What you do have is real work on projects you can hang your hat on. These projects may be specific to your job, meaning you may not be able to share them publicly as proof that you know something. This is where the challenge arises. Sites like LinkedIn are working towards creating ways to show that you have a particular skill via online skill assessments, but this still falls back into the learning enough to check the box approach. Building a solution portfolio that lives publicly on places like GitHub is a great way to show employers you have certain skills, but takes extra time since the solutions you build would have to be outside of your job work.\nOne potential solution that‚Äôs the best of both worlds is to write blogs about what you‚Äôre working on, without spilling any company secrets, and publishing open source software that allows your work to scale to others. That way others can see your growth and commitment to learning, while also reaping the benefits of your work. Truly a win-win."
  },
  {
    "objectID": "posts/2021-08-12-freecodecamp-appreciation/index.html",
    "href": "posts/2021-08-12-freecodecamp-appreciation/index.html",
    "title": "freeCodeCamp Appreciation Post",
    "section": "",
    "text": "Nowadays there are a ton of places to learn programming skills online. From books, to videos, all the way to bootcamps. A lot of these methods work great, but they usually cost money. The free content just teaches the basics and is a hook for you to continue into a paid course or site. freeCodeCamp to the rescue!\nFCC is a free site to learn many aspects of coding. From web development to cybersecurity. They have courses that teach you in depth about programming, with in the browser coding exercises to help hone your skills. To get a certification for a course or topic, all you need to do is complete the project list on the site. This makes it great for complete newbies, as well as people with some experience who would benefit from the project work and certification.\nMy favorite part of FCC is not the site itself, but instead their youtube page. This in a gold mine of useful tutorials and classes taught by experts in their field. The youtube has so much more content than their site. Safe to say it would take years to go through and watch everything üòç.\nOne last final thing I wanted to call out is the newsletter that Quincy Larson, founder of freeCodeCamp, sends out each week. It‚Äôs a fantastic list of learning resources curated from the FCC blog and youtube site. Definitely worth subscribing to."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thoughts on Things",
    "section": "",
    "text": "Inner Circles of Life\n\n\n\n\n\n\nlife\n\n\n\nHow priorities change as we get older\n\n\n\n\n\nJun 28, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft Finance ML Forecasting Journey: Part Two\n\n\n\n\n\n\nfinance\n\n\nmachine-learning\n\n\nforecasting\n\n\n\nCentralizing our largest forecast process with machine learning\n\n\n\n\n\nJun 26, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft Finance ML Forecasting Journey: Part One\n\n\n\n\n\n\nfinance\n\n\nmachine-learning\n\n\nforecasting\n\n\n\nGoing from 0 to 1 with machine learning\n\n\n\n\n\nJun 12, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nHow To Master Storytelling\n\n\n\n\n\n\npodcast\n\n\nlearning\n\n\n\nNo one remembers charts and numbers. They only remember stories. The best story always wins. Just don‚Äôt tell stories about your vacation. No one wants to hear that.\n\n\n\n\n\nJun 4, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Deep Learning Last\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nDeep learning isn‚Äôt as effective as more traditional ML models\n\n\n\n\n\nMay 31, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Model Combinations Are King\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nUsually a combination of multiple models is more accurate than just one model‚Äôs prediction\n\n\n\n\n\nMay 28, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nMultistep Horizon Forecasting With finnts\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\nfinnts\n\n\n\nNew feature that improves forecast accuracy\n\n\n\n\n\nMay 13, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Capture Uncertainty\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nShowing the back testing results and future uncertainty of a model‚Äôs forecast builds more trust\n\n\n\n\n\nMay 7, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Simple Models Are Better Models\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nLike occam‚Äôs razor, the best model is often the one with the least amount of inputs\n\n\n\n\n\nMay 3, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: The Magic Is In The Feature Engineering\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nHow you transform your data before model training can transform a mediocre forecast into a world class forecast\n\n\n\n\n\nMay 1, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Order Is Important\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nWhen time is involved, how your data is ordered makes all the difference\n\n\n\n\n\nApr 23, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (4/19/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nApr 19, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Higher Grain Higher Accuracy\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nForecasting by country is often more accurate than forecasting by city. Forecasting by month is often more accurate than forecasting by day\n\n\n\n\n\nApr 18, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (4/12/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nApr 12, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: The Future Is Similar To The Past\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nIf you expect the future to be drastically different than past data, you will have a hard time training accurate models\n\n\n\n\n\nApr 11, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Garbage In, Garbage Out\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nTraining a model on bad data leads to bad forecasts\n\n\n\n\n\nApr 8, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (4/5/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nApr 5, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Domain Expertise\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nKnowing what factors actually influence what you are trying to forecast is more important than which ML model to train\n\n\n\n\n\nApr 2, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on First Principles in Time Series Forecasting\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nBuilding blocks of creating a great forecast\n\n\n\n\n\nMar 26, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (3/22/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nMar 22, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (3/16/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nMar 16, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on Power\n\n\n\n\n\n\nlife\n\n\n\nMost desires are attempts to gain power, make sure you choose the right kind\n\n\n\n\n\nMar 13, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (2/23/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nFeb 23, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on My Journey in Microsoft‚Äôs Finance Rotation Program\n\n\n\n\n\n\ncareer\n\n\nfinance\n\n\n\nMy path through the FRP and advice to aspiring and current analysts\n\n\n\n\n\nFeb 19, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (2/16/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nFeb 16, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (1/26/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nJan 26, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (1/12/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nJan 12, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (1/5/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nJan 5, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on The Almanack of Naval Ravikant\n\n\n\n\n\n\nbooks\n\n\n\nKey takeaways from the book\n\n\n\n\n\nDec 30, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (12/22/23)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nDec 22, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nIs AI the End of Specialization?\n\n\n\n\n\n\nAI\n\n\nLLM\n\n\n\nHow AI will change high-skill professions like medicine and law\n\n\n\n\n\nDec 11, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (12/9/23)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week.\n\n\n\n\n\nDec 9, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Language Model Use in Finance\n\n\n\n\n\n\nllm\n\n\nfinance\n\n\n\nHow we can start using Generative AI technology within corporate finance\n\n\n\n\n\nNov 24, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTotal Addressable Market for Advanced Analytics in Finance\n\n\n\n\n\n\nmachine-learning\n\n\nfinance\n\n\n\nUsing new technology to reinvent how we look forward, look backward, and make the next best decision in finance\n\n\n\n\n\nOct 19, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThe ML Language Wars Are Over, Large Language Models Won\n\n\n\n\n\n\nmachine-learning\n\n\nopen-source\n\n\n\nPicking the right tool for the job has never been easier with things like Chat-GPT\n\n\n\n\n\nFeb 13, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThree Levels of Machine Learning Adoption in Finance\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\nfinance\n\n\n\nBeating the three levels on your way to machine learning nirvana\n\n\n\n\n\nFeb 11, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nNew Year Resolutions Are Dumb\n\n\n\n\n\n\ngeneral\n\n\n\nDon‚Äôt do new year resolutions, be consistent instead\n\n\n\n\n\nJan 1, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nSwitching to Quarto\n\n\n\n\n\n\ngeneral\n\n\n\nTransitioning my site to Quarto\n\n\n\n\n\nSep 25, 2022\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nFree Book on Advanced Analytics in Corporate Finance\n\n\n\n\n\n\nmachine-learning\n\n\ncode\n\n\nlearning\n\n\nopen-source\n\n\n\nFree resources on building advanced analytics talent that thrives\n\n\n\n\n\nDec 3, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting Simplified with finnts\n\n\n\n\n\n\ncode\n\n\ntime-series\n\n\nml\n\n\n\nNew R package for time series\n\n\n\n\n\nSep 13, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal User Manual\n\n\n\n\n\n\ngeneral\n\n\nwork\n\n\n\nQuick facts about my working style\n\n\n\n\n\nAug 26, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nPro Bono Work for Stretch Projects\n\n\n\n\n\n\ncode\n\n\nlearning\n\n\n\nGetting experience before you‚Äôre qualified to do the work\n\n\n\n\n\nAug 24, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Online Courses Stink\n\n\n\n\n\n\ncode\n\n\nlearning\n\n\n\nPitfalls of technical courses\n\n\n\n\n\nAug 19, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Screenplays in VS Code\n\n\n\n\n\n\ncode\n\n\nmovies-tv\n\n\nopen-source\n\n\n\nOpen sourcing screenwriting\n\n\n\n\n\nAug 17, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nfreeCodeCamp Appreciation Post\n\n\n\n\n\n\ncode\n\n\n\nThis site rocks\n\n\n\n\n\nAug 12, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nBridging the ML Talent Gap in Corporate Finance\n\n\n\n\n\n\nml\n\n\nfinance\n\n\n\nFinding the diamonds in the rough\n\n\n\n\n\nAug 10, 2021\n\n\nMike Tokic\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, my name‚Äôs Mike Tokic. I currently work at Microsoft where I help build things with machine learning. These are my thoughts‚Ä¶.on things. These thoughts are my personal opinions and do not reflect the ideas/beliefs of my employer. There, now I can say whatever I want."
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "",
    "text": "Machine Learning (ML) is exploding, we are only limited by our imagination of how to bake ML into every aspect of our organizations. Within corporate finance the opportunities are endless. Most finance activities can be aided by algorithms.\n\nBudgeting and Forecasting\nAccount Reconciliation\nContract Risk Analysis\nA Million Other Things\n\nWhen finance departments start on their modern finance journey with machine learning, they might tackle the highest impact projects that could benefit the business the most. These will probably revolve around forecasting revenue and various balance sheet risk management solutions.\nWho implements these kinds of game changes in the early days of ML adoption? Help usually comes from these areas.\n\nExisting data scientists that work in another department\nConsultants that come in, build something, then leave\nVendors/Contractors you hire for an extended period of time\n\nLeveraging help from outside of the finance org can get the ball rolling quickly for those big ML solutions that have the most immediate impact. This process usually yields great results. Revenue is forecasted more accurately, and financial risk is better handled across various aspects of the business. Everyone gets a pat on the back and a pep in their step. This initial momentum creates a lot of energy, which stirs up new ideas of where ML can be applied next. This is when you fall into a hidden trap around trying to scale ML in a non-technical discipline like finance.\n\nsource"
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html#getting-the-ball-rolling",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html#getting-the-ball-rolling",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "",
    "text": "Machine Learning (ML) is exploding, we are only limited by our imagination of how to bake ML into every aspect of our organizations. Within corporate finance the opportunities are endless. Most finance activities can be aided by algorithms.\n\nBudgeting and Forecasting\nAccount Reconciliation\nContract Risk Analysis\nA Million Other Things\n\nWhen finance departments start on their modern finance journey with machine learning, they might tackle the highest impact projects that could benefit the business the most. These will probably revolve around forecasting revenue and various balance sheet risk management solutions.\nWho implements these kinds of game changes in the early days of ML adoption? Help usually comes from these areas.\n\nExisting data scientists that work in another department\nConsultants that come in, build something, then leave\nVendors/Contractors you hire for an extended period of time\n\nLeveraging help from outside of the finance org can get the ball rolling quickly for those big ML solutions that have the most immediate impact. This process usually yields great results. Revenue is forecasted more accurately, and financial risk is better handled across various aspects of the business. Everyone gets a pat on the back and a pep in their step. This initial momentum creates a lot of energy, which stirs up new ideas of where ML can be applied next. This is when you fall into a hidden trap around trying to scale ML in a non-technical discipline like finance.\n\nsource"
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html#what-got-you-here-wont-get-you-there",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html#what-got-you-here-wont-get-you-there",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "What Got You Here Won‚Äôt Get You There",
    "text": "What Got You Here Won‚Äôt Get You There\nYou quickly run into issues with the current business model of outside help. The more solutions you want to build, the more resources you need to build them. ML solutions expand on a linear scale. This linear approach runs into a myriad issues.\n\nData scientist teams in other orgs already have a job, and can only offer so much support.\nCosts grow with each new solution. Existing solution work never goes away. There will always be maintenance to constantly update data sources and improve the model as the business evolves. More solutions, more problems.\nPrinciple-Agent problem with vendors. External resources want to make sure they are always needed, which means they will build solutions in a way to ensure new solutions continue to scale linearly. New innovation around scaling solutions or democratization through tools may never come, since that removes them from the picture.\n\nIn addition to scale issues, there is another large problem of domain expertise. Most outside data scientists don‚Äôt understand the work of finance people. Those that claim they have experience in building finance solutions do not understand your company and its various nuances. This creates the need for program manager (PM) roles to be created in order to gather the domain expertise of finance users and translate it to these outside resources to build solutions. More layers of abstraction lead to longer dev times, more communication overhead, and just more everything. The linear approach is a short-term solution to a long-term problem.\n\nLeveraging data science talent outside of finance is a short-term solution to a long-term problem.\n\nYou could also entertain the idea of hiring these vendors and other outside talent as full-time employees to sit within the finance department. This might seem to solve some of the problems called out above, but it still doesn‚Äôt solve the long-term problem. These outside data scientists turned finance employees may not have finance backgrounds, and in some respects are ‚Äúdata mercenaries‚Äù. They could be people that enjoy building ML solutions but do not care who benefits from them or if they are solving the right problems within finance.\nThere‚Äôs got to be a better way!\n\nsource"
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html#long-term-strategy",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html#long-term-strategy",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "Long-Term Strategy",
    "text": "Long-Term Strategy\nThe solution has been right under our nose this entire time. The best people are already within your company, and are already experts in finance. Yes, I‚Äôm talking about your finance employees! What I‚Äôm proposing is to find the data scientists who don‚Äôt know they are data scientists yet, the diamonds in the rough.\nThe diamonds I‚Äôm talking about usually fit a familiar archetype, which you may have already seen within your finance employees.\n\nLike to build things. They hate repeating themselves, and often take pride in the large financial models they have built in excel. They might already know a little bit of programming, whether that‚Äôs excel macros using VBA or having a little python knowledge to break out whenever they feel constrained by the four walls of excel.\nConstantly learning. They are the definition of a growth mindset. Being a ‚Äúlearn it all‚Äù is more important than being a ‚Äúknow it all‚Äù kind of person.\nMaster communicator. This one is typically harder to find. Having someone who loves to build, and then can turn around and sell it to others is hard to do. Being able to explain complex technical subjects is a super power itself.\n\nIf you send out the bat signal looking for these types of folks, many will come running. Others may be more hesitant to reach out. Imposter syndrome could be the biggest reason these diamonds stay in the rough. They could be a rock star within the org, but they may not see themselves that way. So always be on the lookout for talent, even if they don‚Äôt see it in themselves.\nCreating a path from regular finance employee to data scientist is a hard path to build out. This is a problem I have been thinking about for some time, and more details will be provided in a future blog post. At a high level, an apprenticeship program is one that I personally see as a path to building finance centered data science teams. Most of us forgot about this model of learning, and think it‚Äôs only for electricians and plumbers today. I think it‚Äôs making a comeback in a big way, and look forward to detailing how I plan to implement it within my finance org. The best part is, once I do I will report back with information around how you can do it too!"
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html#final-thoughts",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html#final-thoughts",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIn conclusion, scaling out data science work within large finance orgs is tough. What initially got you started isn‚Äôt what will take you to the next level. There are many pitfalls along the way, and it‚Äôs not an easy process. What I hope to do is implement a system for growing data science talent that already exist within finance, then turn around and share those insights with the rest of the world. Stay tuned!"
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html#reversal",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html#reversal",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "Reversal",
    "text": "Reversal\n\nFor smaller finance orgs (&lt;50 employees), leveraging outside data science talent may be your best strategy. There are only a few places that ML can be implemented, and having a full time team on-site may not be the best financial decision.\nHiring external data science help as full time employees could work just fine if you find people who are passionate about solving problems within the business. They may not always be data mercenaries and could have existing business backgrounds in addition to data science skills.\nThere has been a lot of innovation recently in no-code tools that enable anyone to produce a machine learning model for a variety of tasks. This democratization leads to more exponential adoption of ML, as opposed to the linear adoption curve. This can post it‚Äôs own risks, where cookie cutter solutions don‚Äôt fit the need to every problem. Once you do need a more custom solution based on the needs of the business, you are right back at square one in using outside help. Custom solutions and no-code tools will merge into a happy harmony one day, and will intermingle as self-serve solutions need to be transformed into custom ones managed by a data science team.\nIt will take time to grow existing finance talent into data science roles, and it may not work out. There is some downside to this approach, but it comes with potentially unlimited upside when you empower the domain experts to build things they think are useful.\nLack of career path and poor engineering practices are likely to spring up after going down this paths. This is something to think a lot about, and is something I plan to address in a future post around building an apprenticeship program."
  },
  {
    "objectID": "posts/2021-08-17-screenplays-vscode/index.html",
    "href": "posts/2021-08-17-screenplays-vscode/index.html",
    "title": "Writing Screenplays in VS Code",
    "section": "",
    "text": "Today there are a lot of tools for people to write screenplays. Most of them are paid and require hundreds of dollars. I recently stumbled across a VS Code add called Better Fountain that allows you to turn simple markdown documents into a professionally formatted screenplay. It is built upon an open source tool called Fountain, that is a bare bones screenwriting tool that is free and easy to use.\nWhy the heck would you want to write a screenplay is VS Code? I was asking myself the same thing. If you come from a coding background, this is an easy transition. Heck, I‚Äôm writing this post currently in a markdown file within VS Code üòé. What I think is most interesting is the tools we use to code can now be applied to the older art of storytelling with screenwriting.\nThe biggest tool most coders leverage is some sort of version control. VS Code easily integrates with GitHub, so you can push your latest script (screenplay) changes to version control and track their changes with ease. Side note, kinda funny how folks in Hollywood call their screenplays ‚Äúscripts‚Äù. Guess coding and storytelling are more alike then we thought.\nWhat makes the idea of hosting your script on GitHub is how it could be seen by others. It could be a public repository, and essentially open source. An open source movie? That would be crazy. What if others could read the script and make updates through pull requests? It could be the best version of fan fiction. What if others could read the script, upvote it in some way (using GitHub stars) to catch the eyes of top directors and film studios? That way a movie could already be audience approved beforehand.\nThis becomes very interesting when you add in the idea of block chain and smart contracts. If a movie studio wants to pick it up and take it into production, everyone who contributed to the script in the GitHub repo could automatically receive some sort of pre-arranged compensation when the movie gets made. There could even be new types of open source licenses built for these kinds of projects that have specified terms around compensating people who made the project possible.\nThere could also be some kind of crypto coin created for the repo, which would allow movie studios to buy into the project and either fund change requests to the script or set up a subsequent sequel or second season. The possibilities get more exciting the more I think about them.\nTo close this out, let‚Äôs list out everything that makes this a horrible idea. The biggest thing that comes to mind is that people don‚Äôt want to know what‚Äôs going to happen in a movie or tv show. Having the script publicly available somewhere like GitHub could ruin any twists or cliffhangers in a script, but at the same time we run into the same problem today if someone watches a movie anytime after opening day. Everyone knew the plot to Harry Potter before each movie was released, but people lined up around the corner to go watch them anyway. Maybe open source movies are not such a bad thing after all."
  },
  {
    "objectID": "posts/2021-08-24-probono-stretch-projects/index.html",
    "href": "posts/2021-08-24-probono-stretch-projects/index.html",
    "title": "Pro Bono Work for Stretch Projects",
    "section": "",
    "text": "Want to learn a new skill for your career? There are tons of places to learn the actual skill, from in person classes to free YouTube videos. What is sometimes the hard part is the application of the skill that is often required when trying to transition jobs or career paths. You know, the whole ‚Äúexperience‚Äù word employers throw out in job postings. Even entry level jobs ask for so many years of experience. This leads to a chicken or the egg problem. You need experience to get a job, but you need to get a job to gain experience.\nTechnical roles like data science and analytics suffer from this problem in a big way, where you normally see job postings asking for PHD‚Äôs or having 10 years of experience. The term data science has barely been around for 10 years, this is crazy."
  },
  {
    "objectID": "posts/2021-08-24-probono-stretch-projects/index.html#final-thoughts",
    "href": "posts/2021-08-24-probono-stretch-projects/index.html#final-thoughts",
    "title": "Pro Bono Work for Stretch Projects",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWhen learning something new, the best way to master it is by applying it in the real world on something that matters. That could be something within your current job, but oftentimes what you are learning are skills you want to grow into a different job or career. In order to gain experience in those skills before making that job switch, consider volunteering for a non-profit. They would be thrilled to have you."
  },
  {
    "objectID": "posts/2021-09-13-intro-finnts/index.html",
    "href": "posts/2021-09-13-intro-finnts/index.html",
    "title": "Time Series Forecasting Simplified with finnts",
    "section": "",
    "text": "I‚Äôm excited to announce the release of a new R package called finnts, aka Finn. The package helps simplify the process of creating time series forecasts with statistical and machine learning models. Finn automates the more tedious aspects of machine learning. Things like data cleaning, feature engineering, back testing, and model selection are all handled automatically by Finn while still being flexible to many forecasting scenarios.\nFinn is a perfect solution for people new to machine learning as well as seasoned pros looking for a scalable way to put many forecasts into production. Please take a look at the package site, try it out, and let me know what you think!"
  },
  {
    "objectID": "posts/2022-09-25-quarto-switch/index.html",
    "href": "posts/2022-09-25-quarto-switch/index.html",
    "title": "Switching to Quarto",
    "section": "",
    "text": "I‚Äôve recently switched this website over to quarto, the newest scientific publishing technology developed by RStudio, which recently changed its name to Posit. Lot‚Äôs of change going on at Posit, and they are doing some really cool things. One of the most interesting is quarto.\nQuarto builds on top of the already great work of Rmarkdown, which is a way to take code and covert it to other document types like pdf or html. With quarto you can write Python or R code, and create outputs that can be published anywhere, including as web pages.\nI used to leverage some complex Ruby code with Jekyll to produce this site, but with quarto the process was dramatically simplified. The design of ‚Äú.qmd‚Äù documents is very smooth and intuitive, allowing me to fully re-publish the site on a Sunday afternoon. I still leverage github pages, which connects to a ‚Äúdocs‚Äù folder in my repo where the quarto files are rendered as html, css, and javascript. Very simple.\nWith great technology like quarto, it begs the question if folks should use quarto for all written communication. From writing proposals, creating slide presentations, publishing papers, and creating technical documentation. All written as code and stored in version control. Exciting stuff!\nIf you‚Äôd like to learn more about quarto, check out their website!"
  },
  {
    "objectID": "posts/2023-01-01-new-year-resolutions/index.html",
    "href": "posts/2023-01-01-new-year-resolutions/index.html",
    "title": "New Year Resolutions Are Dumb",
    "section": "",
    "text": "New year, new me, right? Today marks the start of many peoples new year resolutions, or big goals they want to start chasing over the next year. For me I think that‚Äôs bad advice. Goals like ‚Äúget in shape‚Äù or ‚Äútravel more‚Äù don‚Äôt really mean anything and can easily get set aside as your life autopilot takes over. No wonder why most resolutions get dropped by February. Why do you think you have to sign a bunch of contracts when you join a gym? They know how our default settings work and ensure you‚Äôre paying for a while regardless if you come in. In reality, they would appreciate it if you didn‚Äôt come in at all. Less of your sweat to mop up.\nInstead I focus on two things, being consistent and trying to map out the next few steps in front of me.\nBeing consistent is what matters in anything you pursue in life. People overestimate what they can accomplish in the next few weeks but underestimate what they can accomplish in a year. Let‚Äôs change the resolutions we called out above to instead be more consistent. Instead of ‚Äúget in shape‚Äù we can rephrase that as ‚Äúexercise 30 min every day‚Äù, or ‚Äútravel more‚Äù can be changed to ‚Äúleave the country once a year‚Äù or ‚Äúget out of my town once a quarter‚Äù. These are now measurable things that will keep you honest instead of loftier ones that are easier to sweep under the rug.\nHaving long term goals is another trap I try to avoid. People plan, and God laughs. Having it all figured out is a quick way to getting let down when things don‚Äôt exactly go as you planned. Instead I focus on the next 1-3 steps in all aspects of my life. I don‚Äôt plan to have 12 pack abs, but rather focus on doing core 3-5 times a week. I don‚Äôt plan to be some hot shot director or senior engineer at my job, but rather focus on shipping the next few features on the big projects I work on. I still maintain a vision of the long term aspects of the things I work on in my job, but know that those are subject to change as I actually start to build out the next 1-3 steps.\nIf you are someone who enjoys writing goals, then all power to you. To each their own. For me I have found being consistent in a few things can compound into something special. Why not try it yourself and see where it takes you in 2023."
  },
  {
    "objectID": "posts/2023-02-12-language-wars/index.html",
    "href": "posts/2023-02-12-language-wars/index.html",
    "title": "The ML Language Wars Are Over, Large Language Models Won",
    "section": "",
    "text": "When I started learning data science there was an essential choice that had to be made at the beginning. One that has consequences that can ripple throughout your machine learning journey and even change your career options. I‚Äôm talking about the classic debate of R versus Python. Just like any hot issue, you will most likely get a different answer to this question based on who you ask. R is great for statistics, but Python is the best for production. You can‚Äôt do deep learning in R. Python is the second best language for everything. Over the years it seems that each side of the debate has only dug their heels in deeper. Whenever people ask me what language they should learn first, I usually give this recommendation. If you can only choose one, learn python, but eventually you will have to learn both to be a good data scientist. Building high quality solutions with data comes down to one thing, picking the right tool for the job.\nEach language has its strengths. Over the years the rise of new open source packages have started to even the playing field. For example R has always had amazing packages for time series, but now python is catching up. The same can be said for the rise of deep learning in R. While I still believe learning both will allow you to thrive in your data career, the rise of large language models (LLM) like OpenAI‚Äôs Chat-GPT are going to change the game forever.\nIf you haven‚Äôt heard of Chat-GPT yet then you are living under a rock, and congrats on being able to read this blog inside your underground doomsday bunker. These LLM‚Äôs can do a lot of things, but the game changer for the open source data space is being able to easily translate one language into another. For example I‚Äôm able to easily translate dplyr code in R into perfect pandas code in python. Over time these models will unlock the potential to translate every package from one language into another. Imagine writing a package in R, and have a GitHub action that calls a LLM to automatically convert it into python or vice versa. A data scientist could even write code in one language during a projects development, then automatically convert it into another when they move into production. The possibilities are endless!\nModels like Chat-GPT may not be at this level of language translation today, but like anything in the AI space the pace of innovation moves exponentially, and could be on the horizon sooner than you think. I for one personally can‚Äôt wait for this day to come."
  },
  {
    "objectID": "posts/2023-11-24-llm-in-finance/index.html",
    "href": "posts/2023-11-24-llm-in-finance/index.html",
    "title": "Large Language Model Use in Finance",
    "section": "",
    "text": "How can we use large language models in corporate finance? A lot of great work has been done inside of Microsoft Finance to find and consolidate ways we can leverage generative AI. Below are the common themes we found. Special shout out to the Microsoft modern finance community for their help in this effort."
  },
  {
    "objectID": "posts/2023-11-24-llm-in-finance/index.html#final-thoughts",
    "href": "posts/2023-11-24-llm-in-finance/index.html#final-thoughts",
    "title": "Large Language Model Use in Finance",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThere is no need to go directly into the deep end of the LLM pool with custom solutions. See what you can start using immediately with existing copilots and then build from there. Technology by definition is something that doesn‚Äôt quite work yet, so don‚Äôt be afraid to experiment and fail."
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#summary",
    "href": "posts/2023-12-11-ai-specialization/index.html#summary",
    "title": "Is AI the End of Specialization?",
    "section": "Summary",
    "text": "Summary\nAI and large language models (LLM) are slowly taking away value from jobs that used to require a lot of schooling, like medicine and law. It will even come for pure software development jobs. What AI cannot replace in the near term is specialized knowledge, skills that can only be obtained through apprenticeships. In the future, we need to focus more on gaining strong domain expertise across business, science, and engineering. But only in ways where specialized knowledge is needed, not simple skill acquisition that a LLM can do faster and cheaper. Going to a coding bootcamp to learn how to build web apps will help you make money this year at a company, but that same company might kick you to the curb three years from now when GitHub Copilot can code the entire app in a day."
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#more-specialization-more-knowledge",
    "href": "posts/2023-12-11-ai-specialization/index.html#more-specialization-more-knowledge",
    "title": "Is AI the End of Specialization?",
    "section": "More Specialization, More Knowledge",
    "text": "More Specialization, More Knowledge\nWhich jobs require the most schooling? This is a good proxy for how much knowledge someone has to attain before they are qualified to work in a specific job. Doctors and lawyers seem to require the most schooling before they can start their careers. Most likely because of all of the information they have to memorize and load into their head. The more specialized someone‚Äôs skills in a domain like medicine, the more money they can make. What‚Äôs interesting is that new large language models are becoming very good at the same thing, and can be trained in months, not years. How will this impact these high status, high pay jobs? Specialization will go through a fundamental shift in the age of AI."
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#story-time",
    "href": "posts/2023-12-11-ai-specialization/index.html#story-time",
    "title": "Is AI the End of Specialization?",
    "section": "Story Time",
    "text": "Story Time\nI recently had to get my knee checked out by a doctor. I tweaked it pretty good playing kickball at my grandma‚Äôs 90th birthday party (story for a different time). Most of my time spent getting my knee examined was not actually interacting with the doctor I came to see. Getting checked into the system, filling out forms, getting an xray. All things where the doctor was no where to be found. The doctor did finally come in to my room, after waiting a while, and briefly talked to me for two minutes. Yes, two minutes. Almost as if they couldn‚Äôt wait to leave and were in a rush to go somewhere else, maybe the golf course. They recommended I go get an MRI, and handed me a phone number to schedule it myself.\nI went to get the MRI, which was even stranger. It was in a different medical building, one with a waiting room the size of a small bedroom. There were no doctors there, just someone to check you in and someone to operate the MRI machine. After getting the MRI, the technician said I had some bruising on my knee. I asked what that meant and they said they were not a doctor and couldn‚Äôt tell me anything about my knee. A qualified doctor was required to do that.\nFinally, after another week I was able to see the doctor again. They looked at the MRI and told me some various technical jargon that meant my knee was ok and that I should just take it easy for another month to heal part of my bone that connects to the knee. This entire process took a month, all to see and speak to a doctor for five total minutes over three appointments. The cherry on top was that it cost me hundreds of dollars even with good insurance. Do you see where I‚Äôm going here?"
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#rethinking-healthcare",
    "href": "posts/2023-12-11-ai-specialization/index.html#rethinking-healthcare",
    "title": "Is AI the End of Specialization?",
    "section": "Rethinking Healthcare",
    "text": "Rethinking Healthcare\nWhat if instead I could go to a local pharmacy, and say that my knee hurts. There could be someone with high tech equipment who could xray my knee, and take an MRI if needed. An AI system would analyze the scans and flag any concerns. I could then get a final recommendation from the system. If something was wrong with my knee, it could recommend some physical therapy exercises. Or if something was really wrong it could route my request to a hospital that could do the surgery for me. This process could happen in an hour, instead of a month, and could cost me half the price without ever needing to see a real doctor. This kind of technology might be closer to reality then some might guess.\n\nDoctors who go to school most of their life could be replaced by a specially trained large language model that was trained on the history of medical research in a few months. It would already know everything about the human body, including what current therapies and medicines work the best. The best part is once it‚Äôs built, it can scale to the entire world. Rural villages in India could get the same healthcare as the rich in Beverly Hills.\nWould doctors still need to go to school? There is still an opportunity for wannabe doctors about to enter school. Instead of going to school for seven years, maybe they just jump to the very end of the training from the get go. Instead of building a strong foundation about everything in the body, they skip and go right to the path of specialization in a specific field of medicine. This is how doctors get paid the most anyways. If someone is interested in oncology (treating cancer), then maybe that‚Äôs what they start studying immediately. They can use AI tools to cover the basics on everything else and spend all of their waking moments on learning about cancer. Lowering the barriers to entry might make it easier for more folks to do medical research, at least until AI starts to get good at that too.\nIf doctors can be replaced, what happens to nurses? They already do a decent amount of the work already in hospitals, and are the main connection to a patient. Over time they could be the ones powered by AI who end of making all of the decisions and determining the best treatment options. A nurses role would change into a combined version of what doctors and nurses have done historically. Maybe even spawning a new type of job. Robotics is another space where AI can disrupt medicine. Would someone rather be treated by a human, or a machine that comes with 90% less medical bills? Money might do the talking there, but replacing human connection could be the most resistant to AI and automation."
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#ai-cannot-replace-specialized-knowledge",
    "href": "posts/2023-12-11-ai-specialization/index.html#ai-cannot-replace-specialized-knowledge",
    "title": "Is AI the End of Specialization?",
    "section": "AI Cannot Replace Specialized Knowledge",
    "text": "AI Cannot Replace Specialized Knowledge\nMemorizing all of the parts of the body and the latest medical research is easily done by LLMs. What could be more difficult is what Naval Ravikant calls specialized knowledge. These are skills that cannot be easily learned from a book or class. Instead they usually get developed through apprenticeships. A student learning from a master, Jedi style. My life in corporate finance is a good example of specialized knowledge. My company hires finance employees from all walks of life. Having a finance degree or experience is good but not crucial to succeeding. This is most likely because the work you actually do within the CFO‚Äôs org is very specialized, where what classes you took in school is not that helpful. Most of it, in the financial planning and analysis space, revolves around financial rhythms like forecasting and close. There is no class in college related to this work. You cannot watch a few YouTube videos and get up to speed in a weekend. You learn by doing, and mostly watching others do it. This apprenticeship like model is a powerful indicator of what jobs are hard to replace with AI. There have been efforts to automate things like forecasting with machine learning (ML) in finance, but we have a long way to go before that becomes the default for everyone. Even then there will be cases where a ML model cannot capture everything about a business in its training data, and a human will always need to be in a loop to impart their domain expertise.\nDomain expertise will become more valuable as LLMs can learn more technical concepts like coding and memorizing all facts on the internet. What it can‚Äôt do (not yet at least) is know how a specific business works inside and out like a human can. A LLM could write all of the code to build your companies website, but it will not know how to piece together logistics, manufacturing, sales, marketing, and design all at the same time like a CEO can. Maybe one day it will get closer, but for now we still need a human at the top directing all of these AI processes. That takes specialized knowledge."
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#final-thoughts",
    "href": "posts/2023-12-11-ai-specialization/index.html#final-thoughts",
    "title": "Is AI the End of Specialization?",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIn my current job, I believe strong domain knowledge about finance and our business is going to be way more important than what kind of coding skills you posses. Since LLMs will take over more of the coding tasks, leaving work that requires specialized knowledge for us humans to do. This could mean less specialists in technical domains that historically have paid the most, since that kind of knowledge was rare in society. With more emphasis on specialization for domain specific knowledge is a business instead. Coders who are like restaurant line cooks working their way through a backlog created by someone else might be at the highest risk of getting their jobs removed. Same goes for doctors who just spend five minutes reviewing a patients chart and telling them what to do next without any follow up or long-term investment.\nFinally, if AI can know the foundation of each profession, that makes it easier for anyone to come into the space and do good work. If they are able to obtain specialized knowledge quickly. This opens up opportunities for people to have multiple careers throughout their life, without the need of always going back to school or getting a credential of some type. Over the long term this gives the power to individuals with high agency and entrepreneurial spirit. Ones who can seize opportunities for innovation and make impact across many different domains."
  },
  {
    "objectID": "posts/2023-12-30-naval-book/index.html",
    "href": "posts/2023-12-30-naval-book/index.html",
    "title": "Thoughts on The Almanack of Naval Ravikant",
    "section": "",
    "text": "The Almanack of Naval Ravikant by Eric Jorgenson is something I like to read every few months. Eric was able to take all public content from Naval and put it into one place. I love that idea and think Eric will be successful with many other thought leaders.\nMeet Naval Ravikant: a modern philosopher-entrepreneur and the co-founder of AngelList. Renowned for his deep insights on technology, investing, and the art of living, Ravikant has become a luminary in Silicon Valley. Beyond his success with startups like X (Twitter) and Postmates, he captivates audiences with his profound perspectives on life and happiness, shared on his popular podcast and Twitter feed.\nEnjoy some of Naval‚Äôs best quotes and ideas I found most interesting.\n\nTypes of Leverage\n\nLabor means people working for you. It‚Äôs the oldest and most fought-over form of leverage. Labor leverage will impress your parents, but don‚Äôt waste your life chasing it. Money is a good form of leverage. It means every time you make a decision, you multiply it with money. It‚Äôs probably been the most dominate form of leverage in the last century.\nCode and media are permissionless leverage. They‚Äôre the leverage behind the newly rich. You can create software and media that works for you while you sleep.\n\nFocus on permissionless leverage. Often people think the next step in their career is managing people. That‚Äôs how they get more money, more power, more status. Is Taylor Swift Time‚Äôs 2023 person of the year because she is a great manager or because she is the best in the world at media (music, video, merch)? We‚Äôre quickly starting to see more one person empires being built. Through media with influencer‚Äôs, and through code like Satoshi Nakamoto (inventor of Bitcoin). Instead of trying to grow you career through scaling of people and capital, try the route with no gatekeepers.\nBalaji Srinivasan made an interesting point that once a robot can do something, you‚Äôve turned labor into capital. Meaning you just need the money to purchase the robot and then code to program it. Not long from now we might see managers only overseeing physical robots operating in the world of atoms (doing physical work) or AI agents operating in the world of bits (traditional knowledge work). The skills needed to manage these new digital workers revolve around writing code and coordination. Not traditional management skills.\n\n\nBuild or Sell\n\nLearn to sell. Learn to build. If you can do both, you will be unstoppable.\n\nI believe Bill Gates said this back in his heyday at Microsoft. This is the most powerful piece of advice in the book. Most businesses boil down to a product or service to sell. The most important people at a company either build that product or sell that product. If you do not do work like that today at your company, you might want to consider changing jobs. These roles have the greatest potential for impact on the world, and create the most financial rewards.\n\n\nSpecific Knowledge\n\nSpecific knowledge is knowledge you cannot be trained for. If society can train you, it can train someone else and replace you. When specific knowledge is taught, it‚Äôs through apprenticeships, not schools.\n\nThis is how you can prevent AI from taking your job. If what you do can‚Äôt be explained in a book, class, or YouTube videos, there is a good chance it will be hard for an AI model to figure out how to do it well.\n\n\nBe You\n\nThe way to get out of the competition trap is to be authentic, to find the thing you know how to do better than anybody. You know how to do it better because you love it, and no one can compete with you. If you love to do it, be authentic, and then figure out how to map that to what society actually wants. Apply some leverage and put your name on it. You take the risks, but you gain the rewards, have ownership and equity in what you‚Äôre doing, and just crank it up. Your goal in life is to find the people, business, project, or art that needs you the most. There is something out there just for you. What you don‚Äôt want to do is build checklists and decision frameworks built on what other people are doing. You‚Äôre never going to be them. You‚Äôll never be good at being somebody else. Technology democratizes consumption but consolidates production. The best person in the world at anything gets to do it for everyone.\n\nNothing to add here, Naval hit the nail on the head.\n\n\nLifelong Learning\n\nThe most important skill for getting rich is becoming a perpetual learner. You have to know how to learn anything you want to learn. The old model of making money is going to school for four years, getting your degree, and working as a professional for thirty years. But things change fast now. Now, you have to come up to speed on a new profession within nine months, and it‚Äôs obsolete four years later. But within those three productive years, you can get very wealthy.\n\nChatGPT was released by OpenAI about a year ago. In that time, people have had the opportunity to get up to speed on using generative AI models like GPT-3.5 and now GPT-4. There was no class in using these models, only the documentation on OpenAI‚Äôs website and videos on YouTube. For the brave folks who went ahead and learned how to use these models, the world will be theres the next three years while everyone else catches up. Learning how to use GPT-4 in your company to improve a product or become more productive is a form of permissionless leverage. Something of a superpower. The recent AI hype cycle might be winding down, but people with these skills are in demand. A few years from now another technology or idea might come out, another thing that‚Äôs not taught in schools. Never stop learning, never stop growing."
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#articles",
    "href": "posts/2024-01-12-weekend-reads/index.html#articles",
    "title": "Weekend Reads (1/12/24)",
    "section": "Articles",
    "text": "Articles\n\nScott Galloway‚Äôs 2024 Predictions\nThe Psychology of Persuasion\nDeep Dive on the AI Revolution"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#videos",
    "href": "posts/2024-01-12-weekend-reads/index.html#videos",
    "title": "Weekend Reads (1/12/24)",
    "section": "Videos",
    "text": "Videos\n\nDoes Athletic Greens Actually Work?\nDo Electrolyte Powders Actually Work?"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#podcasts",
    "href": "posts/2024-01-12-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (1/12/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\n2024 Predictions on All In\nGeorge Mack on Modern Wisdom"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#tweets",
    "href": "posts/2024-01-12-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (1/12/24)",
    "section": "Tweets",
    "text": "Tweets\n\nDiscipline is Overpriced, Incentives are Underpriced\nRicky Gervais at the Golden Globes"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#movies",
    "href": "posts/2024-01-12-weekend-reads/index.html#movies",
    "title": "Weekend Reads (1/12/24)",
    "section": "Movies",
    "text": "Movies\n\nPoor Things: In Theatres\nYou Are What You Eat (A Twin Experiment): On Netflix"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#books",
    "href": "posts/2024-01-12-weekend-reads/index.html#books",
    "title": "Weekend Reads (1/12/24)",
    "section": "Books",
    "text": "Books\n\nHidden Potential by Adam Grant"
  },
  {
    "objectID": "posts/2024-02-16-weekend-reads/index.html#articles",
    "href": "posts/2024-02-16-weekend-reads/index.html#articles",
    "title": "Weekend Reads (2/16/24)",
    "section": "Articles",
    "text": "Articles\n\n8 Lessons from ‚ÄúCurb Your Enthusiasm‚Äù\nThe Knowledge Economy Is Over. Welcome to the Allocation Economy\nThe Race Is On to Stop Ozempic Muscle Loss\nAll My Thoughts After 40 Hours in the Vision Pro"
  },
  {
    "objectID": "posts/2024-02-16-weekend-reads/index.html#tweets",
    "href": "posts/2024-02-16-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (2/16/24)",
    "section": "Tweets",
    "text": "Tweets\n\nHigh Agency by George Mack\nVision Pro Teardown by Trung Phan"
  },
  {
    "objectID": "posts/2024-02-16-weekend-reads/index.html#books",
    "href": "posts/2024-02-16-weekend-reads/index.html#books",
    "title": "Weekend Reads (2/16/24)",
    "section": "Books",
    "text": "Books\n\nThe Boys in the Boat by Daniel James Brown"
  },
  {
    "objectID": "posts/2024-02-16-weekend-reads/index.html#products",
    "href": "posts/2024-02-16-weekend-reads/index.html#products",
    "title": "Weekend Reads (2/16/24)",
    "section": "Products",
    "text": "Products\n\nMaui Nui Sugar Free Venison Jerky"
  },
  {
    "objectID": "posts/2024-02-23-weekend-reads/index.html#articles",
    "href": "posts/2024-02-23-weekend-reads/index.html#articles",
    "title": "Weekend Reads (2/23/24)",
    "section": "Articles",
    "text": "Articles\n\nSora by OpenAI\nOpenAI Forum\nCompany Fires Employees Who Don‚Äôt Embrace New AI Tools"
  },
  {
    "objectID": "posts/2024-02-23-weekend-reads/index.html#videos",
    "href": "posts/2024-02-23-weekend-reads/index.html#videos",
    "title": "Weekend Reads (2/23/24)",
    "section": "Videos",
    "text": "Videos\n\nAswath Damodaran on Prof G Markets\nWhat I Learned From 100 Days of Rejection\nCharlie Houpert on Modern Wisdom"
  },
  {
    "objectID": "posts/2024-02-23-weekend-reads/index.html#podcasts",
    "href": "posts/2024-02-23-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (2/23/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nChris Williamson on Joe Rogan\nLatest AI Developments on All In\nMorgan Housel on Modern Wisdom"
  },
  {
    "objectID": "posts/2024-02-23-weekend-reads/index.html#songs",
    "href": "posts/2024-02-23-weekend-reads/index.html#songs",
    "title": "Weekend Reads (2/23/24)",
    "section": "Songs",
    "text": "Songs\n\nSweet City Woman by Stampeders"
  },
  {
    "objectID": "posts/2024-02-23-weekend-reads/index.html#series",
    "href": "posts/2024-02-23-weekend-reads/index.html#series",
    "title": "Weekend Reads (2/23/24)",
    "section": "Series",
    "text": "Series\n\nUnbreakable Kimmy Schmidt on Netflix"
  },
  {
    "objectID": "posts/2024-03-16-weekend-reads/index.html#videos",
    "href": "posts/2024-03-16-weekend-reads/index.html#videos",
    "title": "Weekend Reads (3/16/24)",
    "section": "Videos",
    "text": "Videos\n\nThe Algebra of Happiness"
  },
  {
    "objectID": "posts/2024-03-16-weekend-reads/index.html#podcasts",
    "href": "posts/2024-03-16-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (3/16/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nChris Davis On The Knowledge Project\nKimbal Musk On Lex Friedman\nDr.¬†Cal Newport On Huberman Lab"
  },
  {
    "objectID": "posts/2024-03-16-weekend-reads/index.html#tweets",
    "href": "posts/2024-03-16-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (3/16/24)",
    "section": "Tweets",
    "text": "Tweets\n\nExcellent Product Placement\nMeet Devin, Your AI Dev Intern\nSpaceX Launch\nChatGPT Writing Prompt\nInvest For The Long Haul"
  },
  {
    "objectID": "posts/2024-03-16-weekend-reads/index.html#books",
    "href": "posts/2024-03-16-weekend-reads/index.html#books",
    "title": "Weekend Reads (3/16/24)",
    "section": "Books",
    "text": "Books\n\nThe Great Mental Models Volume 1 by Shane Parrish"
  },
  {
    "objectID": "posts/2024-03-26-time-series-first-principles-initial/index.html",
    "href": "posts/2024-03-26-time-series-first-principles-initial/index.html",
    "title": "Thoughts on First Principles in Time Series Forecasting",
    "section": "",
    "text": "I‚Äôve been doing time series forecasting with machine learning (ML) most of my career. I believe it‚Äôs still the best AI opportunity in corporate finance, even with all of the latest Generative AI developments in recent years. If you work for the CFO, chances are you often create predictions about the future. Those predictions take time and can always be more accurate. Machine learning can help in both areas. Before you build machine learning solutions in your finance org, it‚Äôs important to understand the true building blocks of making good forecasts.\nIn this post I will overview each first principle, and have follow-up posts digging deeper into each one. Let‚Äôs dive in.\n\nDomain Expertise: Knowing what factors actually influence what you are trying to forecast is more important than which ML model to train.\nGarbage In Garbage Out: Training a model on bad data leads to bad forecasts.\nThe Future Is Similar To The Past: If you expect the future to be drastically different than past data, you will have a hard time training accurate models.\n\nHigher Grain Higher Accuracy: Forecasting by country is often more accurate than forecasting by city. Forecasting by month is often more accurate than forecasting by day.\nOrder Is Important: When time is involved, how your data is ordered makes all the difference.\nThe Magic Is In The Feature Engineering: How you transform your data before model training can transform a mediocre forecast into a world class forecast.\nSimple Models Are Better Models: Like occam‚Äôs razor, the best model is often the one with the least amount of inputs.\nCapture Uncertainty: Showing the back testing results and future uncertainty of a model‚Äôs forecast builds more trust.\nModel Combinations Are King: Usually a combination of multiple models is more accurate than just one model‚Äôs prediction.\nDeep Learning Last: Deep learning isn‚Äôt as effective as more traditional ML models.\n\n\nFinal Thoughts\nThis is not an exhaustive list, but instead principles that I find particularly important when creating a time series forecast. Having a firm understanding of these principles is enough to get the ball rolling on any type of forecast you‚Äôre working on. Thankfully, the very same approach I use in my job to do forecasting is open source and freely available through my R forecasting package called finnts. Even if you‚Äôve never done data science or used R before, finnts makes it easy to get off the ground fast without shooting yourself in the foot when dealing with the above principles. Stay tuned for more posts about each principle."
  },
  {
    "objectID": "posts/2024-04-05-weekend-reads/index.html#videos",
    "href": "posts/2024-04-05-weekend-reads/index.html#videos",
    "title": "Weekend Reads (4/5/24)",
    "section": "Videos",
    "text": "Videos\n\nImproving Focus\nDopamine and Motivation\nOptimal Morning Routine\nSequoia AI Summit\nSBF, Gov Spending, and More on All In"
  },
  {
    "objectID": "posts/2024-04-05-weekend-reads/index.html#podcasts",
    "href": "posts/2024-04-05-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (4/5/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nScott Glenn on Tim Ferriss\nSeth Godin on Tim Ferriss\nDr.¬†Rhonda Patrick on The Knowledge Project"
  },
  {
    "objectID": "posts/2024-04-05-weekend-reads/index.html#tweets",
    "href": "posts/2024-04-05-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (4/5/24)",
    "section": "Tweets",
    "text": "Tweets\n\nSora Goodness\nSatya Nadella Stays Undefeated\nBezos Leadership Principles"
  },
  {
    "objectID": "posts/2024-04-05-weekend-reads/index.html#books",
    "href": "posts/2024-04-05-weekend-reads/index.html#books",
    "title": "Weekend Reads (4/5/24)",
    "section": "Books",
    "text": "Books\n\nGenAI Guidebook"
  },
  {
    "objectID": "posts/2024-04-11-time-series-past-future/index.html",
    "href": "posts/2024-04-11-time-series-past-future/index.html",
    "title": "Time Series First Principles: The Future Is Similar To The Past",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the third principle of a good time series forecast, the future is similar to the past. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nHere Comes The Sun\nFor whatever you‚Äôre trying to forecast, it will be a lot easier to do with with machine learning (ML) if the future is similar to the past. It‚Äôs as simple as that.\nWhen you open the weather app on your phone, have you ever looked at when the sun is expected to rise and set? If you‚Äôre on the new human optimization craze about getting morning sunlight, you most likely have. That forecast is down to the minute, most likely even second, and has a high degree of accuracy. Is the forecast accurate because of expert human judgement, or the type of weather related features fed into a ML model? Nope. It‚Äôs accurate because the sun has risen and set at relatively the same time, based on time of year, for millions of years. We don‚Äôt expect future sun rises and sun sets to change that much going forward, that‚Äôs why your weather app gives you an exact time for the sun rise but gives you only a percent probability of rain. Even then, that chance of rain may not even be accurate. It‚Äôs almost a joke now how many times in Seattle I‚Äôve seen a dry forecast only to step out of my house and have it immediately start raining. At least I know the exact minute when the sun will set that day.\n\n\nHandling A Changing Future\nYour business is most likely not like the sun. It‚Äôs constantly changing, reacting to market forces and industry competitors. The best way to teach a model about your expectations of the future is to give it data about the past and future.\nLet‚Äôs use an example of a monthly revenue forecast for a product. If you only use historical sales data to forecast the product, then you are making the assumption that the future of the product will be almost identical to the past, especially the most recent past. For some established products in mature industries this could be totally fine, but often this is not the case.\nOne thing to try is adding features that can explain how outside forces impact the product. For example, how much money consumers have to spend might greatly impact who buys your product. So using an economic feature like consumer sentiment can help a model adjust it‚Äôs predictions based on changes in consumer spending habits.\nWe can add features into our data in two ways. The first is to just give historical values of that feature. This will force us to only use historical lags of the feature when training a ML model, since we don‚Äôt know what the future value of that feature will be. We can take that original feature and create new features (this is called feature engineering). Ones that are a 3 month lag, 6 month lag, or 12 month lag of the original data. Often macro data like consumer sentiment can be a lagging indicator. Meaning their impact is delayed and takes a while to flow through the economy. Changes in consumer sentiment from 6 months ago can actually have a strong correlation with how customers purchase our product today.\nA second way is to use both historical values and future values. We could use a future forecast of consumer sentiment in our model, in addition to using the historical data. That way a model can learn from any lagged relationships as well as understand how changes in consumer sentiment impact our product in real time. These future values can either come from an expert forecast (like from famous economists) or created by your own ML solution.\n\n\nThe Future Must Always Learn From The Past\nYou might have a ton of ideas for new kinds of future information your can encode as features to train a model. In order to use this data we need to make sure there are historical examples for a model to learn from. The upcoming 2024 presidential election in the US could have a large impact on your business, which will impact your future forecast for the rest of 2024. We know exactly when the election is going to happen, so it‚Äôs easy to give that information into a ML model to learn from.\nThe catch is we need to make sure that we show examples from the past to allow the model to learn how previous elections impact our business and how the model should handle similar events in the future. If we only have product sales data from the last three years, then we cannot feed it future election data because we don‚Äôt have the data from the 2020 or 2016 US presidential elections.\nIf we know something is going to happen in the future, but we can‚Äôt quantify it with historical data for a model, then we need to go old school. Instead we need to use our domain expertise about the business to take the ML output, without knowledge of the future event, and make a manual adjustment to the forecast based on the expected impact of the future. For the election example, maybe your product sales will grow as we get closer to the election, so if you don‚Äôt have enough historical data for a model to learn about the election‚Äôs impact you can make a manual adjustment to the ML forecast based on your assumption about the election‚Äôs impact.\nThis kind of hybrid approach, ML first with a light human touch second, can create a powerful combination. A ML model can do 80-90% of the initial work and a human can make the final manual adjustments based on their domain expertise. This allows a human to add more insight into a forecast that is not easily quantifiable for a ML model to learn from.\n\n\nNew Time Series\nA new product at your company might be exciting, but is harder to forecast accurately with ML models. The lack of historical data will make it hard for any new ML model to learn from. Initial trends and seasonality may not always carry into the future. For example there might be a big spike in initial sales around release but then taper off over time. The new product may not even be on sale yet, so you are now tasked with forecasting something with zero historical data.\nIf the time series in question has some historical data, ideally more than one year of historical observations, a good way to deal with it is to train a ML model with the new time series alongside similar existing products with a lot of historical data. This is sometimes called a ‚Äúglobal model‚Äù, where a model learns from multiple time series instead of one. Training on one specific time series is sometimes called a ‚Äúlocal model‚Äù. Training a global model allows the ML model to learn general trends and seasonality patters across similar time series and apply it to the newer time series. This can work well if the other time series are similar to one another.\nIf the product you want to forecast has no historical data, then you are in a tough boat. Traditional time series methods cannot help you, since they all rely on quality historical data. What you can do is take a more traditional machine learning regression approach. This involves taking all historical products that have launched over time and training a model to understand the initial demand of a new product and how it either grows or shrinks over time. For example with iPhone sales, you can train a model on the initial sales of each iPhone model from V1 to V14, then use that model to predict the kind of demand the latest V15 iPhone might have. This type of approach would need a more detailed post to explain fully, but hopefully you get the broader picture.\nTo learn more about forecasting new time series, check out the forecasting bible written by our forecasting godfather Rob Hyndman. The chapter on judgemental forecasts goes deep into forecasting new products and discusses other approaches you can take.\n\n\nReversal\nWhen using future values of a feature, there is a risk of compounding errors. Let‚Äôs go back to the consumer sentiment example. If you create your own expectation of future consumer sentiment, or use an economist‚Äôs prediction, there is a good chance the forecast will be wrong. If the forecast about consumer sentiment is wrong, and that forecast is fed into a model to predict your product‚Äôs sales, then your sales forecast will be even more wrong. The errors compound. Add in other features and you can see how the house of cards can tumble pretty fast. Always be weary about using future values of features where you don‚Äôt have 100% confidence in their future value. For example using future holiday features are great because they will always happen on a specific day with 100% certainty, but trying to tell a model where inflation is headed in the future can get you in trouble.\nHaving a human make manual adjustments after the initial ML forecast can add unneeded human bias to the final forecast. This bias can sometimes be wrong and hurt the accuracy of the forecast. It‚Äôs good practice to capture these adjustments and always report on the forecast accuracy of the pure ML model and the accuracy for the model + human adjustments. That way you can track how helpful the manual adjustments are, and remember why they were made in the first place.\n\n\nFinal Thoughts\nWhen embarking on the journey of time series forecasting, remember it‚Äôs more art than exact science‚Äîakin to predicting rain in Seattle. The key takeaway? Use the past as a guide but sprinkle in educated guesses about the future with caution. Whether you‚Äôre launching new products or navigating established markets, blending machine learning with a dash of human intuition can create robust forecasts. May your forecasts be as reliable as the sunrise, with just enough flexibility to handle an unexpected downpour."
  },
  {
    "objectID": "posts/2024-04-18-time-series-grain/index.html",
    "href": "posts/2024-04-18-time-series-grain/index.html",
    "title": "Time Series First Principles: Higher Grain Higher Accuracy",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the fourth principle of a good time series forecast, the higher the grain the higher the accuracy. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nClear Skies\nWhen planes take off from the ground they climb high into the sky. During that 5-10 minute period passengers have to stay seated with their seatbelt fastened. It‚Äôs only after the plane reaches 10,000 feet people can start to get up and move around the plane. Eventually the plane can reach an altitude of 40,000 feet. To compare, the peak of Mount Everest is 29,000 feet off the ground. Planes go up that high because it‚Äôs easier to fly the plane and more efficient. If planes flew a few feet off the ground it would be a lot bumpier ride, having to deal with changing weather and turbulence.\nForecasting is similar to flying a plane. Training a machine learning (ML) model at a higher grain of data is akin to a plane climbing in altitude. There is less turbulence (noise) in the data and your forecast has a better chance of being more accurate. You can either climb in altitude at the individual time series grain, or by the date grain. Let‚Äôs discuss each of them along with other methods.\n\n\nTime Series Grain\nThink of a higher time series grain as an aggregation of your original data. For example you might have product sales across a bunch of cities, where each city is a time series. Individual cities might have hard to model trend and seasonality, but when combined at a total country-level, it can be easier to model. Take the example charts below. Each city might be noisy but climbing in altitude up to the county level makes it easier to spot trends and seasonality.\n \n\n\nDate Grain\nIn a perfect world we would be able to forecast our businesses down to the day, or even minute, across the next 10 years. This is sadly not the case. The more granular you try to forecast at the date grain, the noisier the data is going to be, and the harder it will be to create accurate forecasts. Unless there is an absolute need to forecast at a certain level, I almost always recommend a higher date grain. Take the example charts below. See how aggregating daily data to a monthly date grain level makes it easier to spot trends and seasonality.\n \nIn the finance org, the most common date grain to forecast at is month. This gives a healthy balance of being able to forecast long periods of time while also being able to update your forecast with new historical data every few weeks.\nHere is another important point to call out. The longer your forecast horizon, the higher the date grain you should forecast at. Here are some recommendations based on your forecast horizon (how many periods you want to forecast). For example, if you‚Äôre trying to forecast out the next 6 months at the daily grain, you might get better results if you aggregate up to the monthly grain and forecast by month instead.\n\nDaily Grain: 1-90 day forecast horizon\nWeekly Grain: 1-12 week forecast horizon\nMonthly Grain: 1-18 month forecast horizon\nQuarterly Grain: 1-8 quarter forecast horizon\nYearly Grain: more than 1 year forecast horizon\n\n\n\nHierarchical Forecasting\nA potential ‚Äúbest of both worlds‚Äù solution to the data grain issue is to use a hierarchical forecast. This is where you can train models at different grains of the data, then use a statistical process to reconcile each forecast together so they are in sync. Our forecasting godfather, Rob Hyndman, has done a lot of great work in this space. Here is a chapter from his book on hierarchical forecasting.\nLet‚Äôs go back to our time series grain example. Using a hierarchical forecast you could train models and create forecasts for each city, then do the same at the total country-level, then finally do the same at a total world wide level across all countries. This is a standard hierarchical approach shown in the chart below. This hierarchical process blends a ‚Äúbottoms up‚Äù forecast of creating predictions at the lowest level by city, with a ‚Äútops down‚Äù forecast of creating predictions at the highest global level. A statistical process is then used to make the ‚Äútops down‚Äù forecast equal the ‚Äúbottoms up‚Äù forecast, optimizing for accuracy at all levels of the hierarchy.\n\nThe same idea can be applied at the date grain too. Where you can forecast at the daily level, weekly level, and monthly level. Then use a reconciliation process to get the final forecast at the daily level that is also accurate when summing up by month. This can work well if a monthly forecast is more accurate, but the final forecast needs to be at a daily level.\n\n\nAllocations\nAnother option is to take a forecast at a higher grain and allocate it down to a lower grain using simple allocation logic. This process can replace the more complicated hierarchical forecasting discussed earlier. Simple allocations can be done in two ways.\nThe first is to take historical values and create a percent split to apply to the final forecast. For example we can create a forecast at the country-level, then split that out by city. The split percent by city (allocation percent) can be calculated based on how much each city was the percent of total country over the last few years. This can be broken down by period. So you can get a specific percent split for each month on average in the past. This approach helps maintain historical seasonality across each time series (each city). See the charts below for an example of using two historical years of monthly data to create the final allocation percentages.\n\nHistorical splits by city\n\n\nMonth\nCity A %\nCity B %\nCity C %\n\n\n\n\nJan 2021\n30.87%\n30.08%\n39.05%\n\n\nFeb 2021\n28.11%\n23.90%\n47.99%\n\n\nMar 2021\n23.77%\n47.43%\n28.81%\n\n\nApr 2021\n31.26%\n18.42%\n50.32%\n\n\nMay 2021\n39.50%\n34.08%\n26.42%\n\n\nJun 2021\n50.58%\n30.98%\n18.44%\n\n\nJan 2022\n36.33%\n31.79%\n31.88%\n\n\nFeb 2022\n19.79%\n40.74%\n39.47%\n\n\nMar 2022\n19.95%\n28.99%\n51.06%\n\n\nApr 2022\n20.37%\n26.76%\n52.87%\n\n\nMay 2022\n41.37%\n41.53%\n17.10%\n\n\nJun 2022\n43.86%\n41.10%\n15.04%\n\n\n\n\nAverage of city split by month\n\n\nMonth\nCity A %\nCity B %\nCity C %\n\n\n\n\nJan\n33.60%\n30.93%\n35.46%\n\n\nFeb\n23.95%\n32.32%\n43.73%\n\n\nMar\n21.86%\n38.21%\n39.93%\n\n\nApr\n25.82%\n22.59%\n51.60%\n\n\nMay\n40.43%\n37.81%\n21.76%\n\n\nJun\n47.22%\n36.04%\n16.74%\n\n\n\n\nFinal forecast using the city splits\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\nCountry Forecast\nCity A %\nCity B %\nCity C %\nCity A Forecast\nCity B Forecast\nCity C Forecast\n\n\n\n\nJan 2023\n15000\n33.60\n30.93\n35.46\n5040\n4639.5\n5319\n\n\nFeb 2023\n15200\n23.95\n32.32\n43.73\n3640.4\n4912.64\n6646.96\n\n\nMar 2023\n15400\n21.86\n38.21\n39.93\n3366.44\n5884.34\n6149.22\n\n\nApr 2023\n15600\n25.82\n22.59\n51.60\n4027.92\n3524.04\n8049.6\n\n\nMay 2023\n15800\n40.43\n37.81\n21.76\n6387.94\n5973.98\n3438.08\n\n\nJun 2023\n16000\n47.22\n36.04\n16.74\n7555.2\n5766.4\n2678.4\n\n\n\nThe second approach is to use a future forecast to create the allocation splits. For example we can create future forecasts at the country-level and also at the city-level. Then we can create the split percent for each city by taking the city forecast and summing it up to the country-level, then taking the percent split for each city. These splits can then be applied to the final country-level forecast to get the final forecast by city. This approach uses the more robust country-level forecast, while still trying to capture future changing trends and seasonality by city.\n\nInitial forecast, where country and each city are forecasted separately\n\n\n\n\n\n\n\n\n\nMonth\nCountry Forecast\nCity A Forecast\nCity B Forecast\nCity C Forecast\n\n\n\n\nJan 2023\n10,000\n4,000\n3,500\n2,000\n\n\nFeb 2023\n10,500\n4,200\n3,000\n3,300\n\n\nMar 2023\n11,000\n4,500\n3,200\n3,300\n\n\nApr 2023\n11,500\n4,800\n3,400\n3,300\n\n\nMay 2023\n12,000\n5,000\n3,500\n3,500\n\n\nJun 2023\n12,500\n5,200\n3,800\n3,500\n\n\n\n\nCalculating the percent splits by city\n\n\nMonth\nCity A %\nCity B %\nCity C %\n\n\n\n\nJan 2023\n42.11%\n36.84%\n21.05%\n\n\nFeb 2023\n40.00%\n28.57%\n31.43%\n\n\nMar 2023\n40.91%\n29.09%\n30.00%\n\n\nApr 2023\n41.74%\n29.57%\n28.70%\n\n\nMay 2023\n41.67%\n29.17%\n29.17%\n\n\nJun 2023\n41.60%\n30.40%\n28.00%\n\n\n\n\nFinal forecast after applying the city splits to the country-level forecast\n\n\n\n\n\n\n\n\nMonth\nCity A Final Forecast\nCity B Final Forecast\nCity C Final Forecast\n\n\n\n\nJan 2023\n4,211\n3,684\n2,105\n\n\nFeb 2023\n4,200\n3,000\n3,300\n\n\nMar 2023\n4,500\n3,200\n3,300\n\n\nApr 2023\n4,800\n3,400\n3,300\n\n\nMay 2023\n5,000\n3,500\n3,500\n\n\nJun 2023\n5,200\n3,800\n3,500\n\n\n\n\n\nReversal\nA more granular forecast can sometimes be more accurate, especially if the more detailed grain uncovers more stable trends and seasonality that can be modeled. Take for example a product whose sales are impacted by Chinese New Year. That holiday doesn‚Äôt happen on the same day every year, and it can even happen in different months. Sometimes in January, and sometimes in February. Since it happens over multiple days the split between the two months can change drastically from year to year. Creating a forecast at the daily level, adding information around when Chinese New Year is happening, could result in a more accurate forecast. You could also take the approach of a monthly forecast, and have a numeric feature that lists how many days of Chinese New Year falls within each month.\nIf your initial data at the higher grain is noisy or has low forecast accuracy, consider asking the domain expert if there could be more insightful trends and seasonality at a lower grain.\n\n\nfinnts\nHierarchical forecasting is a tricky business, thankfully my open-source package finnts can automatically do hierarchical forecasting. The package can even use external regressors (features) in the hierarchical approach! Today finnts supports hierarchical forecasting at the time series grain. Hopefully one day we will implement hierarchical forecasting at the date grain, stay tuned. This is the same package I use internally at my job, allowing my company to replace hundreds of billions of manual forecasts with machine learning. Check out the package and see for yourself.\n\n\nFinal Thoughts\nJust as pilots navigate to higher altitudes to find smoother skies and better efficiency, so too must we elevate our approach to data granularity in forecasting when needed. By stepping back from the minutiae of daily or city-level data and ascending to monthly or country-level aggregations, we enable our models to capture more coherent patterns and deliver forecasts with improved precision. This strategic shift‚Äîfrom a granular view to a broader perspective‚Äîis not just about avoiding turbulence; it‚Äôs about leveraging stability to enhance predictability.\nHowever, the real magic often lies in blending these approaches through hierarchical forecasting. This method combines the detailed insights available at lower levels with the clarity and simplicity of higher-level forecasts, ensuring both depth and breadth in our predictive capabilities. As we continue to refine our techniques and tools, like the finnts package, we are paving the way for a future where complex, multi-tiered forecasting is as streamlined as a flight cruising at 40,000 feet.\nIn your journey through data, remember that the right altitude can make all the difference. Rising above the noise can provide not just clearer views, but also far-reaching insights. So, buckle up‚Äîwe‚Äôre about to take forecasting to new heights."
  },
  {
    "objectID": "posts/2024-04-23-time-series-order/index.html",
    "href": "posts/2024-04-23-time-series-order/index.html",
    "title": "Time Series First Principles: Order Is Important",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the fifth principle of a good time series forecast, order is important. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nBaking Cakes Over Making Smoothies\nMachine learning (ML) is a lot like cooking. You have various ingredients and can combine them together in clever ways to make for a tasty dish. Most machine learning approaches like classification (predicting an outcome) and regression (predicting a number) can follow a similar process to making a smoothie. We can take some data (fruits and veggies) and blend it all together inside of our model blender.\nTime series forecasting is a whole other beast. It still technically falls under the regression family tree but has to be handled very differently. Forecasting is more like baking a cake, where the order in which you do things is very important. For example, you cannot switch when you add the eggs and when you add the frosting. If you do you will certainly not be invited back to your nephew‚Äôs birthday party next year. In order to bake something tasty please follow the below guidance.\n\n\nTime Series Training\nTraining any sort of machine learning model often requires two separate historical data sets. One that is used to train the initial model, then another that is set aside to create predictions based on the initial model. We can then see how accurate the predictions were on the test data set. This ensures that our new ML model can generalize well to new and unseen data, making sure our model doesn‚Äôt overfit to the training data.\nCommon ML approaches like classification and regression don‚Äôt need a lot of sophistication when splitting up the historical data between a training set and a testing set. Often it will be split randomly. This is similar to making a smoothie. You can randomly throw in bananas, apples, spinach, and blueberries. All without having to think about the order of when you do it.\nTake the below housing data. This is a traditional regression problem. Let‚Äôs use the total square feet and number of bedrooms to predict how much the house will cost. We can randomly split 80% of the data to train the model, then hold out 20% of the data to test how accurate the model is. Randomly splitting the data ensures we get a healthy mix of different data in each split.\n\nExample fake housing data for a regression model\n\n\nSquare_Feet\nBedrooms\nTotal_Cost\nSplit\n\n\n\n\n3774\n2\n822732\nTraining\n\n\n1460\n1\n245280\nTraining\n\n\n1894\n4\n602292\nTraining\n\n\n1730\n4\n550140\nTraining\n\n\n1695\n4\n539010\nTesting\n\n\n3692\n5\n1358656\nTraining\n\n\n2238\n1\n375984\nTraining\n\n\n2769\n5\n1018992\nTraining\n\n\n1066\n5\n392288\nTesting\n\n\n1838\n1\n308784\nTraining\n\n\n\nA time series has a built in order to it. It‚Äôs said right there in the name, time. Ignoring the order based on time can have disastrous consequences, resulting in your final future forecast not being accurate. Just like baking a cake, we need to make sure how we train a model is done in the right order. When splitting a historical time series into a training set and a testing set, splitting not at random but based on time is the proper way to go. Using the oldest data as the training set and the newest data as the testing set makes sure we respect the order of our data based on time. The example table below is a made up time series of the price of one specific house. In reality we would need a lot more data to train a good time series model but just be cool for a minute and go with me on this one. The split column now has the test data set at the very end instead of randomly split across time. We can now use features like interest rates and gdp growth to help us forecast the price of this house over time. The first 9 months of data will train the model, and the final 3 months will be used to test the model‚Äôs accuracy.\n\nExample fake time series for the price of a specific house\n\n\n\n\n\n\n\n\n\nDate\nInterest_Rate\nGDP_Growth\nTotal_Cost\nSplit\n\n\n\n\nJanuary 2002\n3.43635\n1.58111\n315052\nTraining\n\n\nFebruary 2002\n4.87679\n0.03085\n314723\nTraining\n\n\nMarch 2002\n4.32998\n-0.04544\n312854\nTraining\n\n\nApril 2002\n3.99665\n-0.04149\n311865\nTraining\n\n\nMay 2002\n2.89005\n0.26061\n309452\nTraining\n\n\nJune 2002\n2.88999\n0.81189\n311106\nTraining\n\n\nJuly 2002\n2.64521\n0.57986\n309675\nTraining\n\n\nAugust 2002\n4.66544\n0.22807\n314681\nTraining\n\n\nSeptember 2002\n4.00279\n1.02963\n315097\nTraining\n\n\nOctober 2002\n4.27018\n-0.15127\n312357\nTesting\n\n\nNovember 2002\n2.55146\n0.23036\n308345\nTesting\n\n\nDecember 2002\n4.92477\n0.41591\n316022\nTesting\n\n\n\n\n\nData Leakage\nWhenever time is involved in machine learning, the probability of shooting yourself in the foot rises. This has to do with the concept of data leakage. Data leakage occurs when information from outside the training dataset is used to create the model, leading it to make overly optimistic predictions. It can also happen when we train with data that may not be available in the future when we need to create new predictions.\nIn time series forecasting we have already discussed one component of data leakage, related to splitting the data correctly based on time. Take the below table, instead of splitting properly by time the data is now split randomly. Our model can now ‚Äúsee ahead in time‚Äù when training, and in effect cheat when being evaluated on the testing splits. For example, for the test observation in July 2002 the model can learn from data on either side of that month. Figuring out previous and future trends and seasonality. This makes it easy to predict what the housing cost in July should be, since it has information before and after that month. With this approach our test accuracy will be a lot better than in the previous example where the splits are based on time. Future forecast performance will suffer though, since we have now trained and chosen a model that may only be good at figuring out how to extrapolate between two points, instead of trying to create predictions on unseen data in the future.\n\nIncorrect train and test splits\n\n\n\n\n\n\n\n\n\nDate\nInterest_Rate\nGDP_Growth\nTotal_Cost\nSplit\n\n\n\n\nJanuary 2002\n3.43635\n1.58111\n315052\nTraining\n\n\nFebruary 2002\n4.87679\n0.03085\n314723\nTesting\n\n\nMarch 2002\n4.32998\n-0.04544\n312854\nTraining\n\n\nApril 2002\n3.99665\n-0.04149\n311865\nTraining\n\n\nMay 2002\n2.89005\n0.26061\n309452\nTraining\n\n\nJune 2002\n2.88999\n0.81189\n311106\nTraining\n\n\nJuly 2002\n2.64521\n0.57986\n309675\nTesting\n\n\nAugust 2002\n4.66544\n0.22807\n314681\nTraining\n\n\nSeptember 2002\n4.00279\n1.02963\n315097\nTraining\n\n\nOctober 2002\n4.27018\n-0.15127\n312357\nTraining\n\n\nNovember 2002\n2.55146\n0.23036\n308345\nTraining\n\n\nDecember 2002\n4.92477\n0.41591\n316022\nTesting\n\n\n\nOk, so we know not to split data randomly when training. Another thing to watch out for is how features are used in the model. In our time series housing example, we can use date information (month, quarter, etc) along with our macro features like interest rates and GDP growth. Let‚Äôs say we follow the right approach, split the data based on time, and see that we get good results on the test data. We can now take our model into production and try to create a forecast for the future. But wait, what do we do with the future feature values of interest rate and GDP growth? This is another potential data leakage issue, where data used to train the model is not available to create new predictions in the future. You might be thinking, no problem we can just create a forecast of future interest rates and gdp growth right? Wrong. If you can produce accurate interest rate and GDP growth forecasts, then you shouldn‚Äôt be reading this post. You should instead be sitting on your own private island, watching the return on your flagship hedge fund skyrocket. See where I‚Äôm going here? If you cannot perfectly predict future values of the feature you want to use, then you should consider not using that feature. You might be able to know with 100% certainty when a holiday or special event will take place, but not even expert economists can perfectly predict interest rate fluctuations. Instead you can look at using feature lags. Where instead of using real time interest rates or GDP growth you can instead use lags of them. For example, using a 3 or 12 month lag of each feature. Using lags in this example can help prevent compounding errors in your forecast.\n\n\nfinnts\nLooking for a way to to never worry about the order of your time series data again? Have no fear because finnts is here! Ok enough with the used car salesman talk. The finnts package is something myself and other outstanding team members have built to automate all of the tedious aspects related to time series forecasting. The package can automatically handle the proper splits of your data and has build in data leakage prevention. Check out the package to learn for yourself how easy forecasting can be.\n\n\nFinal Thoughts\nRemember, order is important in forecasting. Make sure you don‚Äôt mix up your data when training models, and keep a look out for data leakage. Do this right and you just might get invited back to your nephew‚Äôs birthday party next year."
  },
  {
    "objectID": "posts/2024-05-03-time-series-simple-models/index.html",
    "href": "posts/2024-05-03-time-series-simple-models/index.html",
    "title": "Time Series First Principles: Simple Models Are Better Models",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the seventh principle of a good time series forecast, simple models are better models. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nOccam‚Äôs ML Model Razor\nWilliam of Ockham was a 14th-century English Franciscan friar, philosopher, and theologian. In his work he preached that for most things in life the simplest explanation is the correct one. I‚Äôve learned this inadvertently in my life many times. For example, when I was studying for the ACT in high school a teacher told me that on the english questions it‚Äôs usually the shortest answer that is often correct. You could get a decent score just by following this one rule, even if you couldn‚Äôt read or speak english. This one tip saved my ass more than I‚Äôd like to admit, and I could read and speak english. Or so I thought.\nOften in life, just like the ACT english section, it‚Äôs usually the simplest approaches that provide the best results. You can hire a fitness coach and buy all the supplements in the world but you‚Äôll probably get similar results following a handful of simple exercise and eating tips. The same applies in the world of machine learning. The more complexity you add to your data and models, the less likely they are going to be useful in the end. Let‚Äôs walk through how simplicity helps in all aspects of machine learning, from the data you use all the way down to models you train.\n\n\nMore Features, More Problems\nIn the world of time series forecasting, there are so many ways we can do feature engineering. Learn more about feature engineering in a previous post. A dataset containing two columns, a date and value column, can be transformed into 100+ new features. This can easily get out of hand once we add external regressors (outside variables like consumer sentiment or inflation data) and create new features from them.\nEach feature you add to a dataset hurts your model in multiple ways.\n\nTrain Time: It can slow down model training, meaning it will take longer to train the model. This may not seem like a big deal with small datasets but once you start having tens of thousands of rows in a dataset, adding a new feature can really slow things down.\nOverfitting: Adding more features can lead to overfitting, meaning your model might be very accurate on the data it was trained on but cannot generalize well to unseen data in the future. Your model will learn from the noise in the data instead of the signal.\nInterpretability: Adding more features makes it harder to explain the model‚Äôs predictions. If you can‚Äôt explain your forecast to non-technical business partners, then the forecast may not be used by anyone. I‚Äôve seen this countless times in my work. An accurate model doesn‚Äôt help anyone if the end user ultimately wants to know how the prediction was created. More on that in this post.\n\n\n\nFeature Selection\nOne way to simplify your data before model fitting is to implement a feature selection process. It‚Äôs called selection but it‚Äôs more like removal, where we drop any features that do not contribute to a model that can generalize well to new data. Here are a few techniques for feature selection.\n\nDomain Expertise: Remove features that don‚Äôt make sense to you as a human. For example, the annual rain fall in Iceland might be perfectly correlated to Coca Cola sales in South America, but it doesn‚Äôt pass our smell test of being a factor that impacts the business. When in doubt take it out.\nCorrelation: If a feature has a strong correlation to the target variable (what we want to forecast) then we keep it in, but only after it passes our domain knowledge smell test.\nModel Specific: Some models, like certain flavors of linear regression, have built in feature selection or feature importance. We can use that info to remove features and can then retrain on any kind of ML model.\n\nThere are many other methods for feature selection, but are out of the scope of this post. The ones called out above are a good starting point.\n\n\nSimple Models\nSimplifying our data is helpful, but sometimes simplifying our models is even better. When starting a new forecast project, you might feel tempted to go out and build an advanced deep learning model, using all of the latest bells and whistles. That model may show promising results, but often a simpler model like linear regression can get the same or even better results. Models like linear regression are faster to train and have better model interpretability.\n\nWe can even go one level deeper and not use any features at all. Univariate statistical models like ARIMA or exponential smoothing are classic time series forecasting models that only need one column of data, the historical values of your target variable. That‚Äôs what makes them univariate (one variable). They have built in feature engineering under the hood that allows them to learn from historical trends and seasonality in the data, so no additional work is needed to create features. Often in time series forecasting competitions a large team of deep learning researchers can just barely beat a single person team who uses simple models like ARIMA or random forest models. More on that in a future post.\n\n\nfinnts\nMy forecasting package, finnts, has built in feature selection and other techniques to ensure simple models are built in ways that produce accurate forecasts. Check out the package and see for yourself.\n\n\nFinal Thoughts\nUltimately, the goal of any forecasting model is to provide clear, accurate, and quick results. Simpler models often meet these criteria better than complex ones because they‚Äôre easier to understand, faster to run, and just as accurate. By focusing on simplicity and minimizing inputs, we ensure that our forecasts are not only effective but also user-friendly. This approach doesn‚Äôt just save time; it makes the insights gained from the data accessible to everyone involved in the decision-making process. Simplicity, therefore, isn‚Äôt just a principle; it‚Äôs a practical strategy for better forecasting."
  },
  {
    "objectID": "posts/2024-05-13-finnts-multistep-horizon/index.html",
    "href": "posts/2024-05-13-finnts-multistep-horizon/index.html",
    "title": "Multistep Horizon Forecasting With finnts",
    "section": "",
    "text": "TL;DR\nI‚Äôm excited to announce that we just released a new feature in our machine learning forecast package, finnts, centered around multistep horizon forecasting. It‚Äôs a mouthful to say but at a high level it helps improve forecast accuracy by optimizing models to be accurate at each period of a forecast horizon. For example, a 3 month forecast would then be optimized for the forecast in each month (period) of the future forecast.\nLet‚Äôs dive in to how multivariate modeling used to work in the package and how multistep horizon forecasting can help.\n\n\nHow It Used to Work\nLet‚Äôs use an example of a monthly revenue forecast for our business‚Äôs main product. In practice we would want more than a year of data but let‚Äôs just keep it simple today.\n\n\n\nDate\nRevenue\n\n\n\n\n2023-01-01\n120\n\n\n2023-02-01\n135\n\n\n2023-03-01\n140\n\n\n2023-04-01\n145\n\n\n2023-05-01\n150\n\n\n2023-06-01\n155\n\n\n2023-07-01\n160\n\n\n2023-08-01\n165\n\n\n2023-09-01\n170\n\n\n2023-10-01\n175\n\n\n2023-11-01\n180\n\n\n2023-12-01\n185\n\n\n\nIf we wanted to forecast the next 3 months of revenue using multivariate machine learning models we would have to do some feature engineering to get our data in good shape. This involves creating lags on our target variable. Let‚Äôs try create some lags on this data.\n\n\n\nDate\nRevenue\nRevenue_Lag1\nRevenue_Lag2\nRevenue_Lag3\n\n\n\n\n2023-01-01\n120\n\n\n\n\n\n2023-02-01\n135\n120\n\n\n\n\n2023-03-01\n140\n135\n120\n\n\n\n2023-04-01\n145\n140\n135\n120\n\n\n2023-05-01\n150\n145\n140\n135\n\n\n2023-06-01\n155\n150\n145\n140\n\n\n2023-07-01\n160\n155\n150\n145\n\n\n2023-08-01\n165\n160\n155\n150\n\n\n2023-09-01\n170\n165\n160\n155\n\n\n2023-10-01\n175\n170\n165\n160\n\n\n2023-11-01\n180\n175\n170\n165\n\n\n2023-12-01\n185\n180\n175\n170\n\n\n2024-01-01\n???\n185\n180\n175\n\n\n2024-02-01\n???\n???\n185\n180\n\n\n2024-03-01\n???\n???\n???\n185\n\n\n\nWe added some rows onto the bottom of the data, to allow us to forecast out the next 3 months after our historical data ends. That‚Äôs what we have question marks ‚Äú???‚Äù for those values. We also added lags for a 1 month, 2 month, and 3 month lag. But hey, looks like we have a problem. We have more question marks for a few future months for lag 1 and lag 2. If we wanted to forecast the next three months we wouldn‚Äôt be able to use those lags, since once we get out further in the forecast horizon we start to have missing lag data.\nThis means that the smallest lag we could use would always be equal to or greater than the forecast horizon. Since our forecast horizon is 3 than the smallest lag we could use to train a model on would be lag 3. This approach can yield good results, but it removes a lot of potential signal in the data. Revenue next month is most likely impacted by how revenue grew in the current month, but if our forecast horizon is long a lot of this insight has to get thrown away before we can train models. Imagine a forecast horizon of 12. For a monthly forecast this limits our lags to 12 months or more, which is a really bummer since our business can drastically change within 3-6 months, and not using that information in our model can hurt forecast accuracy.\n\n\nHow Multistep Horizon Forecasting Works\nMultistep horizon helps fix this issue that allows us to use smaller lags while still being able to have long forecast horizons. In our 3 month forecast horizon example, we can keep the lag 1 and lag 2 features, but how the model gets trained will be different.\nIn the non-multistep horizon approach, a specific model is trained once on the data using lags that are equal or greater than the forecast horizon. When we run a multistep horizon approach, we can actually train multiple sub models under the hood of a specific model. In our 3 month forecast horizon, here‚Äôs how one model like linear regression will be trained.\n\nFor the first month in the forecast horizon, we can use all available lags. Lag 1, lag 2, and lag 3 of revenue will all be used to predict the first month.\nIn the second month of the forecast horizon, we will use lag 2 and 3 to predict the second month.\nIn the third month of the forecast horizon, we will use lag 3 to predict the third month.\n\nAre you starting to get the hang of it? With multistep horizon forecasting we can still have one model that under the hood has multiple sub models that are each optimized on forecasting out a specific part of our forecast horizon. This allows us to have greater accuracy in the first few periods of our forecast horizon. In a non-mulitstep horizon approach, we are always optimizing for the last period in a forecast horizon. If the forecast horizon is 12 months, the way we do the feature engineering and train models is optimized for forecasting out the 12th month. When running a multistep horizon approach, we instead optimize for every period of the forecast horizon.\nThis kind of approach is so crucial to forecasters in the corporate finance space. Often these financial analysts are tasked with always forecasting out the rest of the entire fiscal year, even though they might only care about the next 3 months, since they are most likely going to be re-creating a new forecast in the following quarter. Multistep horizon forecasting allows these analysts to still forecast out long forecast horizons like 9 or 12 months, while still being able to optimize for the next 1-3 months. How cool is that!\n\n\nReversal\nIf each specific model can have 2-5 sub models under the hood, the amount of time needed to train these models can multiply by the same amount. Make sure to keep that in mind if run time is a big factor in your forecasting process.\nA multistep horizon forecast may not result in a more accurate forecast for smaller forecast horizons. Some time series may have a strong relationship with a 12 month lag, but less with a 1 month or 2 month lag. This means there is strong yearly seasonality in the data. If there is not a strong relationship with 1 month or 2 month lag, then having multiple sub models optimize for each future month in a multistep horizon approach may not result in more accurate forecasts. Consider doing some exploratory data analysis to see what kind of relationships there are with historical lags of your target variable and other features.\n\n\nFinal Thoughts\nThe new multistep horizon forecasting approach in finnts allows users to create even more accurate forecasts, regardless of their forecast horizon length. If you‚Äôd like to learn more check out the official finnts documentation to see how you can use the newest multistep horizon feature!"
  },
  {
    "objectID": "posts/2024-05-31-time-series-deep-learning/index.html",
    "href": "posts/2024-05-31-time-series-deep-learning/index.html",
    "title": "Time Series First Principles: Deep Learning Last",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the tenth and final principle of a good time series forecast, deep learning last. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nThe Shiny New Thing\nDeep learning is the latest frontier in the field of machine learning. It‚Äôs a subset of machine learning that uses neural networks with many layers (hence ‚Äúdeep‚Äù) to model complex patterns in data. These neural networks are built to resemble how human brains work. There are a lot of different types of deep learning models. Even the latest large language models from OpenAI are using deep learning techniques.\nSince deep learning is getting all the hype nowadays, it can be tempting to go straight to training deep learning models when starting a new forecasting project. This is a bad idea. While deep learning can be very effective, there are many reasons I‚Äôll call out in this post that make deep learning hard to use for forecasting projects. You can still use a deep learning model in your forecast, but I recommend exhausting all other avenues before trying deep learning. Let‚Äôs dive into why deep learning should be tried last.\n\n\nReasons To Use Deep Learning Last\n\nLack of Quality Data\nDeep learning can work well if you have thousands, or better yet millions, of observations in your historical data. In my job we might be trying to forecast a monthly time series for a single product, but only have the last three years of historical data. That‚Äôs 36 data points. This lack of data is a common problem at my company, where new products are released constantly (meaning they have limited data) and our business shifts so often that even historical years from six years ago may not be relevant to where our business is headed. If you don‚Äôt have tons of historical data, it becomes very hard to train an accurate deep learning model.\n\n\nExpensive Hardware\nDeep learning requires millions of matrix algebra calculations. Think of it as multiplying two sets of tables together. Regular computers have CPUs (central processing unit), which are designed for sequential processing. Even if you have 10+ CPUs on a computer, it will take a while to crank through the millions of matrix operations needed to train a deep learning model. GPUs on the other hand, are specialized to have thousands of cores and parallelize matrix operations effectively. They were initially built for video game graphics, hence the name graphical processing unit, but in recent years have stumbled across a new use case in training deep learning models. This is why Nvidia is the third most valuable company at the time of this writing, since they are the leading manufacturer of GPUs. These GPUs are hard to build, making them expensive to buy or rent from a cloud provider. Because they are expensive to use, they make it harder for anyone to start using them. With non-deep learning models you can start training them on your local computer, but to train a deep learning model you either have to camp out at Best Buy to purchase a Nvidia H100 or jump through a lot of hoops with a cloud provided to rent one by the minute. The juice may not be worth the squeeze.\n\n\nBigger Black Box\nDeep learning models are often harder to interpret than other machine learning models. This is due to them having up to billions of parameters (model inputs) that are abstracted between multiple layers. This means one layer of parameters feeds into another layer of parameters. There are ways to interpret the inner workings of these models, but they are often just an educated guess. Can anyone explain how a deep learning model like GPT-4 came up with its answer? Not likely.\n\n\n\nWhat to Use Instead\nMaybe I‚Äôve convinced you to not chase after the shiny thing and try something non-deep learning first. What model should I use instead? Here is what to try first. Once you have tried these models and evaluated their performance, you can then see if the juice is worth the squeeze with deep learning.\n\nUnivariate Models\nThese are models that only need one variable, historical values of what you‚Äôre trying to forecast. Univariate models are more statistics than machine learning, and are custom built for time series. They train very fast and are tuned for each specific time series in your data. One weakness is some of these models cannot take in outside data in the form of features. With that said they are a terrific starting point for any new forecasting project. Often they can get you the required accuracy needed, and if they don‚Äôt they can serve as the benchmark to beat with other models. Here are a few common univariate models to try first.\n\nARIMA: An ARIMA (AutoRegressive Integrated Moving Average) model predicts future values in a time series by combining differencing (modeling the difference between periods), autoregression (using past values), and moving averages (using past forecast errors).\nExponential Smoothing: Forecasts future values in a time series by applying decreasingly weighted averages of past observations, giving more importance to recent data points to capture trends and seasonal patterns.\nSeasonal Naive: Predicts future values by simply repeating the observations from the same season of the previous period, assuming that future patterns will mimic past seasonal cycles.\n\n\n\nTraditional ML Models\nAfter trying univariate models, it‚Äôs time to try more traditional machine learning models. These are models built specifically for tabular data, or data that can live in a SQL table or excel spreadsheet. These models are multivariate, which allow them to incorporate outside variables as features to improve their forecast accuracy. They require more handling than a model like ARIMA, since they need feature engineering and proper time series cross-validation. Multivariate models can also learn across multiple time series at the same time, instead of being trained on just a single time series like a univarite model. Here are a few common multivariate models.\n\nLinear Regression: Predicts future values by fitting a line to the historical data, where the line represents the relationship between the dependent variable and one or more independent variables.\nXGBoost: Predicts future values using an ensemble of decision trees, boosting their performance by iteratively correcting errors from previous trees, resulting in a highly accurate and robust prediction model.\nCubist: Predicts future values by combining decision trees with linear regression models, creating rule-based predictions that incorporate linear relationships within each segment of the data for greater accuracy.\n\n\n\n\nReversal\nDo mega retail corporations like Amazon or Walmart only use ARIMA or linear regression models when trying to forecast the millions of product skus in their universe? Probably not. When the stakes are that high, and they can hire hundreds of data scientists to forecast, then they most likely build their own custom deep learning approaches that can learn from billions of data points to produce robust forecasts. With limitless resources and data, deep learning becomes easy. Assume you are not them.\nExciting startups like Nixtla have been doing great work on deep learning transformer models. These are the types of models that power products like GPT-4 from OpenAI. They built something called TimeGPT-1, which is a generative model for time series. They trained this model on billions of publicly available time series, creating the first GPT model tailored to time series. What required special hardware and tons of data to train can now be a simple API call in any programming language. This is a potential game changer and can completely change how forecasting is done, turning it more into a software engineering problem than a data science problem. Keep a close eye on this space as innovations like this can move at light speed.\n\n\nFinal Thoughts\nWhile deep learning holds great promise and can offer high accuracy in certain scenarios, it is often not the best starting point for most forecasting projects. The need for extensive data, expensive hardware, and the complexity of interpreting deep learning models make it a less practical choice compared to more traditional methods. Starting with simpler, well-established models like ARIMA, exponential smoothing, or traditional machine learning models often provides sufficient accuracy with lower costs and greater interpretability. As innovations continue to emerge, especially with models like TimeGPT-1, the landscape of time series forecasting may shift, making deep learning more accessible and practical. However, for now, prioritize simpler, more transparent models and reserve deep learning as a last resort when simpler methods fall short.\n\n\nSeries Wrap Up\nThat‚Äôs a wrap on our First Principles in Time Series Forecasting series! My goal was to walk through core concepts of creating a strong time series forecast. Instead of diving deep into code and super technical concepts, I wanted to give timeless knowledge that will serve anyone who builds or consumes time series forecasts. Hopefully you enjoyed the series and learned a lot ü§û."
  },
  {
    "objectID": "posts/2024-06-26-msft-ml-fcst-journey-2/index.html",
    "href": "posts/2024-06-26-msft-ml-fcst-journey-2/index.html",
    "title": "Microsoft Finance ML Forecasting Journey: Part Two",
    "section": "",
    "text": "This is a multipart series:\n\nPart One\nPart Two\n\nThe success of Microsoft finance‚Äôs first machine learning (ML) forecast spread like wildfire throughout finance. The ML forecast was shared with all finance leaders. So naturally knowledge of ML‚Äôs potential trickled down to more people across the organization. Eventually the news came to a team in central finance (CFT). Think of this team as Microsoft‚Äôs core FP&A team across the entire company. After seeing the accuracy at a worldwide level, this team knew it could help in the biggest forecast process at Microsoft. Something called the commercial field forecast. This forecast is created by finance members who sit in the ‚Äúfield‚Äù. The field is just a cool way to say regional offices all around the world. These field finance teams support the sales teams who also sit in the field. How could they take a worldwide forecast by product and break it down into specific countries all across the world? Well buckle up gang, it‚Äôs time to find out! This is how a tool called ‚ÄúCommercial Predict‚Äù was born in 2017.\n\nHow Things Used to Work\nBefore we dive into all the ML goodness, we have to understand how the old way used to work. I know, it‚Äôs kind of like eating your vegetables. But we just have to do it real fast then we can get to the fun parts.\nIn the past, each finance team in the field was responsible for their own forecast each quarter. These forecasts would happen in ‚ÄúCFO forecast cycles‚Äù. With cycles happening in October, January, and April. Microsoft‚Äôs fiscal year runs from July - June so these forecast cycles happen at the start of Q2, Q3, and Q4. The forecast at the start of Q1 is budget (that‚Äôs a story for a different day). Each cycle, a forecast would be created for the remainder of the fiscal year.\nMicrosoft sells products in over 100 countries. Most of those countries have a sales team that tries to sell products to companies in that geographical region. If there‚Äôs a sales team, then there is a finance team who supports them. This means there are dozens of sales finance teams creating quarterly forecasts for the rest of the fiscal year each CFO forecast cycle. Each team had their own secret recipe of how the forecasting was done. Often a custom excel model that would create the forecast. This model needed to be handled with care. Since each quarter it would have to be rolled over and prepared for the next forecast cycle. Anyone who has ever created and owned a financial model in excel knows the anxiety faced with trying to build and maintain one. These models were complex, and you said a little prayer every time you opened the file. Hoping it wouldn‚Äôt crash your machine because it was so large.\nOnce each team in the field had their forecast for their geography, it would get sent up the food chain. Forecasts from each country would be combined to form higher level aggregations in Microsoft‚Äôs sales territories. Each aggregation added more countries and continents together. This continued until you got the total worldwide number for the entire commercial business. Each time the forecasts got combined together at a higher level, senior finance leaders had the opportunity to make adjustments to that forecast. Based on their domain knowledge of the business. Eventually the final forecast the CFO, Amy Hood, saw was something completely different than what was initially created by each sales finance team for their specific geography.\nLayers upon layers of bias were added to the forecast. Some was good bias that could improve forecast accuracy, but often it was too many cooks in the forecast kitchen. Too many people touching a forecast that didn‚Äôt need to be touched. Resulting in worse accuracy and more confusion once the books were closed at quarter end. This process would take upwards of a month every quarter. From the initial forecast created by field team all the way up the food chain to the CFO. In the spirit of every good infomercial, ‚Äúthere just has to be a better way!‚Äù.\nNow you know why finance had to do something different. Drastically different.\n\n\nExcel Prototype Built in a Redmond Garage\nAll good things start from humble beginnings. The team in CFT wanted to centralize the field forecast process for the commercial business. Create a single way that everyone in the field would follow to create a forecast. To make this a reality, they started with the swiss army knife of every finance professional. That‚Äôs right. You guessed it. They started with excel. Like any innovative project, it quickly became their baby. And all babies need a name. The named it Commercial Predict.\nThe team created an excel prototype of a single model that every field finance team could use. It was a combination of the old and the new. First was old but reliable PxQ forecasts. Where you take what‚Äôs in the sales pipeline for a quarter and multiply it by how many deals on average have closed in similar historical quarters. Second was classic CAGR and year over year percentage growths, which acutally still work quite well. These traditional methods were combined with more statistical rigor. Something more along the lines of machine learning. They built by hand, formula by formula, exponential smoothing statistical models. Which is a common model in time series forecasting. It‚Äôs more stats than machine learning, but still performed really well. Today exponential smoothing is a simple function call in excel, but this team built it from scratch. I tip my cap to them, because that was hard to do.\nNow there were multiple forecasting methods in this mega excel model. The beauty of the idea is that someone could come into the model and choose what methods they wanted to use to forecast a specific geography, customer segments, and products. Users could even combine multiple methods together to get a more accurate forecast. This was powerful because someone could use the PxQ sales pipeline method for products that depended on big customer deals landing, and use the other methods for things that had more stable trends and seasonality.\nIt was genious prototype. The team was able to take this to their leadership team and show how one single approach to do forecasting in the field could save thousands of days of combined human effort across the field every year. One mega model to rule them all. It had the promise to cut forecasting down by 50%. Would it work though? To test it out, the team ran this excel model alongside the traditional bottoms up forecast process from each field team. They could then compare the results and even track accuracy across the old and new ways. The results were good. The new prototype was the same or even better than the existing process, but was 50% faster.\nThe new approach was fast, and it was accurate. The final roadblock before adopting the new approach was to get everyone in the field to agree on what level they should forecast at. This historically was a difficult subject to discuss, with everyone having differing opinions. Thankfully this was solved by getting the buy in from the top senior finance leaders in central finance and the field. Once that happened everyone was able to get on board.\nOk, so the team had a cool protoype that knew worked well. But if they just used that excel model, then they are still maintaining a model that is messy and requires constant upkeep. It would be hard to scale. They needed something more robust. A real tool that was built by engineers. Thankfully there was a team who could do just that.\n\n\nBuilding the Tool\nThe vendor team who was created to take over the initial ML forecasts was the up to the task. They had the data science knowledge, but the data engineering and software engineering needed to build a software tool to scale out the excel prototype was missing. So the team got other vendors to fill in those gaps. Now there was a team of engineers all capable of making the tool a reality.\nThe first version of the production level Commercial Predict tool had to be built fast, before an upcoming CFO forecast cycle. V1 was built into excel within six months as an add-in field users could download and connect to. It needed to combine new machine learning methods with traditional PxQ and CAGR/YoY run rate methods.\nHere‚Äôs how it worked.\n\nData engineers would pull historical revenue and sales pipeline data. All the forecasts methods were precomputed and saved in a database that was turned into a cube. This would be the starting point for all field members. Instead of calculating these forecasts by hand in their old excel models, it would be precomputed for them. At scale.\nThe forecasts in the cube were then served to users in a custom excel file. Each field team could come into the tool and select the geography, products, and segments they were responsible for forecasting. After making these selections, all the forecast methods would populate in the excel.\nUsers could then see each forecast method and see which ones pass their smell test of what they expect to happen in the business based on their domain knowledge. They could choose a specific forecast method to use, or combine multiple methods together to get a more robust forecast. Finally there might be things these forecast methods don‚Äôt know about. Like upcoming tax changes or product strategy changes. Field users could ultimately make manually adjustments to get a final forecast.\nOnce the final forecast was created, they could save it back to the cube. This allowed finance leaders to see the forecast creation in real time. Also it would prevent the classic ‚Äúexcel crash without saving‚Äù headache we‚Äôve all been through in the past.\nOnce the forecast was complete for each field team, a static output file was created at the touch of the button. Teams could take this output and load it into the final planning system.\n\nBefore official launch, training sessions were held to make sure everyone knew how to use it. It was also a good opportunity to fix any bugs in the tool. This resulted in some late nights and even weekend shifts, but the job got done. The tool was launched on time and the rest is history.\nWe were able to go from a forecast process of 21 business days each quarter, down to just 10. It was a revolution. This saved Microsoft millions of dollars each year of human capital. Finance teams in the field could now forecast faster, with less headaches, and prevent the layering of bias that was a staple of the previous way.\n\n\nEvolutions\nAfter this officially launched in 2018, the Commercial Predict tool has gone through a lot of iterations. What started in excel then moved into a web based tool. Then back to excel. With each iteration, we got better at the machine learning methods. Better at adding more features to give users more control over the final forecast.\nEventually Commercial Predict evolved into a much broader solution called ‚ÄúFusion‚Äù in 2022. Think of it as a tool that could still do the commercial field forecast process but now also take on other forecasts within Microsoft finance. A true one stop shop for all things planning. Fusion is an excel add-in with a built in UI on the side of excel. Kind of like how excel copilot opens on the side of your excel tab, Fusion does the same. A user could select what forecast they want to do, select the parts they‚Äôre responsible for forecasting, and Fusion would populate the blank excel file with all the information they need to finalize their forecast. Methods like PxQ and machine learning are still ran ahead of time. The UI was truly dynamic. You could take any excel file and open the Fusion app inside to get going on the forecast. Fusion allowed finance to scale the learnings of Commercial Predict to so many other forecast processes. Improving the impact ML and centralization can have on forecasting.\nPlanning tools like Fusion will most definitely change in the future. As the business evolves, so should our way of forecasting it. What doesn‚Äôt change is how ML has become a central part of the forecast process.\n\n\nLessons Learned\n\nIterate Iterate Iterate\nRome wasn‚Äôt built in a day. Instead of building this complex centralized forecasting tool from the start, we started small. Built a prototype. Got senior leadership buy in. And continued to make it better every 6-12 months. Even as I write this we are in the process of improving the ML accuracy of the commercial field forecast. If you‚Äôre coasting, you‚Äôre going downhill. You need to continue to iterate.\n\n\nCombine the Old with the New\nML didn‚Äôt outright replace every part of the commercial field forecast. Instead we combined ML techniques with older methods like PxQ sales pipeline methods. This allowed us to use the strengths of each approach based on what product was being forecasted. Some products are sensitive to large customer deals closing, so PxQ works best. Others have stable trends and seasonality, that‚Äôs where ML shines. Using both gives us the best of both worlds.\n\n\nSenior Leadership Buy In\nA forecast process is like a ship. The bigger the ship, the harder it is to change course. So the bigger the forecast process, the higher the buy in needed from a senior leader. Getting a GM or CVP level support allowed us to supercharge the change management. It‚Äôs easy to get bogged down in arguing with senior finance managers about how a forecast process should be done. Once a CVP (someone who reports to the CFO) comes in and says this is how we‚Äôre going to do it. Then everyone gets on board and starts turning the wheel of the ship togethger to change direction. The commercial field forecast had to get support from GM and CVP level leaders or else it would have taken years to change it instead of months.\n\n\n\nFinal Thoughts\nThe hardest part of any ML project comes down to people. Training models is easy, convincing people to use them is hard. It takes time. It takes senior leader buy in. It takes an open mind to rethink how your job can be done. It might be hard, but in the end it‚Äôs worth it.\nOften we get asked by finance teams outside the company if they can take our ‚ÄúCommercial Predict‚Äù or ‚ÄúFusion‚Äù tool off the shelf and start using it at their own company for forecasting. Sadly you cannot. We build a lot of these custom tools because we don‚Äôt have a choice. Microsoft‚Äôs business is complex. Often we need custom solutions that are hard to standardize in external products. Thankfully the machine learning methods we use are available for free as an open-source R package. Check it out if you‚Äôd like to learn more."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software I‚Äôve Written",
    "section": "",
    "text": "finnts - low code package for time series forecasting"
  },
  {
    "objectID": "software.html#r",
    "href": "software.html#r",
    "title": "Software I‚Äôve Written",
    "section": "",
    "text": "finnts - low code package for time series forecasting"
  }
]