[
  {
    "objectID": "start_here.html",
    "href": "start_here.html",
    "title": "Learn How I Think",
    "section": "",
    "text": "Machine Learning\n\nMachine Learning Forecasting Fundamentals\nMicrosoft Finance ML Forecast Journey\nMachine Learning Forecasting FAQ\nFirst Principles of Time Series Forecasting\nThree Levels of Machine Learning Adoption in Finance\n\n\n\nCareer\n\nMy Journey in Microsoft’s Finance Rotation Program\nGoing from Finance Intern to Senior Software Engineer at Microsoft\nPersonal User Manual\n\n\n\nLife\n\nPrinciples\nPower\nPriorities"
  },
  {
    "objectID": "posts/2025-03-21-ts-fundamentals-data-cleaning-stationary/index.html",
    "href": "posts/2025-03-21-ts-fundamentals-data-cleaning-stationary/index.html",
    "title": "Data Cleaning: Stationary",
    "section": "",
    "text": "This post is part of the data cleaning chapter within a larger learning series around time series forecasting fundamentals. Check out the main learning path to see other posts in the series.\nThe example monthly data used in this series can be found here. You can also find the python code used in this post here.\n\nTrends Are Hard\nIn the world of forecasting, many models need the trend component to be removed in order to learn the right signal from the data. The process of removing a trend (and sometimes seasonality) is called differencing, which can make a time series become stationary.\nStationary just means that the time series has a constant mean and variance over time. Put simpler, it’s a time series that looks kind of random with no discernable trend or repeating seasonality. If a time series does have a trend, then we can remove it through a process called differencing, which involves subtracting a value in one time period from a value in a previous time period. More on this later.\nSome models need data to be stationary to ensure it’s learning the correct relationships in the data, and doesn’t get fooled from thinking trends will always remain the same.\nLet’s take a look at one of our time series. Does it look like it has a constant mean and variance over time?\n\nHmmm, not really. Seems like there is an upward trend to the data. This means that the mean over time will also rise. It probably also has a slightly growing variance. Meaning the year over year growth amount is getting bigger and bigger. Let’s see what we can do to make this data stationary through differencing.\n\n\nTypes of Differencing\nThere are two main types of differencing. The first is simply taking the difference between two consecutive periods. For example taking the sales in December and subtracting the sales in November (previous month). This is called first order differencing. Let’s try that on our time series and see what happens.\n\nOk it’s looking a lot better! Our data is centered around zero, and doesn’t seem to be growing in magnitude over time. Seems like it could be stationary! Another way to apply differencing is by doing it twice, this is called second order differencing. You start by doing a first order difference, then do that same process again on that data. So instead of ending up with the change from period to period. You have the change of the change from period to period.\n\nA first order difference measures the change in position over time, which is like velocity\nA second order difference measures the rate of change of the rate of change, which is like acceleration (the change in velocity over time)\n\nTaking a second order difference is done when the time series still doesn’t have constant mean or variance. Let’s take a second order difference of our example time series.\n\nThings changed a little, but not much. Seems like a second order difference may not always be necessary. The final way to difference your time series is through seasonal differencing. This is the difference between an observation and the previous observation from the same season. For example with monthly sales, a season difference would be taking the difference between a month this year and the same month in the previous year. Taking a seasonal difference removes most seasonality in the data, hence the name. Let’s see how our data looks by just taking a seasonal difference.\n\nThe seasonal difference result looks good. Constant mean and variance throughout time. You can also combine both standard differencing with seasonal differencing. If your data has some seasonality, it’s best to do the seasonal difference first, then take the first order difference after. Let’s see how it looks.\n\nWe now have a nice mean around zero and no major variance throughout the data. Nice! But how the heck do we know if the data is stationary once we difference it?\n\n\nChecking if Your Data is Stationary\nIn order to check if the data is stationary, either the original data or data that’s been differenced, you can use something called a unit root test. Specially a Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test. This kind of test runs a few statistical processes and lets you know if the data it was fed was stationary or not. We can take this once step further by leveraging tools that build on this test and tell you how many differences should be applied on your data.\nLet’s try running this test on our data and see how many differences we might need to make the data stationary.\n\nFirst order difference: stationary ✅\nSecond order difference: stationary ✅\nSeasonal difference: stationary ✅\nSeasonal and first order difference : stationary ✅\n\nSweet! All of our differencing techniques each resulted in our data being stationary. It’s a good idea to pick the method that is the simplest way to get a stationary time series, which in our case is the first order difference. So we can use that going forward when training models.\n\n\nBack Transformations\nAfter you train a model and produce a forecast, the work is still not done. That forecast is the predicting the change between periods, not the actual values for that period. In order to get a final forecast we can use we need to transform that data back to the original units.\nTo do this we need all original values of the time series. When we difference our data, the process creates missing values at the start of the time series, because we cannot calculate the difference of the first period of the data since it’s the first period of the data. So a first order difference has one missing value at the start of the time series, a second order difference has 2, and a seasonal difference has many more depending on the seasonal periods in the data.\nTo get our final forecast we need to keep these original values, add them back to the differenced data, then start to add them up one by one. For a first order difference, that just means taking that original value in the first period, adding it to the differenced value in the second period, then adding that new value to the differenced value in the third period, and so on. It’s a daisy chained process to get the original data back. We carry this math forward into the future with our predictions, with the final output being our finished forecast!\nHere’s a super simple table breaking this down.\n\n\n\n\n\n\n\n\n\nMonth\nOriginal Series (Yₜ)\n1st Order Difference (ΔYₜ = Yₜ - Yₜ₋₁)\nTransformed Back to Original Units\n\n\n\n\nJan 2024\n100\n—\n—\n\n\nFeb 2024\n105\n+5\n—\n\n\nMar 2024\n108\n+3\n—\n\n\nApr 2024\n112\n+4\n—\n\n\nMay 2024\n115\n+3\n—\n\n\nJun 2024\n118\n+3\n—\n\n\nForecast\n—\n—\n—\n\n\nJul 2024\n—\n+3 (forecasted)\n121 (118 + 3)\n\n\nAug 2024\n—\n+3 (forecasted)\n124 (121 + 3)\n\n\nSep 2024\n—\n+3 (forecasted)\n127 (124 + 3)\n\n\n\n\n\nReversal\nThere are cases where differencing your data to ensure it’s stationary is not a good idea. Some models, like arima, have the differencing process built in. So you don’t need to do it beforehand. Other models, like standard linear regressions, don’t need stationary data at all since they can easily learn from trends and seasonal patterns in the data.\nDifferencing the data can also limit model interpretability, or the ability to explain the forecasted outputs. This becomes even harder once you start to difference your data two or more times, since you are not explaining changes between observations, but instead the change in the changes or even worse. This becomes near impossible to understand for any regular human that might use your forecasts.\nA final thing to call out with differencing is that it can remove historical data from the beginning of your time series. This may not be a big deal with a first order difference, but taking a seasonal difference on a monthly data set will remove the first 12 months of the time series. That may not be impactful if you have 10+ years of historical data, but if you only have 3 years to start with this becomes very detrimental to the process. That’s why I try to limit any seasonal differencing when the amount of historical data is low.\n\n\nFinal Thoughts\nStationarity is fundamental to reliable forecasting because it ensures your models are learning stable, meaningful patterns rather than temporary or misleading trends. A stationary time series maintains a constant mean and variance over time, making its behavior predictable and easier for statistical models to learn from.\nDifferencing is a powerful yet straightforward way to achieve stationarity, paving the path for more accurate forecasts. While simple first-order differencing often does the trick, be cautious with higher-order or seasonal differences, as they can reduce interpretability, remove valuable historical context, and complicate back transformations. Always prioritize the simplest approach that achieves stationarity, keeping both modeling and explanations clear and intuitive. Ultimately, knowing how—and when—to apply stationarity transformations is key to building robust, effective forecasts."
  },
  {
    "objectID": "posts/2025-03-01-february-learnings/index.html",
    "href": "posts/2025-03-01-february-learnings/index.html",
    "title": "February Learnings",
    "section": "",
    "text": "I love reading books, watching YouTube videos, listening to podcasts, you name it. Anything learning related is my jam. But I realized that if I don’t take notes on what I’m learning, I will probably forget everything. Now when I hear something interesting, I write it down in an Apple note for that month. Below are some of the learnings I jotted down in February, summarized by ChatGPT with additional context added by their new deep research feature. I hope you find them as interesting as I did."
  },
  {
    "objectID": "posts/2025-03-01-february-learnings/index.html#ai-machine-learning",
    "href": "posts/2025-03-01-february-learnings/index.html#ai-machine-learning",
    "title": "February Learnings",
    "section": "AI & Machine Learning",
    "text": "AI & Machine Learning\n\nKnowledge Distillation\n\nAI Distillation: Process of transferring knowledge from a large, complex AI model (teacher) to a smaller, efficient model (student). Example: DeepSeek training their R1 model using GPT-4 answers as training data.\n\n\n\nLarge Language Model (LLM) Architectures\n\nTransformer Architecture: Neural network architecture characterized by self-attention mechanisms, enabling parallel processing and context awareness.\nMixture of Experts (MoE): Architecture where subsets of neurons (“experts”) activate selectively per task, significantly reducing computational overhead and inference costs.\n\n\n\nEconomics of AI\n\nJevons Paradox: Lowering the cost of AI increases its usage dramatically.\nToken Costs: Input tokens (~¼ the cost of output tokens) allow parallel processing, whereas output tokens are sequential and auto-regressive.\nGPU Performance Factors: Critical aspects for AI training and inference are Floating-point operations (FLOPS), Memory bandwidth (IO), and Chip interconnectedness.\nCost Evolution of AI Models: GPT-3 costs dropped drastically from ~$60 to ~$0.05 per million tokens between 2022 and 2024, a 1,200x reduction.\n\n\n\nGeopolitics and AI Chips\n\nChinese companies circumvent U.S. Nvidia GPU export bans by routing purchases through third-party vendors in Singapore.\nDeepSeek Origin: Began as a spinoff of the Chinese hedge fund “High Flyer,” originally focused on GPU-based trading algorithms, later pivoting to AI models."
  },
  {
    "objectID": "posts/2025-03-01-february-learnings/index.html#cognitive-performance-optimization",
    "href": "posts/2025-03-01-february-learnings/index.html#cognitive-performance-optimization",
    "title": "February Learnings",
    "section": "Cognitive & Performance Optimization",
    "text": "Cognitive & Performance Optimization\n\nMost Important Question (MIQ) Framework\n\nRegularly clarify and revisit the single most critical, high-level question guiding your decisions, ensuring deep strategic alignment.\n\n\n\nPeak Performance Work Strategies\n\nOptimize intense, focused effort for fewer hours (3–5 hours/day) rather than prolonged mediocre sessions.\nReserve creative, high-focus tasks for peak-energy periods; routine tasks during lower-energy periods (e.g., noted dip around 2:30–3:30 pm).\n\n\n\nMental State Management\n\nLeverage breathing techniques to control adrenaline and enhance decision-making speed and clarity.\nPrioritize reflective time by avoiding immediate stimuli (e.g., smartphones) first thing in the morning.\n\n\n\nQuality Over Quantity\n\nShort bursts of deep work significantly outperform extended hours of moderate or distracted effort."
  },
  {
    "objectID": "posts/2025-03-01-february-learnings/index.html#health-fitness",
    "href": "posts/2025-03-01-february-learnings/index.html#health-fitness",
    "title": "February Learnings",
    "section": "Health & Fitness",
    "text": "Health & Fitness\n\nTraining Principles\n\nOptimal Training Splits: “Push-Legs-Pull” split recommended for optimal muscle recovery and growth.\nAim to train each muscle group twice weekly for maximal growth stimulus.\nUnderstand the interference effect—heavy back workouts might negatively impact squatting performance the next day.\n\n\n\nProtein Intake Recommendations\n\nNon-lifters: ~0.8 grams per pound of body weight daily.\nLifters aiming for muscle growth: ~1.3 grams per pound daily; significant growth benefit compared to lower intakes.\n\n\n\nEffective Supplements for Muscle Gain\n\nCreatine (improves strength and power), Protein powder (supports muscle recovery), Multivitamins (general health), Carb powders (Gatorade, Powerade for energy replenishment), Caffeine (enhances workout focus).\n\n\n\nRisks of Sedentary Behavior\n\nProlonged sitting significantly increases mortality risk; offset by regular activity and frequent movement breaks.\n\n\n\nBehavioral Eating Habit Adjustment\n\nWhen stressed or craving food, first take a 30-minute walk to distinguish genuine hunger from stress-induced urges."
  },
  {
    "objectID": "posts/2025-03-01-february-learnings/index.html#business-strategy",
    "href": "posts/2025-03-01-february-learnings/index.html#business-strategy",
    "title": "February Learnings",
    "section": "Business & Strategy",
    "text": "Business & Strategy\n\nNew Product Strategy\n\nStart by identifying core insights, fire small “test” bullets (low-risk experiments), and upon success, scale with larger, well-resourced “cannonballs.”\n\n\n\nMarket Psychology\n\nOsborn Effect: Announcing future products too early can stall current sales as customers delay purchasing decisions.\nHalo Effect: Successfully launching a new product enhances brand image, positively affecting the sales of existing products.\n\n\n\nStrategic Hiring & Expert Filtering\n\nIdentify talent by querying top performers for recommendations, refining the hiring funnel to focus on demonstrated capability and peer recognition.\n\n\n\nEconomic Insights\n\nGDP Growth Simplified: Driven by the productivity per worker and the total workforce.\nIdiot Index: Metric comparing raw material costs to finished product price, highlighting operational inefficiencies and pricing strategies."
  },
  {
    "objectID": "posts/2025-03-01-february-learnings/index.html#historical-cultural-insights",
    "href": "posts/2025-03-01-february-learnings/index.html#historical-cultural-insights",
    "title": "February Learnings",
    "section": "Historical & Cultural Insights",
    "text": "Historical & Cultural Insights\n\nMongol Empire\n\nExceptional warfare tactics: mobility, psychological intimidation, and merit-based leadership. Each soldier crafted their own bows, ensuring skill and accountability.\nMongol rule lasted until Soviet invasion (~1220–1920), marking the longest family-led empire in history.\nGenghis Khan’s aggressive strategies rooted partly in personal revenge and necessity after familial trauma.\n\n\n\nKorean Language Origins (Hangul)\n\nHangul invented explicitly due to difficulty in learning complex Chinese characters. Designed for rapid acquisition, enhancing widespread literacy.\n\n\n\nStanford’s “Touchy-Feely” Class\n\n“Interpersonal Dynamics” at Stanford GSB teaches emotional intelligence, authentic communication, and relationship-building skills via experiential T-group sessions.\n5 levels of communication\n\nRitual: classic conversations like general greeting\nExtended ritual: asking about the weather or a recent sports game\nContent: facts about a project or work items\nEmotional self-disclosure: saying how you feel emotionally (I feel sad)\nNeutral emotional self-disclosure: expressing emotion at another person (I feel proud of you, angry at you, etc)\n\nTo have strong relationships in life and career, you have to get to level 4 and 5"
  },
  {
    "objectID": "posts/2025-03-01-february-learnings/index.html#psychology-behavioral-science",
    "href": "posts/2025-03-01-february-learnings/index.html#psychology-behavioral-science",
    "title": "February Learnings",
    "section": "Psychology & Behavioral Science",
    "text": "Psychology & Behavioral Science\n\nBehavioral Loops & Addiction\n\nScarcity Loop: Opportunity → Unpredictable reward → Quick repeatability drives addictive behaviors (gambling, social media).\nNear-Miss Effect: Close failures in gambling or gaming increase dopamine and repeat behaviors, driving addiction.\n\n\n\nImportance of Courage\n\nCourage is central to achieving significant outcomes; talent and intelligence without courage lead to stagnation and regret.\n\n\n\nNova Effect\n\nSudden positive events (e.g., winning the lottery) can negatively impact perceived happiness afterward. Gradual progress allows better psychological adaptation."
  },
  {
    "objectID": "posts/2025-03-01-february-learnings/index.html#miscellaneous-insights-notable-quotes",
    "href": "posts/2025-03-01-february-learnings/index.html#miscellaneous-insights-notable-quotes",
    "title": "February Learnings",
    "section": "Miscellaneous Insights & Notable Quotes",
    "text": "Miscellaneous Insights & Notable Quotes\n\nPhilosophical Reflections\n\nSuccess & Failure: “Success is going from failure to failure without loss of enthusiasm.”\nLife Razor (Personal Decision Framework): Prioritize actions granting maximum control over personal time.\nSeasons of Life: Life priorities shift naturally through different life stages; adaptability is crucial.\nLearning & Reflection: True learning arises when experience intersects with thoughtful reflection.\n\n\n\nIntriguing Observations\n\nDiamonds priced disproportionately at milestone sizes (2-carat vs. 1.99-carat).\nChronic strep throat indicates potential mold exposure.\nEpstein-Barr virus (mono) infection necessary precursor for multiple sclerosis.\nIcelandic dating app created by the government prevents accidental familial relationships.\nDrug companies advertise heavily on news networks, indirectly shaping media narratives.\n\n\n\nNotable Quotes\n\n“Your life is in your hands; don’t drop it.”\n“The generation that lights the fuse usually gets buried in the rubble.”\n“Be interested, not interesting.”\n\n“If more money wouldn’t change how you spend your time, you’re already rich.”\n\n“Your health at age 80 is a reflection of your relationships at age 50.”"
  },
  {
    "objectID": "posts/2025-02-05-ts-fundamentals-data-cleaning-boxcox/index.html",
    "href": "posts/2025-02-05-ts-fundamentals-data-cleaning-boxcox/index.html",
    "title": "Data Cleaning: Box-Cox Transformations",
    "section": "",
    "text": "This post is part of the data cleaning chapter within a larger learning series around time series forecasting fundamentals. Check out the main learning path to see other posts in the series.\nThe example monthly data used in this series can be found here. You can also find the python code used in this post here.\n\nKeeping It In Between The Lines\nMost of time series forecasting is all about trying to fit a line to your data. There are a million ways to do it, but at its core we want to fit a line that’s as close as possible to the original data and where we think future data points might lie.\nThe easiest line to fit is already a straight line, or one that has linear growth. Consider the chart below. What do you think the future forecast will be? Pretty easy, a model can just extend the same upward linear growth going forward.\n\nWe would always prefer to model a time series that has a linear trend. But often that’s not the case in businesses. Revenue might double every year, and that my friend is not a linear line, but instead an exponential one. A time series growing exponentially is harder for many models to forecast because most models expect the variance of the data to stay the same over time (more on this in the stationary post), and a expanding time series violates this assumption. Consider one of our example time series in our dataset below. See how it grows not at a linear pace but a hockey stick like exponential pace?\n\nA good way to handle this exponential growth is to transform it into one that looks more linear. This is where a technique called a box-cox transformation comes in handy.\n\n\nBox-Cox Enters The Chat\nThe Box-Cox transformation is a family of power transformations that aim to make data more normally distributed and stabilize its variance​. In simple terms, it transforms your original values using an exponent (λ, “lambda”) to reduce skewness.\nLambda can take many values. A lambda of 0 corresponds to a log transform. A value of 0.5 is a square root transform. By transforming our data this way we can in essence de-scale the data to ensure any large spikes are dampened and easier for a model to fit the data. This can transform our data to be homoscedastic, or having constant variance. Let’s see how our time series looks after applying a few box-cox transformations.\n\n\nThe log transformation (lambda = 0) seems to have done a great job at turning our exponential growth into linear growth. The square root transformation (lambda = 0.5) didn’t do as well. Choosing the right value of lambda is important, thankfully there is an automated process that can do it for us. Let’s see how the data looks with an optimal lambda value chosen automatically.\n\nThe automated process chose a lambda value very close to zero. So it basically did a simple log transformation. Now our data has become much easier to model correctly, how cool is that!\n\n\nReversal\nThere are no perfect solutions, only tradeoffs. Here are a few when using box-cox transformations.\n\nRunaway exponential trends: If your time series has exponential trend aspects, but isn’t actually a true exponential trend, there can be big repercussions. A model might take that transformed data and start creating future forecasts that are astronomical in size, going from thousands to trillions in a few periods. A good example is a time series that is stable for most of its history, then at the very end it all of a sudden has tripled in value over one or two periods. A box-cox transformation will pick up on this and transform the data in a way that a model might pick up on as a trend that should grow into the future. So each subsequent forecast period might continue to triple in value. That’s not good.\nTime series can’t be zero or negative: All zero values must be changed to be a placeholder value like 0.1 or 1, that way things like a log transformation can work. There also can’t be any negative values, again because you can’t take the log of a negative number, which in some time series is unavoidable.\nNeed to back transform the data after forecasting: After you train a model and create a forecast on the transformed data, you must transform the forecast back to the original units. So you have to make sure you keep the lambda value used in the box-cox transformation handy, because you will need it to inverse the transformation.\nLess model interpretability: Because a model is learning on transformed data, being able to explain the models decisions become harder. Know there is always a tradeoff between model accuracy and model interpretability.\n\n\n\nFinal Thoughts\nBox-Cox transformations are a powerful tool for taming exponential growth in time series, making trends easier to model and forecast. However, like any transformation, they come with tradeoffs. Potentially misleading trends, the need for careful handling of zeros and negatives, and additional steps to back-transform predictions. Despite these challenges, applying box-cox correctly can make a significant difference in forecasting accuracy. The key takeaway? Use transformations thoughtfully, always validate their impact on your specific dataset, and remember that the best forecast starts with well-prepared data."
  },
  {
    "objectID": "posts/2025-02-01-january-learnings/index.html",
    "href": "posts/2025-02-01-january-learnings/index.html",
    "title": "January Learnings",
    "section": "",
    "text": "I love reading books, watching YouTube videos, listening to podcasts, you name it. Anything learning related is my jam. But I recently realized that if I don’t take notes on what I’m learning, I will probably forget everything. Now when I hear something interesting, I write it down in an Apple note for that month. Below are some of the learnings I jotted down in January, summarized by ChatGPT. I hope you find them as interesting as I did.\n\n1. Work-Life Balance & Fulfillment\n\nFun, not discipline: Scheduling enjoyable activities each week prevents work from creeping into all free time.\n\nSuccess definition: True success is loving both your work and your personal life.\n\nCapping success: Decide how much growth you can handle without losing who you are.\n\n\n\n2. Tradeoffs & Decision-Making\n\nNo perfect solutions: Everything has pros and cons; success lies in choosing the right tradeoff.\n\nRed flags: If you find yourself thinking “but it’s so much money,” reassess your motives.\n\nFear of big goals: While aiming high is less crowded, it can also be used as an excuse if you fail.\n\n\n\n3. Competition & High Performance\n\nHidden costs: Emulating high performers may mean accepting hidden sacrifices.\n\nConsistency: Deliberate, ongoing practice filters out those who can’t sustain effort.\n\nFocus vs. Obsession: The lazy lose to the average, the average lose to the focused, the focused lose to the obsessed.\n\n\n\n4. Adaptability & Innovation\n\nPivot or perish: Like Western Digital clinging to telegrams post-telephone, ignoring new tech can be fatal.\n\nCompany reinvention: Nintendo, YouTube, and Slack all started with entirely different products.\n\n\n\n5. Human Needs & Motivations\n\nBeyond basics: After food and shelter, people want affiliation, status, and freedom from fear.\n\nStatus & affiliation: These drive many personal and social decisions, from kids to career choices.\n\n\n\n6. Simplicity & Execution\n\nComplexity as an enemy: Simpler approaches often yield better execution (Tony Robbins).\n\nStress from neglect: Stress arises from ignoring important tasks or problems.\n\n\n\n7. Personal Growth & Mindset\n\nMeditation zones: Mindfulness, Support, Flow, and Awakening each expand consciousness in different ways.\n\nCourage gap: For most smart people, the missing piece is the bravery to act.\n\nThinking in writing: Writing clarifies thoughts and fosters deeper understanding.\n\n\n\n8. Entrepreneurship & Career\n\nKeys to success:\n\nCourage to start\n\nPerseverance to test multiple approaches\n\nSkills to build, sell, and harness luck\n\n\nBuilding an audience: Focus on the right followers, share your niche expertise, and aim for ongoing improvement.\n\nIf you don’t choose a goal: Society will assign you one you might not want.\n\n\n\n9. Health & Well-Being\n\nWeight control: Balance palatability with low calorie density.\n\nDental health: Poor oral care can raise dementia risk by up to 30%.\n\nMarriage vs. career: A great marriage and an average job can be happier than a great job but unhappy marriage.\n\n\n\n10. Education & Parenting\n\nRaising a sovereign child: Freedom over control; allow exploration and independent thought.\n\nScaffolding: Provide temporary support or structure until a child (or learner) can handle it on their own.\n\nBeware status games in youth: Sports can become more about prestige than personal growth.\n\n\n\n11. Angel Investing & Business Acumen\n\nFounder first: Investors should look at who’s running the startup more than the idea itself.\n\nPratfall effect: Revealing small imperfections after showing competence makes you more relatable.\n\n\n\n12. Societal Observations & History\n\nSlowing progress: Bigger scientific fields often have slower breakthroughs.\n\nPolicy & power: Politicians can stifle innovation (e.g., Facebook’s Libra).\n\nOdd facts: 80% of US cash is in $100 bills; 70% of restaurant food is delivered.\n\n\n\n13. Influence & Conformity\n\nPreference falsification: Publicly supporting what you don’t believe privately.\n\nConformity vs. prestige: We follow the crowd for safety or follow those with status to elevate ourselves.\n\n\n\n14. AI & the Future of Work\n\nLeadership vs. management: As AI handles tasks, leadership (direction, strategy) grows more important than micromanagement.\n\nUsing AI: Either harness AI to assist your projects or risk falling behind.\n\n\n\n15. Overarching Wisdom\n\n“Enough” vs. “more”: Don’t let the pursuit of endless growth blind you to the beauty of having enough.\n\nFail fast, learn faster: The cost of being wrong is often lower than the cost of doing nothing.\n\nMind as friend: Wisdom comes from taming your own mind."
  },
  {
    "objectID": "posts/2025-01-24-ts-fundamentals-eda-xregs/index.html",
    "href": "posts/2025-01-24-ts-fundamentals-eda-xregs/index.html",
    "title": "Exploratory Data Analysis: External Regressors",
    "section": "",
    "text": "This post is part of the exploratory data analysis chapter within a larger learning series around time series forecasting fundamentals. Check out the main learning path to see other posts in the series.\nThe example monthly data used in this series can be found here. You can also find the python code used in this post here.\n\nMay The Outside Forces Be With You\nWhen creating a time series forecast, you really only need three columns. One to identify one time series from another, a timestamp (date) column, and your target variable. With these three things you can produce high quality forecasts. Unfortunately most time series do not live in a vacuum, and are subject to outside forces. These forces could be things like macroeconomic factors like inflation or consumer sentiment. The growing market share of your competitors. Or even when your product might go on sale. These are all external regressors (xregs), or outside data points that can be used to improve the predictive accuracy of our time series forecast.\nExternal regressors come in many shapes and sizes. They are usually chosen based on the domain expertise of whatever you’re trying to forecast. Outside of a hunch that they might work, how can we tell if they should be used in our time series forecast? Let’s walk through a few methods to measure the relationship between external regressors and our target variable.\n\n\nMeasuring Linear Relationships\nThe most straightforward approach is a simple correlation analysis. We take the external regressor time series, and calculate the correlation against our target variable time series. This gives us a value between -1 and 1. Having either a strong negative correlation or positive correlation are both helpful. Here are a few rules of thumb to measure strong correlations of external regressors based on the date grain of your data.\n\nYear/Quarter/Month: Absolute value correlation of 0.5 or higher.\nWeek/Day: Absolute value correlation of 0.2 or higher.\n\nIt’s important to note that those values are arbitrary. This isn’t a perfect threshold to use. So make sure you adjust that based on your own time series data. Let’s take a time series from our example dataset and calculate the correlation against a few external regressors.\n\n\n\nDate\nCountry\nProduct\nRevenue\nxreg1\nxreg2\n\n\n\n\n2019-01-01\nUnited States\nCookies\n25\n12.37\n27\n\n\n2019-02-01\nUnited States\nCookies\n10\n15.73\n2\n\n\n2019-03-01\nUnited States\nCookies\n15\n39.68\n67\n\n\n2019-04-01\nUnited States\nCookies\n45\n60.17\n95\n\n\n2019-05-01\nUnited States\nCookies\n55\n72.84\n5\n\n\n2019-06-01\nUnited States\nCookies\n65\n53.31\n21\n\n\n2019-07-01\nUnited States\nCookies\n25\n9.65\n31\n\n\n2019-08-01\nUnited States\nCookies\n15\n12.20\n3\n\n\n2019-09-01\nUnited States\nCookies\n10\n10.04\n8\n\n\n2019-10-01\nUnited States\nCookies\n5\n19.62\n88\n\n\n\n\nThe first external regressor (xreg1) seems to have similar trend and seasonality patterns as our revenue target variable. The second external regressor (xregs2) on the other hand is all over the place. Here is the correlation with each xreg.\n\n\n\nVariable 1\nVariable 2\nCorrelation\n\n\n\n\nRevenue\nxreg1\n0.83\n\n\nRevenue\nxreg2\n0.05\n\n\n\nThe table confirms our initial smell test after reading the line chart. Xreg1 has a strong correlation, and xreg2 does not. This tells us that xreg1 could be a good candidate to use as a feature in our ML forecast model. What if we don’t know what xreg1 values are going to be in the future? That’s where calculating correlation on lags becomes valuable. We can calculate lags for 1 month, 2 months, 3 months, and onward to see if lag values of our external regressors can also be used as features.\n\n\n\nVariable 1\nVariable 2\nCorrelation\n\n\n\n\nRevenue\nxreg1\n0.83\n\n\nRevenue\nxreg2\n0.05\n\n\nRevenue\nxreg1_lag_1\n0.64\n\n\nRevenue\nxreg1_lag_2\n0.24\n\n\nRevenue\nxreg1_lag_3\n-0.17\n\n\nRevenue\nxreg1_lag_6\n-0.07\n\n\nRevenue\nxreg1_lag_9\n-0.43\n\n\nRevenue\nxreg1_lag_12\n0.65\n\n\nRevenue\nxreg2_lag_1\n0.00\n\n\nRevenue\nxreg2_lag_2\n-0.11\n\n\nRevenue\nxreg2_lag_3\n-0.15\n\n\nRevenue\nxreg2_lag_6\n0.17\n\n\nRevenue\nxreg2_lag_9\n0.04\n\n\nRevenue\nxreg2_lag_12\n0.15\n\n\n\nIn addition to the original xreg1 (at lag 0), the 1 and 12 month lags also have a high correlation. A 12 month lag shows that there is strong seasonality in both our original time series and xreg1. This will be helpful once we start to train models.\n\n\nMeasuring Non-Linear Relationships\nTraditional correlation is great and all, but falls short if there are non-linear relationships between the variables. In order to capture non-linear relationships, we have to bring in some heavy firepower. Using either of the below approaches.\n\nMutual Information (MI)\n\nOverview:\n\nMeasures the shared information between two variables (or time series).\nCaptures linear and non-linear dependencies.\nBased on information theory: higher MI indicates more shared patterns or dependencies.\n\nKey Points:\n\nValues are always non-negative.\nMI = 0: No dependence (variables are independent).\nHigher MI: Greater dependency or shared information.\n\nInterpretation:\n\nMI doesn’t distinguish between positive or negative relationships (unlike correlation).\nIt only reflects the strength of dependency, not its direction or type.\n\n\nDistance Correlation (dCor)\n\nOverview:\n\nMeasures the dependence between two variables by analyzing distances between all pairwise points.\nCaptures both linear and non-linear relationships.\nUnlike MI, dCor is normalized to a specific range.\n\nKey Points:\n\nValues range from 0 to 1.\ndCor = 0: Variables are independent.\ndCor = 1: Perfect dependence (linear or non-linear).\n\nInterpretation:\n\nA value closer to 0: Weak or no dependency.\nA value closer to 1: Strong dependency.\nIt is symmetric and doesn’t distinguish between causation or directionality.\n\n\n\nDistance correlation is easier to understand and compare scores across different data sets, but mutual information can understand more complex non-linear relationships. So either can be helpful. Dealers choice on what you want to use. Let’s calculate a mutual information score on our external regressors and their lags.\n\n\n\nVariable 1\nVariable 2\nMutual Information\n\n\n\n\nRevenue\nxreg1\n0.52\n\n\nRevenue\nxreg1_lag_1\n0.18\n\n\nRevenue\nxreg1_lag_2\n0.00\n\n\nRevenue\nxreg1_lag_3\n0.01\n\n\nRevenue\nxreg1_lag_6\n0.00\n\n\nRevenue\nxreg1_lag_9\n0.04\n\n\nRevenue\nxreg1_lag_12\n0.05\n\n\nRevenue\nxreg2\n0.00\n\n\nRevenue\nxreg2_lag_1\n0.00\n\n\nRevenue\nxreg2_lag_2\n0.00\n\n\nRevenue\nxreg2_lag_3\n0.00\n\n\nRevenue\nxreg2_lag_6\n0.00\n\n\nRevenue\nxreg2_lag_9\n0.00\n\n\nRevenue\nxreg2_lag_12\n0.00\n\n\n\nSimilar to regular correlation, xreg1 shows a strong relationship with revenue. But for MI it only has a relationship with lag 0 and lag 1. All other values are either very small or zero, meaning there is not strong relationship. This differs from the original correlation values. Usually it’s a good idea to use a few of these methods together. If external regressors are flagged as important in more than one of these approaches, then it’s safe to say they have some predictive power for our target variable. So for our data we can keep xreg1 to use in our model and drop xreg2 since it’s mostly noise with no signal.\n\n\nDealing With Binary Regressors\nThere’s one final type of analysis we are missing. Sometimes adding simple binary variables as external regressors can result in major accuracy improvements. These are 1 or 0 values that can represent a certain event or action happening. For example you might have new product launches in random months throughout the year. These can be represented with a 1 when there is a product launch, and a 0 when there is not. Without this binary data, a model may pick up on trends and patterns that may not repeat in the future. Your business might thrive during the months leading up to the US presidential election, but that only comes around every four years. So giving the model context of those events is important.\nAn interesting binary regressor in recent years has been a COVID-19 flag. This shows when COVID was impacting the world the most. When looking at the chart of our time series above, we can see a large dip in 2020. So it looks like COVID has a potential effect. Let’s create a COVID flag with a value of 1 for every month in 2020 and a value of 0 for all other months. We will also create a second flag that contains random 1 and 0 values to see if we can determine which regressor is important to our specific time series.\nWe can calculate a mutual information score for each one.\n\n\n\nVariable 1\nVariable 2\nMutual Information\n\n\n\n\nRevenue\nCOVID Flag\n0.0033\n\n\nRevenue\nRandom Flag\n0.0000\n\n\n\nWhile the mutual information score for COVID is small, it’s still above zero. Which proves that this COVID flag may help improve our models predictive performance.\nFinally there are other ways to figure out the importance of external regressors, but most of them deal with training actual models. I’ll wait on discussing those techniques until we build up some more foundational knowledge and eventually get to the sections on models themselves. Stay tuned.\n\n\nReversal\nWhen working with external regressors, it’s important to approach correlations and relationships critically. A strong correlation might suggest a connection, but it does not prove causation. This distinction is crucial, as relying on variables without clear logical links to your target can mislead your analysis and undermine your forecast.\nAnother issue to watch for is multicollinearity. This happens when external regressors are highly correlated with one another. Models can struggle to differentiate the effects of variables that overlap significantly, leading to instability in coefficient estimates and diminished interpretability. To mitigate this, examine the correlations among your regressors before including them. If two variables are redundant, consider removing or transforming one to reduce redundancy.\nUltimately, external regressors should be chosen based on both statistical evidence and domain knowledge. They should have a clear, interpretable relationship with the target variable. Without this foundation, even the most promising-looking variables can introduce unnecessary complexity and noise.\n\n\nFinal Thoughts\nIncorporating external regressors into your time series analysis can significantly enhance the accuracy and robustness of your forecasts when done carefully. The methods discussed, including correlation analysis and mutual information, are essential tools for evaluating potential regressors. However, they should always be applied alongside sound reasoning and a thorough understanding of the business.\nIt’s also worth noting that the relationship between external regressors and your target may not always be straightforward. Non-linear relationships, lags, and interactions with other variables often require additional exploration to capture their full impact. This underscores the importance of iterative experimentation and validation throughout the forecasting process.\nAs you refine your approach, prioritize quality over quantity. A well-selected set of regressors tailored to the problem at hand will yield better results than an overly broad or poorly vetted selection. The goal is not just to improve the model’s performance but to ensure that the improvements are grounded in meaningful insights.\nIn time series forecasting, the integration of external regressors bridges the gap between historical patterns and the broader forces shaping the future. By combining statistical rigor with contextual understanding, you can build models that not only predict but also provide actionable insights into the dynamics of your business."
  },
  {
    "objectID": "posts/2025-01-01-december-learnings/index.html",
    "href": "posts/2025-01-01-december-learnings/index.html",
    "title": "December Learnings",
    "section": "",
    "text": "I love reading books, watching YouTube videos, listening to podcasts, you name it. Anything learning related is my jam. But I recently realized that if I don’t take notes on what I’m learning, I will probably forget everything. Now when I hear something interesting, I write it down in an Apple note for that month. Below are some of the learnings I jotted down in December, summarized by ChatGPT. I hope you find them as interesting as I did.\n\n1. Business & Entrepreneurship\n\nPassive Income & Productized Services\n\nBest passive income model: Offer a service that helps companies make more money.\n\nIdeally, “productize” your service so it’s standardized, repeatable, and less reliant on your day-to-day involvement.\n\n\n\n\nStarting a Company From Scratch\n\nValidate demand first: Ask people if they want what you’re selling; presell if possible.\n\nFind a repeatable way to get customers (test channels early).\n\nBuild only after talking to potential customers; keep it simple until proven.\n\nKnow if you’re building to sell or building to run:\n\nLLC if you want to pay yourself along the way.\n\nC Corp if you want to sell eventually (consider QSBS after five years).\n\n\n\n\nMonetization & Branding\n\nEven in a commodity space, branding can create a unique edge (e.g., Liquid Death water).\n\n“Startups fail, founders don’t”: Persistence leads to eventual success if you keep learning.\n\n\n\nKey Mindsets\n\n“The fundamental algorithm of life is to repeat what works.”\n\nPair your primary metric with a “check” metric (e.g., revenue + profit margin) to avoid over-optimizing in one direction.\n\n\n\n\n2. Management & Leadership\n\nTypes of Delegation\n\nTell them what to do.\n\nAllow them to make decisions.\n\nAim for #2, but you might start with #1 until trust and competence are established.\n\n\n\n\nAvoid “Monkeys on Your Back”\n\nWhen employees bring a problem, coach them to leave with their own solution.\n\nAsk guiding questions (“Are you sure we need to do anything right now?”) instead of providing answers.\n\n\n\nOne Metric Management (Peter Thiel’s Approach)\n\nAssign each employee exactly one main metric or goal.\n\nThat’s all you focus on in 1:1s.\n\nThey own the outputs and figure out the inputs themselves.\n\n\n\nFiring & Role Clarification\n\nThe moment you think “this might be a firing situation,” have a clarifying conversation about expectations.\n\nFiring ultimately benefits both parties if the role isn’t a fit.\n\n\n\nHiring Strategy for New Business\n\nFirst hire: Virtual assistant to free up time (low-risk, low-cost).\n\nSecond hire: Sales/marketing to boost cash flow, then operations.\n\nHiring internationally can be a “no-brainer” for cost savings (e.g., via Shepherd).\n\n\n\nWho, Not How\n\nWhen a problem arises, ask “Who can solve this for me?” instead of “How will I solve it?”\n\nThis mindset shift helps you move from owning a ‘job’ to owning a ‘business.’\n\n\n\n\n3. Productivity & Personal Growth\n\nDaily “Vitamins” for Health\n\nWalks/Nature\n\n45 minutes of exercise\n\nReading, YouTube (educational), Writing, Building\n\n1:1 time with family/friends\n\n150 grams of protein\n\n\n\nPreventing Burnout\n\nAsk: “Is this important? Will it make a difference?” Only focus on the important.\n\nOur state determines our story—manage your energy and emotions.\n\n\n\nHabits & Dopamine\n\nDopamine gained without effort (e.g., instant gratification) can be dangerous.\n\nGood results today come from consistent, small choices made previously.\n\n\n\nTime Freedom Over Money\n\nMany people chase money for freedom but ironically lose freedom by obsessing over it.\n\nConsider designing a life you enjoy now, rather than waiting for a financial milestone.\n\n\n\n\n4. Health & Longevity\n\nKey Factors\n\nExcess fat (adipose tissue) has the largest negative impact.\n\nMaintain a healthy weight and an 80–90% healthy diet for most of the benefits.\n\n\n\nOther Longevity Insights\n\nDeveloping countries often have lower life spans due to poor indoor air quality.\n\nAs you age, vacations should be longer because your body needs more time to relax.\n\nCreativity correlates with longevity.\n\n\n\nEnvironmental & Lifestyle Factors\n\nBlue light and flickering LEDs can contribute to stress or weight gain.\n\nPosture affects how much you believe your own thoughts—mind your physical stance.\n\n\n\n\n5. Mindset & Philosophy\n\n3Q Filters Test (Jon Acuff)\n\nIs it true?\n\nIs it helpful?\n\nIs it kind?\n\nIf your thoughts are not one of these, discard it immediately.\n\n\nSocratic Method & Curiosity\n\nInstead of disagreeing, ask questions to uncover someone’s reasoning.\n\nServe as a “midwife” to help them birth new ideas rather than forcing your own.\n\n\n\nQuotes & Wisdom\n\n“Content is about emotion, not information.” —Shaan Puri\n\n“Slow is smooth, smooth is fast.”\n\n“No one is wrong on purpose.” —Socrates\n\n“Make a sense of humor your default emotion.” —Matthew McConaughey\n\n“If you want to win, only run races where winning is up to you.” —Epictetus\n\n\n\nIdentity Diversification\n\nDon’t let your job (or parenthood) be your sole identity.\n\nSpread your sense of self across multiple areas for resilience.\n\n\n\nLive Verb-Focused, Not Noun-Focused\n\nFocus on what you do, not the title you hold.\n\nPrioritize experiences and meaningful actions over static labels.\n\n\n\n\n6. Relationships & Social Insights\n\nDelegation & Autonomy\n\nNot every employee wants total autonomy; many prefer structure and a clear path to win.\n\n\n\nFriendships & Connections\n\n“What is a friend? A single soul dwelling in two bodies.” —Aristotle\n\n~200 hours of shared experience can turn an acquaintance into a close friend.\n\n25% of adults are estranged from a parent; relationships can be complex, so invest in those that matter.\n\n\n\nFamily & Societal Trends\n\nPeople have fewer kids because expectations (and costs) for each child have gone up dramatically.\n\n\n\nHandling Lows in Relationships\n\n“It’s the lows, not the highs, that make or break a relationship.”\n\nApproach conflicts with empathy, short bursts of feedback, and open communication.\n\n\n\n\n7. Planning & Year-End Reflection\n\nClose Out the Year\n\nClear old emails, clothes, subscriptions; do a full evaluation.\n\nImplement a consistent “planning system” to incorporate mini-adventures regularly.\n\n\n\nAdding One New Winning Habit per Quarter\n\nIncremental habit-building: small, sustainable changes that compound over time.\n\n\n\nMisogi (Year-Defining Challenge)\n\nEach year, take on one big challenge or adventure.\n\nMeanwhile, schedule smaller weekly “mini-adventures” to stay engaged and curious.\n\n\n\n\nCommon Threads\n\nEmpowerment Through Delegation & Systems\n\nFrom “One Metric Management” to “Who Not How,” there’s a recurring emphasis on freeing yourself from minutiae so you can focus on strategic, high-impact work.\n\nValidation & Focus in Business\n\nYour notes repeatedly stress testing ideas early (e.g., presales), focusing on the one crucial metric or channel, and doubling down on what works.\n\nHealthy Mind & Body for Sustainable Success\n\nLongevity strategies, stress management, and daily rituals (walks, exercise, reading) show a consistent theme of taking care of yourself to thrive over the long term.\n\nMindset Shifts & Philosophical Anchors\n\nUsing the 3Q filter, the Socratic method, and quotes that highlight freedom, creativity, and identity diversification. Emphasizing how perspective, curiosity, and humor make life richer.\n\nImportance of Connection & Relationships\n\nWhether it’s rotating leadership roles, maintaining close friendships, or supporting employees appropriately, relationships and the way you interact with people matter.\n\nContinuous Reinvention & Reflection\n\nThe idea of year-end reviews, building new habits quarterly, and tackling “misogi”-level goals underscores the desire for ongoing personal evolution and adaptability."
  },
  {
    "objectID": "posts/2024-12-06-chatgpt-pro-is-cheap/index.html",
    "href": "posts/2024-12-06-chatgpt-pro-is-cheap/index.html",
    "title": "ChatGPT Pro is Cheap",
    "section": "",
    "text": "The latest offering from OpenAI costs $200 a month, and it’s actually a steal. It’s called ChatGPT Pro and it’s 10x more expensive than their existing paid product called ChatGPT Plus. That sounds like a steep price tag but if you write code or do any complex reasoning for a living it could well be worth it.\nLet’s break down the math. The average software engineer in the United States get’s paid $105,000 a year. In Seattle and SF that can be 2-3x higher. But let’s just use the average. The new ChatGPT Pro subscription costs around 2% of their annual salary. To be worth it the product needs to give them back the same amount in productivity.\n\n$105,000 salary\n250 days worked a year (50 weeks of 5 days, 2 weeks for vacation)\n$420 daily salary (105000/250)\n\nIf a dev makes $420 a day writing code, then paying $2,400 a year for ChatGPT Pro equals out to just under 6 days of their salary. The new models offered in ChatGPT Pro are the best in the world at coding. So it’s safe to say using them will make a developer more productive. If using the service saves you one hour a week in time spent coding then it will have already paid for itself.\n\n200x12 = $2,400\n2,400/420 = 5.7 days\n5.7x8 = 46 hours\n\nSo by using the service you are technically getting 6 more hours of yearly productivity for free, which is about $315. Using the new service is actually putting more money in your pocket.\nIf you think paying buku bucks for services like this is stupid, well you are just not using them correctly. AI will become the most powerful form of leverage in the world, so not using it to its max today will only hurt you tomorrow. OpenAI said they will also be rolling out more features to the pro service going forward. This could include access to the next generation of models like GPT5 and access to their upcoming agents capabilities. Once these come out you will gladly fork over thousands a month for them, because they will allow you to get 2-3x more done. So paying $200 a month now might just be a steal."
  },
  {
    "objectID": "posts/2024-11-27-diy-ozempic/index.html",
    "href": "posts/2024-11-27-diy-ozempic/index.html",
    "title": "DIY Ozempic For 90% Less Cost",
    "section": "",
    "text": "I’m not a doctor, I don’t play one on TV. This is not health advice. Make your own damn decisions.\nIt’s the day before Thanksgiving. Or as I like to think of it, the day before the month+ holiday eating binge. While it truly is the most wonderful time of the year to eat, it’s not the most healthy. I think everything should be moderation, including moderation. So I like to go nuts myself during this time of year and eat whatever. For most American’s though, going nuts on food for a month straight is not a good idea. Most are obese, and have been that way for decades.\nThankfully new weight loss medicines like Ozempic have risen in popularity over the last few years. These drugs contain a hormone called GLP-1. Which is something your body produces naturally. It’s what makes you feel full after eating a meal. If your body consumes this hormone from Ozempic, it will naturally make you feel full quicker. Which makes you less hungry. Which makes you eat less. Causing you to lose weight. Seems like a miracle drug right! Unfortunately there are no free lunches in the world of biology. It always comes with a catch. For drugs like Ozempic the catch has to do with it’s price ($1,000 a month) and potential impact on major organs like your thyroid, pancreas, and heart. But if you’re 100 pounds overweight, your organs are already suffering, so taking the drug knowing the tradeoffs is probably worth the risk. Being overweight is bad, mmmk?\nThe rise in these drugs has definitely caught my attention, so I’ve been studying them closer. They have the potential to not just make people more healthy, but their downstream impacts can be enormous. Planes may not need as much fuel on flights, because people onboard will weigh less. McDonald’s stock might drop 50% because people aren’t impulsively buying milkshakes after work each day. Even gyms could close down, because people can now look good while only taking a pill/shot versus doing 45 min on the stairmaster every day. There will be countless effects. Both good and bad.\nWhat if you could get all of the good effects from Ozempic without having to pay an arm and a leg while also avoiding the harmful side effects? I think I’ve come across a combination that will naturally produce more GLP-1 in your body, while also being 1/10th the price of Ozempic. Here is the stack.\n\nPrebiotic Shakes by Supergut\nGLP-1 Probiotic by Pendulum\n\nI’ve written about Supergut in a previous post. It’s a fiber supplement that contains prebiotic fiber that has been proven to stimulate the release of GLP-1 in your gut. Pendulum is a company specializing in supplements for your gut microbiome. Their GLP-1 probiotic contains living bacteria that creates the precursors to GLP-1. I first learned about Pendulum from Kevin Rose, and the science behind it looks legit.\nI think the combination of both could be a game changer. The prebiotic fiber from Supergut will help give your microbiome the food it needs to thrive. And the probiotics from Pendulum will help develop the right strains of bacteria in your gut that help produce more GLP-1. This one two punch could be very powerful. I don’t think they’ve been studied in combination before, so this could definitely be some bro science, but my research so far tells me it’s a good combination.\nThese two products combined cost around $100 a month, compared to $1,000 for Ozempic. So you don’t have much to lose by trying these products. At a minumum your bathroom visits will become 10x more productive and your microbiome will be better off. Why not try it and see how it might help you fight the good fight during this upcoming season of countless food temptations?"
  },
  {
    "objectID": "posts/2024-11-15-hacking-blood-sugar/index.html",
    "href": "posts/2024-11-15-hacking-blood-sugar/index.html",
    "title": "Hacking My Blood Sugar With Fiber",
    "section": "",
    "text": "For years, I’d heard about continuous glucose monitors (CGMs) , mostly from health enthusiasts on podcasts. These small devices, which attach to your upper arm, measure blood glucose (blood sugar) every five minutes and send the data to your phone via Bluetooth. Originally developed for diabetics who need to monitor their blood glucose throughout the day, CGMs are now becoming popular among non-diabetics as a tool to track metabolic health.\nMetabolic health refers to how well your body processes and manages energy from food. Good metabolic health means your body efficiently converts food into energy without putting too much strain on organs like the heart, pancreas, liver, and muscles. People with optimal metabolic health typically have stable blood sugar, good insulin sensitivity, healthy cholesterol levels, and normal blood pressure. Poor metabolic health can increase the risk of conditions like diabetes, heart disease, stroke, and obesity.\nKey indicators of metabolic health often include:\n\nBlood sugar levels - Balanced glucose levels without spikes or drops.\nBlood pressure - Normal range without requiring medication.\nCholesterol and triglycerides - Healthy levels that reduce risk of cardiovascular issues.\nWaist circumference - Indicates abdominal fat, linked to higher risk of metabolic disorders.\nInsulin sensitivity - Ability to regulate blood sugar without excessive insulin release.\n\nUsing a CGM to track blood glucose (BG) is a great way to gauge metabolic health. When your BG stays within a healthy range (70-140 mg/dl), your body functions optimally. Repeatedly leaving that range can lead to serious consequences. Over 1/3 of Americans have either diabetes or prediabetes, with many unaware of their condition. Heart disease is the leading cause of death, accounting for 20% of all deaths in America. These illnesses, with the exception of type 1 diabetes, can be mitigated with good metabolic health, which involves maintaining stable BG throughout the day.\nA few months back I purchased a CGM device, made by Dexcom, to start tracking my BG. I did it for a few reasons. A family member of mine has type 1 diabetes, so I wanted to see how my own BG levels look throughout the day. While also test driving to device before recommending it to them. I also wanted to understand how certain foods impact my blood sugar. I’ve heard on podcasts that you’d be surprised what kinds of foods impact your BG. Which is different for everyone. Some foods people thought as healthy, like bananas, might send their BG spiking. Finally, if I could get a better handle on my BG, I could also get better control of my energy throughout the day. When your BG spikes above the normal range, it will eventually come crashing down. Going far lower than the normal range. This spike and fall of BG leads many people to have energy crashes throughout the day. This saps their energy, while also giving them cravings to eat more food. This is a vicious cycle. And one that I wanted to break.\nI purchased the CGM from Dexcom and slapped it on my arm. It didn’t hurt to apply and after a few minutes I forgot it was even there. One device lasts for about 15 days so every couple of weeks you have to switch it out for a new one. I now got a stream of data every five minutes alerting me to my BG level. For a data nerd like me it was awesome. I quickly become some sort of mad scientist. Eating things at random and seeing how my BG changed in the next 60 minutes. The findings for my body were very straight forward. If I ate processed junk, my BG would spike. If I ate unprocessed whole food, my BG would stay stable. As long as I didn’t eat processed food, my BG would stay in the healthy range. There is just one problem though, I love junk food.\nEverything in moderation, including moderation. Right? I usually try to eat clean +90% of the week. But I am human, and love to pig out every once in a while. I have been known to subscribe to the practice known as “Faturday”. Where for one day of the week I go absolutely nuts and eat everything in site. This kind of barbell strategy, either eating 100% clean or 100% junk, has kept me in balance over the years. It keeps me on track of my health goals without being too much of a stickler. The only downside of this approach is the massive hits my BG takes on the days I let loose. Once I got the CGM I could see the roller coaster ride my BG would take on these cheat days, and I could match my mood and energy level with where my BG was at the time. This made me think more about ways I could control my BG when I wanted to treat myself, without putting myself at risk of the consequences of poor BG management.\nThere are a few tried and true ways to control your BG. The first, and most boring, is to just eat healthy unprocessed foods. A second way is to incorporate movement into your day after eating. Simply going on a walk after eating can have a profound impact on stabilizing your blood glucose. The same goes for more intense movement like lifting weights or cardio. These two strategies are strong, but they didn’t help me when I wanted to treat myself. I quickly learned that it’s hard to outrun a donut you just ate. That’s when I came across a third way to control BG.\nA favorite podcast of mine is called All In. It features four men who are titans in Silicon Valley. From founders of Paypal to early Facebook and Google employees, these guys know their stuff. During one episode I learned about a supplement company one of the members of the podcast invested in, called Super Gut. The company is all about fiber, which sounds like the worlds most boring supplement, but in reality has countless benefits. It turns out that 95% of Americans do not consume enough fiber, which is around 30 grams a day. So we all need to look for more ways to consume more fiber.\nSuper Gut uses prebiotic fiber found in foods like green bananas, oats, and potatoes. This preobiotic fiber is the best kind of food to provide your gut microbiome, since it can’t be converted into fuel for your body like other macronutrients. Fiber is bulky, so it easily expands to fill your stomach, causing a feeling of being full. It also slows your digestion, which can prolong the feeling of being full, and has the additional benefit of regulating your BG. Finally, there have been recent findings that show consuming the right types of fiber makes your body release hormones like GLP-1. This is the main ingredient behind booming drugs like Ozempic, used for weight loss. Why pay thousands for synthetic GLP-1 when you get get more of it naturally with fiber?\nEach serving of the Super Gut shake contains 15 grams of fiber, which is around 50% of the fiber we need each day. That’s a crap ton of fiber. To put that into perspective, you would have to eat about 5 medium sized potatoes (with skin on) to get the same amount of fiber. 10 seconds of consuming a fiber supplement compared to 20+ minutes of force feeding potatoes. The choice is easy.\nHow do I use it? First and foremost I use Super Gut to get the daily recommendation of fiber. Usually consuming a serving at the start of each day. Going from maybe 10 grams of daily fiber to 30+ is life changing in more ways then one. Bathroom trips get a lot more interesting. Any previous problems in that department gets solved immediately. What has surprised me the most is its impact on my blood sugar when I eat junk. When I treat myself on a Faturday, I first consume fiber, then eat whatever crap comes my way. Ice cream went from spiking my BG over 200 mg/dl to now only going up slightly to 130-140 mg/dl. Which is within the healthy range. That small spike is short lived, compared to previous Faturday’s where my BG would spike for hours at a time. Now when I eat pizza, my responds like I’m eating chicken and rice. I usually have a 2-3 window after consuming fiber where I can eat junk without spiking my BG. It’s truly mind blowing.\n\n\nNow this doesn’t mean you should go out and eat whatever you want, as long as you consume some fiber beforehand. Don’t do that. But when you do want to treat yourself on occasion, you can do so knowing that it won’t wreck your metabolic health. Try it for yourself! I’m sure there are other supplements besides Super Gut out there that can do the same thing. Regardless of whether you’re trying to pig out or not, getting more fiber into your life will improve your life in more ways than one."
  },
  {
    "objectID": "posts/2024-11-08-cancelled-streaming/index.html",
    "href": "posts/2024-11-08-cancelled-streaming/index.html",
    "title": "Why I Cancelled All My Streaming Apps",
    "section": "",
    "text": "Last weekend I was in Dallas for a close friend who was getting married. It was the Friday before the wedding and I had some time to kill. So I walked around Dallas for a few hours. That’s my favorite way to explore the city. I eventually got hungry and settled on a restaurant in uptown. After ordering I sat outside to enjoy the nice Texas weather. Which funny enough looked the exact same as a winter in Seattle (clouds, dark, rainy).\nA waitress slowly walked up behind me, looking concerned. She asked what I was doing, making sure I was ok. I wasn’t doing anything crazy, just looking out across the city and thinking. It took me a while to realize that she was concerned because I wasn’t engrossed in my phone. Checking X, Instagram, or TicTok. After realizing this I told her that I was trying to stare at my phone less and was enjoying the new city. Maybe she thought I just got dumped and was looking grimly out on this cruel world. Or that I just got fired and was re-evaluating my life choices. Overall she was concerned because I was not looking at my phone to pass the time. Now that definitely concerned me.\nWeeks before this incident I made conscious decision to unsubscribe and delete all TV/Movie streaming apps. Things like Hulu with live TV, Netflix, MAX, Paramount+. I had them all. And they all had to go. Why would I do something so crazy! The explanation is simple my dear Watson. I wanted to create more opportunities to be bored in my life. Call me crazy, but it’s working out better than I’d imagined.\nWhen I was a college freshman at Kansas, circa 2012, I was part of a fraternity. As a rule, all pledges of the fraternity were not allowed to have any form of TV or video game console in their room. I like to think it was to give parents peace of mind that they would be studying more than goofing off. But the real goal was something entirely different. When 40 pledges can’t pass time by watching TV they have to do other things. Together. Instead of binge watching a show alone in your room you now have to be a real human being and get together with other pledges to pass the time. My older brother, who was two years older and in the fraternity, knew this when he bought me a dart board as a present before school started. He said I would use this more than anything else in my room. You know what? He was absolutely right. That dart board become a sort of water cooler that allowed other pledges to come to my room and pass the time by playing darts. In addition to darts, we did all sorts of crazy things to pass the time in between classes. One week it was playing cards. Another it was creating playlists on Spotify. We even created a sacred coffee ritual before we all went off to study each night. This included a special coffee blend, singing an old Folgers jingle from the 90s, and maybe a little dance if we were feeling up to it. My mom even knew what to expect when I bought a desk chair before school started. She said I needed to get the cheapest one because she knew with absolute certainty that it was going to get destroyed through activities like chair racing and playing a form of floor hockey while riding a chair. That was a common activity when my older brother was a pledge. Safe to say, that chair did not make it through my freshman year unscathed. All of those activities may sound dumb, but that’s how each pledge got to know each other. Going from strangers to brothers in a short six months.\nCutting out all streaming apps from my life allowed me to help recreate the kind of boredom I felt in college to do things that were actually good for me. Instead of watching a show for four hours like a zombie. There are simply too many good shows and movies out right now. All at our fingertips waiting to be watched. This creates an opportunity to always be entertained, always have something to occupy your mind. That kind of stimulation cannot be good for us, and that’s why I’m trying to remove it from my life.\nHere are a few other reasons why I wanted to remove TV from my daily equation:\n\nTime suck: You can easily watch TV for eight hours straight on a Saturday. That is time you just never get back. Robert Green has an idea called “alive time vs dead time”. Alive time is doing things that are creative and beneficial to your life. Things like reading, writing, painting. Dead time is essentially a waste and time you never get back. Things like watching TV or scrolling social media. Alive time gives you life, while dead time takes it away from you.\nNews is bad for you: Most news out there is about terrible things happening in the world. That’s what creates the most viewers, which bring in ad revenue to stay on air. Humans are not designed to know everything bad going on in the world at once. It’s a form of sensory overload that leaves us more anxious and pessimistic about societies future. It’s a form of entertainment junk food.\nTV meeting your social needs: I think an unnoticed impact from TV is that it fills a lot of the socialization needs us humans crave every day. A feeling of being with friends or loved ones. Instead of going out to hang with a friend, you might just stay in and hang out with your friends that live inside a TV show. An introvert like me can only have so much social stimulation in a day, and watching too much TV leaves less opportunities to socialize with real people.\nTV to forget problems: Ever had a bad day at the office and come home to watch a funny sitcom to relax? We’ve all done it. And in moderation it’s probably an ok thing to do. But if you’re doing that every day after work, then you are just numbing bigger problems in your life. TV won’t fix that, it just delays the problem for another time. Which can eventually blow up in your face. Taking that salve away gives us no choice but to face problems in our lives. Giving us the time to actually do something about versus just get by another day.\n\nNow my dear reader, you might still think I’m insane. What do you do with all of this newfound boredom? Aren’t people who say they don’t own a TV just pretentious do****s? Here’s how I now spend my time:\n\nBooks: When TV is out of the question, you can easily crush 2+ books a week. Books become your constant companion. I’ve recently fallen back in love with fiction books. For over a decade I’ve been strictly reading non-fiction at a clip of 1-2 books per month. Now I read multiple non-fiction and fiction books at once. And it’s amazing. You know how people say the book was better than the movie? They are usually right. What was a 2.5 hour movie is actually a 300+ page book that takes over 10 hours to read. Talk about bang for your buck.\nMusic: Sitting down to solely listen to music has been lost on me the last decade. As a teenager a good song could change my life, but in recent years I haven’t ventured out of my old favorites. Now I sit down and explore new artists, albums, and deep cuts from my top bands. This is now the most relaxing thing I do. Nothing fixes a rough day like a good playlist.\nSports: The biggest drawback of no TV is no live sports. I love everything soccer and Kansas City sports. So whenever I want to go watch a game I now have to get off my butt and go watch it somewhere. This now forces me to be in the company of others, most often close friends. This leads me to my next point.\nGet out of the house: TV is the ultimate ball and chain tying you to your house. Without it I now have pressure to get out into the world and explore what’s going on in my community. Art shows. Farmers markets. Anything and everything going on in the world is now at my disposal since I literally have nothing better to do. I now can’t say no to things because if I did I would just be bored at home. This creates more serendipity in my life.\nTV in the wild: Coming across a movie or show when I’m out and about is now a treat. I enjoy it 100x more because it doesn’t happen that often. Last weekend my Alaska Airlines flight had free movies I could watch. I chose a new Guy Ritchie movie about WW2 and it blew my mind. I wouldn’t have appreciated it in the same way if it was the third movie I watched on a lazy Sunday afternoon.\n\nWho knows, maybe in a months time I will relapse and sign back up for all of the streaming apps I quit. But so far it’s been a good experience. Maybe try it out yourself for a few weeks and see how it might improve your life. I think we all need to engineer more opportunities to be bored. Because when we’re bored, that’s when good ideas come to us and when serendipity can change your life forever."
  },
  {
    "objectID": "posts/2024-10-30-newsletters-stink/index.html",
    "href": "posts/2024-10-30-newsletters-stink/index.html",
    "title": "Newsletters Stink",
    "section": "",
    "text": "Too Many, Too Fast\nWhen I open my email, I cringe. I have hundreds of unread emails. Not from coworkers about important work. But instead from the many newsletters that every influential person on the internet now has. When I signed up for these newsletters, I was excited. Insights delivered right to my mailbox! What was initially 3-4 newsletter emails a week quickly turned into over two dozen. For someone who likes the practice of inbox zero this caused unneeded stress.\nIn order to offload these emails into another place for reading, I signed up for services like ReadWise. Which has a great reading app you can have newsletters automatically get sent to. This was helpful at first, but eventually turned back into email. When I look at my “feed” on the Reader app, I have 121 unseen newsletter emails. That’s insane. I cannot read all of these, nor do I want to. Something needs to change.\n\n\nWhy Newsletters Stink\nNewsletters serve a good purpose. They are a way to push out content and build audiences. Which can lead lead to $ for their creators. These good intentions come with some drawbacks. Let’s call them out.\n\nPush, no Pull: Newsletters get thrusted upon us once we sign up. Some get sent every single day. This kind of push process creates build ups of unread articles in our mailboxes. The bottleneck is us. It’s our time. We don’t have a lot and we want to make the most of it. Ideally we would instead prefer a pull approach where once we read one newsletter, we then get the next one. Instead of having 20 stacked up in our inboxes.\nNew is Not Always Better: The “news” in newsletter is what makes them bad. I think we all know that consuming news is not good for us. It raises our anxiety and is a form of entertainment junk food. Instead of always getting the freshest take on business or technology today, you’d probably get more insight from articles written years ago that are timeless. Reading a Paul Graham essay on building a business written 10 years ago is probably better than getting a newsletter from someone today who has only build one business. Also there is pressure for creators to push these out on a regular cadence, which means quality goes down. Pushing 4 newsletters out a month will most likely lead to less quality then taking the time to write one newsletter over 1-2 months. New and more is not always better. We need more evergreen content.\nArchives Are Graveyards: Have you ever gone looking through old newsletters from your favorite creator? Probably not. Only the brave of heart dive into the dumpster of a creators website and dig around for little nuggets of gold. There might be a newsletter from five years ago that could change your life, but to find it you might have to dig through dozens of others about the NFT market in 2021 that no one cares about anymore. Again, see my point ini #2. We need a better way of finding the best evergreen content.\nNo Personalization: Once you sign up for a newsletter, you are at the mercy of reading all of them. Regardless of the content in each one. This might be ok if the newsletter author writes about one thing, but a disaster when they are wide ranging. For example, let’s say Joe Rogan had a newsletter (he doesn’t but let’s pretend). You might love all of the content around culture, politics, and comedy but absolutely hate it every time he talks about the UFC. Maybe you don’t like MMA, and could care less who Connor McGregor may fight next. We need a better way of finding the best content relevant to our interests.\nThey Replaced Blogs: People that used to write blogs now write newsletters. What a shame! You could in theory just take the posts you used to publish to your blog and now just send out via newsletter. But people don’t do that anymore. What a bummer. We need more blogs and less newsletters.\n\n\n\nWho Does Things Right\nYou know who does things right? YouTube. They have it all figured out. Instead of seeing a list of videos in chronological order, they show you what videos you might be most interested in. Not just from channels you follow but for any channel. You can interact with videos you like by watching them multiple times, liking the video, or subscribing to the channel. This ensures you see more content like it. Or if you hate the video you can give the thumbs down, click on “show me less of this”, or even block the channel entirely. With a few smart features they fixed every thing that is wrong with other forms of content like newsletters and blogs.\nWhen I open up my YouTube homepage on the app, I see a sea of great options. The first video recommended on the page is for a podcast I like that just posted a new video two hours ago. As I scroll I see content from last week, last month, even as far back as two years ago. These are all things that interest me, not just the latest content from channels I follow. I even see content from channels I’ve never heard about, but is chosen based on my interest in other liked videos. This is why YouTube dominates.\n\nSource - Variety\n\n\nMake Blogs Great Again\nIdeally there should be a service that is like YouTube but for blogs. Anyone who has a blog can connect their site to the service, and posts are algorithmically provided to the user based on what the user likes. It won’t be what causes the most outrage or reactions, but instead what makes them entertained or more informed. The site would have no news, just evergreen content that’s just as relevant next year as it is now. Sites like Medium try to do this today, but their biggest flaw is that they live behind a walled garden. Meaning a blog owner has to only post their stuff to Medium and not on their own site. This is not ideal. Since you now have to play by Mediums rules and all of your content now lives on a platform you don’t control. You could argue YouTube is the same but YouTube is the only way people consume user submitted video content today. There is no second choice. Compared to blogs where anyone can host their own for free via GitHub (that’s what I do).\nThere could also be amazing AI features built into this new blog recommendation service. What if all blog posts on the site were indexed and you could ask questions about the blog via GenAI? Imagine having a conversation with Paul Graham, powered by every piece of advice he has ever written about building businesses or technology. It would basically be a mentor on demand. Always ready to bounce ideas off of or think through hard problems.\nThe world has too many new(s) things. We need to make old things, like blogs, new again."
  },
  {
    "objectID": "posts/2024-10-03-ts-fundamentals-eda/index.html",
    "href": "posts/2024-10-03-ts-fundamentals-eda/index.html",
    "title": "Exploratory Data Analysis For Time Series",
    "section": "",
    "text": "This post is part of a larger learning series around time series forecasting fundamentals. Check out the learning path to see other posts in the series.\n\nExploratory Data Analysis Overview\nExploratory data analysis, or EDA, is the process of understanding the patterns in your data before you train any machine learning model. It’s the first step in the data science lifecycle. Blindly throwing your data into a model before understanding it yourself is a recipe for disaster. If your data is garbage, you will create a garbage forecast. Gargbage in, garbage out.\n\n\nEDA for Time Series\nApplying EDA to time series data is a unique process, different from every other kind of data used in machine learning. Here are the building blocks of good time series EDA. Click on each to explore further.\n\nShape of the Data\nTime Series Decomposition\nAutocorrelation\nMissing Values, Outliers\nExternal Regressors"
  },
  {
    "objectID": "posts/2024-09-25-ts-fundamentals/index.html",
    "href": "posts/2024-09-25-ts-fundamentals/index.html",
    "title": "Thoughts On Time Series Forecasting Fundamentals",
    "section": "",
    "text": "The ability to create forecasts about the future is a superpower. Machine learning (ML) models take this to another level by increasing forecast accuracy while reducing the time spent creating the forecast. Using ML might seem a little scary at first. You may not know where to start. Type “intro ML course” into Bing and you’ll probably get millions of links. Which ones are good? Why do they cost $5,000? That’s why I wanted to create a gentle introduction to time series forecasting with ML. Where I start from first principles and work our way up to shipping forecasts in production. The intent is to cover the core theory of ML forecasting, and less on the code itself. The code can come later, but anyone who consumes the outputs from ML or helps train ML models needs a strong understanding of how this process works.\nThe sequence of what you learn is just as important as what content you learn. I have developed a learning path that takes you from absolute beginner and slowly adds new concepts until you’re a forecasting master! Please click on each link in order to get up to speed, or skip around to any topic you want to dive into again. Don’t try to read this all in a day, take your time, take notes, and maybe even paste some of the posts into your favorite AI tool to quiz yourself on the topics. Happy learning!\n\nOur Learning Journey\n\nWhat’s a time series?\nExploratory Data Analysis\n\nShape of the Data\nTime Series Decomposition\nAutocorrelation\nMissing Values, Outliers\nExternal Regressors\n\nData Cleaning\n\nMissing Values, Outliers\nBox Cox Transformation\nStationary\n\nUnivariate Models\n\nARIMA (in progress)\nExponential Smoothing\nSimple Benchmark Models\n\nEvaluation Metrics\nFeature Engineering\n\nDate\nTarget Variable\nExternal Regressors\n\nMultivariate Models\n\nLocal Models\nGlobal Models\nHyperparameter Tuning\nLinear Regression\nDecision Trees\nRandom Forest\nGradient Boosting (XGBoost, LightGBM)\nFeature Selection\nMutlistep Horizon, Autoregressive\n\nModel Training Lifecycle\n\nTrain/Test Splits\nTime Series Cross Validation\nEvaluation Metrics\n\nHierarchical Forecasting\n\nStandard Hierarchy\nGrouped Hierarchy\nClustering\n\nCombining Models\n\nSimple Averages\nWeighted Averaged\nEnsemble Models\n\nPrediction Intervals\nModel Interpretability\n\nModel Specific\nModel Agnostic\n\nGlobal Methods\nLocal Methods\n\n\nForecasts in Production\n\nParallel Computing\nModel Training and Serving\n\nAppendix\n\nML Term Glossary"
  },
  {
    "objectID": "posts/2024-08-30-weekend-reads/index.html#articles",
    "href": "posts/2024-08-30-weekend-reads/index.html#articles",
    "title": "Weekend Reads (8/30/24)",
    "section": "Articles",
    "text": "Articles\n\nElon Musk’s Foundation Page: money talks, wealth whispers\nThiel Fellowship App: should be the application for most jobs\nPrompt Engineering Fundamentals"
  },
  {
    "objectID": "posts/2024-08-30-weekend-reads/index.html#videos",
    "href": "posts/2024-08-30-weekend-reads/index.html#videos",
    "title": "Weekend Reads (8/30/24)",
    "section": "Videos",
    "text": "Videos\n\n24 Controversial Truths About Success and Failure on Modern Wisdom\nOne of a Kind Guy on My First Million\n\nPart 1\nPart 2\n\nIncentive Masterclass on My First Million\nWHOOP CEO on Diary of a CEO\nNew OpenAI Developments on AI for Humans"
  },
  {
    "objectID": "posts/2024-08-30-weekend-reads/index.html#tweets",
    "href": "posts/2024-08-30-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (8/30/24)",
    "section": "Tweets",
    "text": "Tweets\n\nAmazon CEO Automates Tech Debt with AI"
  },
  {
    "objectID": "posts/2024-08-30-weekend-reads/index.html#products",
    "href": "posts/2024-08-30-weekend-reads/index.html#products",
    "title": "Weekend Reads (8/30/24)",
    "section": "Products",
    "text": "Products\n\nHeadband That Puts You to Sleep"
  },
  {
    "objectID": "posts/2024-08-30-weekend-reads/index.html#songs",
    "href": "posts/2024-08-30-weekend-reads/index.html#songs",
    "title": "Weekend Reads (8/30/24)",
    "section": "Songs",
    "text": "Songs\n\nMona Lisa by Lil Wayne"
  },
  {
    "objectID": "posts/2024-08-12-ml-fcst-faq/index.html",
    "href": "posts/2024-08-12-ml-fcst-faq/index.html",
    "title": "FAQ on Machine Learning Forecasting",
    "section": "",
    "text": "Over the last few years I’ve presented to hundreds of people outside of Microsoft around how we approach machine learning (ML) forecasting within Microsoft finance. A lot of great questions were asked during those conversations. In this post I want to highlight some of the most commonly asked questions and my take on answering them. Hopefully this can be a quick reference for anyone ML curious or want to deepen the ML work being done on their teams. Use the table of contents to skip around to the sections you’re most interested in. If there are any topics missing please reach out to me via LinkedIn and I will continue to update this post."
  },
  {
    "objectID": "posts/2024-08-12-ml-fcst-faq/index.html#toc",
    "href": "posts/2024-08-12-ml-fcst-faq/index.html#toc",
    "title": "FAQ on Machine Learning Forecasting",
    "section": "FAQ Table of Contents",
    "text": "FAQ Table of Contents\n\nData\n\nGetting high quality historical data\nUsing third party data\nNew info not found in the training data, one time events\nHandling outliers\n\n\n\nTechnical\n\nInterpreting the black box\nWhat models to use, should we use deep learning\nWhat programming language or framework to use\nUsing large language models like ChatGPT to forecast\nWhat level of accuracy is good\n\n\n\nHumans\n\nHow to make forecast owners accountable for the ML number\nBuilding trust in the ML forecast\nWho owns the ML creation process\nHow to get started with ML\nGoing from ML as a triangulation point to replacing manual human forecasts\nBuilding data science talent in finance"
  },
  {
    "objectID": "posts/2024-08-12-ml-fcst-faq/index.html#data",
    "href": "posts/2024-08-12-ml-fcst-faq/index.html#data",
    "title": "FAQ on Machine Learning Forecasting",
    "section": "Data",
    "text": "Data\n\nGetting high quality historical data\nGarbage in, garbage out. That’s probably the most common saying in the world of ML. If you cannot get high quality historical data, then there is no easy way to produce an accurate ML forecast. You can’t work your way around noisy or incomplete data. If your data is messy, hard to find, and comes from 10 different systems, then ML is not something you should be worried about. A nice bow on top of a pile of crap is still a pile of crap. Fix your data first, then focus on ML after.\nBack to Table of Contents\n\n\nUsing third party data\nYour company’s business is most likely impacted by greater market forces outside of your control. For example the health of the economy or how much money customers have to spend. Adding data from outside of your company (third party data) as features in your ML models is a good way to improve forecast accuracy, while also being able to describe what outside forces impact your business the most. Some data is freely available, while others have to be paid for. What data you use is up to your domain knowledge of your business.\nFree Data\n\nFRED\nWorld Bank\nInternational Monetary Fund\nUnited Nations\nGoogle Trends\n\nPaid Data\n\nIDC\nTrading Economics\n\nBack to Table of Contents\n\n\nNew info not found in training data and one time events\nIf you are changing the price of your product in three months, this is most likely going to impact your future revenue forecast. But if you have never changed the price of your product before, then a ML model cannot learn from that information. For a model to learn from one time events, it needs to be present in the historical data a model is trained on. The future must always learn from the past.\nBack to Table of Contents\n\n\nHandling Outliers\nOutliers in your data can have a bad impact on your future forecast. It can hurt accuracy or give false signals of the future. There are many ways to deal with outliers. The easiest way is to use statistical methods to identify and remove them. Treating them as a missing value you can then replace. Sometimes outliers are more subtle, and take a trained eye to spot them. This is where the domain expertise of a person comes into play.\nBack to Table of Contents"
  },
  {
    "objectID": "posts/2024-08-12-ml-fcst-faq/index.html#technical",
    "href": "posts/2024-08-12-ml-fcst-faq/index.html#technical",
    "title": "FAQ on Machine Learning Forecasting",
    "section": "Technical",
    "text": "Technical\n\nInterpreting the black box\nThis one is a toughie. When a person creates a forecast manually, most often using excel, someone else can come into that financial model and trace cell by cell exactly what’s going on. Going from input data, to assumptions, to the formulas that create the final output. This is the kind of exactitude that allows accountants to sleep peacefully at night. Everything is in order and everything is perfectly understood. But we are not accountants. This is finance. We have to make calls about the future that are uncertain. We can never have 100% certainty that something is going to happen. If that’s the case then your company is doing something illegal. Get out now!\nThe biggest paradigm shift someone has to make with machine learning is giving up this total control of the forecast. And in essence take a leap of faith. Machine learning models are enigmas. Sometimes akin to magic. The cannot be perfectly understood because the capture non-linear relationships in data that a human never could. That’s why we have them, because they can work better and faster than our human brains in some tasks.\nThere are ways to understand these models, but they cannot be perfectly audited like a manual forecast done in excel. Instead they have to be interrogated. Not like a criminal wanted for war crimes but more like a therapist talking to their patient. There is no way for a therapist to know exactly what’s going on inside of their patients mind. But they can start to ask questions that can give clues into what’s going on and see why the person has made past decisions in their life.\nThe best resource I know on explaining ML models is Interpretable Machine Learning by Christoph Molnar. Here’s a quick overview of the method’s described in the book.\n\nModel Specific: These are models like linear regression or a single decision tree where we can see exactly what’s going on under the hood. The model structure is more like an excel formula we can trace step by step. But because they are easy to explain, they are simple in nature and may not produce the most accurate forecast. That is the tradeoff between having a forecast that can be easily explained versus having a forecast that is the most accurate. The more accurate the forecast, the more likely you cannot explain it perfectly.\nModel Agnostic: For more advanced models like gradient boosted trees and deep learning, we can use methods that approximate what’s going on under the hood of a complex model. This is when we have to act like a therapist and start asking questions to our model and see what answers it gives back. There are two ways of doing this.\n\nGlobal Interpretability: This uses methods that can see overall what’s impacting the model the most. For example you can see what input variable (or feature) is the most important in the model overall.\n\nLocal Interpretability: This uses methods to see what’s going on for each individual forecast data point. For example you can see for a specific future forecast what’s impact that number the most.\n\n\nThe last thought I’d leave you with is this. Have you ever not used ChatGPT because you couldn’t get an explanation of its answer? For example maybe you asked it to help you write some excel formulas to format dates. Do you trust the output it gave you because the excel formula was correct or because it could tell you exactly how it came to that conclusion? What if the explanation it gave was made up or a hallucination? If the excel formula is correct you would still use it right? Even the CEO of OpenAI, Sam Altman, cannot explain how models like GPT-4 think under the hood. But hey, ChatGPT was still the fastest growing product of all time. Sometimes imperfect interpretability is ok. But maybe your CEO is still demanding an explanation of the forecast numbers, so this is still a hard problem to solve in finance.\nBack to Table of Contents\n\n\nWhat models to use, should we use deep learning\nPeople are always attracted to the hot new thing, and I can’t blame them. New is exciting. When it comes to ML forecasting, new isn’t always better. The newest trend in ML is all about deep learning. Or models that can mimic the human brain. While they work really well for things like analyzing photos and text, using them on tabular data (aka excel data) hasn’t always worked out well. That’s why I recommend using deep learning last.\nHere are the models to use first, then you can always resort to deep learning if need be.\n\nUnivariate Models: These are the simplest forecasting models. Since they only need one variable, hence the name univariate. If you want to forecast revenue, then you only need historical revenue and you’re off and running. They run extremely fast and can scale to millions of data combinations without spending too much on cloud compute. Here are a few popular ones.\n\nARIMA: An ARIMA (AutoRegressive Integrated Moving Average) model predicts future values in a time series by combining differencing (modeling the difference between periods), autoregression (using past values), and moving averages (using past forecast errors). It’s the most common univariate model in the forecasting game.\nExponential Smoothing: Forecasts future values in a time series by applying decreasingly weighted averages of past observations, giving more importance to recent data points to capture trends and seasonal patterns.\nSeasonal Naive: Predicts future values by simply repeating the observations from the same season of the previous period, assuming that future patterns will mimic past seasonal cycles. Don’t sleep on this one! You’d be surprised how often it comes in handy as a good benchmarking model to compare with more complicated models.\n\nTraditional ML Models: After trying univariate models, it’s time to try more traditional machine learning models. These are models built specifically for tabular data, or data that can live in a SQL table or excel spreadsheet. These models are multivariate, which allow them to incorporate outside variables as features to improve their forecast accuracy. They require more handling than a model like ARIMA, since they need feature engineering and proper time series cross-validation. Multivariate models can also learn across multiple time series at the same time, instead of being trained on just a single time series like a univariate model. Here are a few common multivariate models.\n\nLinear Regression: Predicts future values by fitting a line to the historical data, where the line represents the relationship between the dependent variable and one or more independent variables.\nXGBoost: Predicts future values using an ensemble of decision trees, boosting their performance by iteratively correcting errors from previous trees, resulting in a highly accurate and robust prediction model.\nCubist: Predicts future values by combining decision trees with linear regression models, creating rule-based predictions that incorporate linear relationships within each segment of the data for greater accuracy.\n\n\nBack to Table of Contents\n\n\nWhat programming language or framework to use\nShould I use python? But what if I learned R in my statistics class? What about stata or good ole javascript? Ask 10 data scientists what programming language to use and you’ll probably get 10 different answers. There is no right answer. It’s kind of like arguing what hammer to use when building a house. Shouldn’t we just be worried about getting the house built? And make sure we don’t screw it up?\nHere is my take on the ML language wars. You should use both, python and R. Different languages offer different things, and depending on the task you might want to use one over the other. Often it’ll boil down to specific open source software that might only be available in one language but not the other. So over the course of a long data career it’s probably a good idea to be competent at both.\nWith that said, if you could only learn one programming language, learn python. That’ll get you the farthest the fastest in terms of useful knowledge to get building. I think in a few years these debates will go away, because large language models (LLM) will come and save the day. Imagine writing code in your favorite language, using the packages you like, and then have a LLM take your code and translate it into blazing fast machine code that runs like a race car. So it won’t matter if you only know one language or the other. That’s where I hope we’re headed.\nBack to Table of Contents\n\n\nUsing large language models like ChatGPT to forecast\nWith the explosion of large language models (LLM) that can do everything from tell dad jokes to write production grade code, can’t we just offload all forecasting work to them? In essence you could, but I think it’s kind of overkill and leaves a lot to be desired. LLMs take in text, and spit out text. They are good with words, but not that good with numbers. They can’t perform on par with a calculator, because that’s not how they were designed. So you can’t just copy a table from excel, give it to ChatGPT, and hope to get next quarters revenue forecast. It might give it to you, but it won’t be a good forecast. It might even make something up.\nThe more sensible route to take is to have the LLM write code that can create forecast models and have it execute it for you. For example use the code interpreter feature in ChatGPT to have it take your uploaded CSV file and execute a bunch of python code against it to get a final forecast. This kind of workflow might be good for initial exploration, but it shouldn’t be used in a production setting where you need an updated forecast each month. It’s kind of like needing a place to sleep and each night you build a new house from scratch, only sleep there one night, then the next night build another house from scratch. Having LLMs produce code on the fly each time can lead to inconsistent results that may not be reproducible when you ask ChatGPT to do it again. You could take the code from the first forecast iteration, save it, and either run it yourself each time or give it to ChatGPT as a prompt. But even then you are still missing out. Some automated forecasting packages, like the one I own called finnts have over 10,000 lines of code. So a LLM will most likely not be writing that much code to answer one prompt, and if they did having you trying to save and manage that code is out of the question.\nI think LLMs can still have some part in the forecasting process. They are useful before and after the actual model training is done. For example you can have code interpreter analyze your data for things like outliers or help in running correlation analysis to see what variables could help improve your forecast accuracy. Then you will take those learnings and run the forecast outside of the LLM environment, or you could have the LLM call an outside function to kick off a forecast. This is called “function calling”, where a LLM can use an outside tool (most often calling an API via code) that can accomplish the task a LLM cannot (like getting the current weather). LLMs can then be used after the forecast process is ran to then analyze the final forecast outputs. Tools like code interpreter can make charts and analyze historical back testing accuracy.\nMaybe in the future the LLM can help explain how the final forecast was created, or better yet answer questions about forecast variance once actuals land in the future. But using it to actually create predictions or train models itself is still a tall order. With that said there are new time series specific LLMs that are being released, so this is an exciting area to watch closely.\nBack to Table of Contents\n\n\nWhat level of accuracy is good\nThere is no right answer. It all depends on the specific data you are using and how it compares to non-ML approaches you have done previously.\nA common metric in time series forecasting is the MAPE error metric. MAPE stands for mean absolute percentage error. This is very similar to variance to forecast percent metrics you might already use. Think of it as the average percent error (as an absolute value) across every forecasted data point. There are plenty of other metrics out there, but MAPE is a good one since it’s not dependant on scale and percents are easy for anyone to wrap their head around.\nA MAPE of 5% means that on average, your forecast is off plus or minus 5%. So the closer to zero the better. If you ask any finance person what kind of MAPE they want for their forecast, almost all of them will say less than 1%. Or more than 99% accuracy. Think of accuracy as the inverse of MAPE. Maybe the ML forecast has a MAPE of 10%, but your previous manual excel model ended up having a 20% MAPE. The ML forecast has a 50% reduction in forecast error, even though it’s still in the double digits. So evaluating ML is always relative to what kind of performance you got with other forecast methods probably done in excel.\nWith that said, here are some rules of thumb I use when running my own ML forecasts.\n\nDaily and Weekly Data: Less than a 10% MAPE is terrific. But even MAPEs as high as 30% are ok, because often we might sum up a daily forecast to a monthly or quarterly level. And when evaluated at that aggregate level you may start to have drastically improved MAPEs.\nMonthly, Quarterly, Yearly Data: There are a few levels of accuracy that I think are good at this level. Again, it’s all relative to what kind of performance you had before using ML.\n\nLess than a 5% MAPE is good\nLess than a 3% MAPE is great\nLess than a 1% MAPE is amazing\n\n\nEven going from a 3% MAPE to a 2% MAPE is a big achievement. Because it’s a lot harder to do that than to go from a 15% MAPE to a 10% MAPE. It’s still the same level of improvement but once you get closer to zero the MAPE improvements seem to happen on a log scale. Where each percentage point you reduce continues to get harder as you get closer to zero.\nBack to Table of Contents"
  },
  {
    "objectID": "posts/2024-08-12-ml-fcst-faq/index.html#humans",
    "href": "posts/2024-08-12-ml-fcst-faq/index.html#humans",
    "title": "FAQ on Machine Learning Forecasting",
    "section": "Humans",
    "text": "Humans\n\nHow to make forecast owners accountable for the ML number\nHaving a ML model automate your forecast process is great, until you have to tell your CFO why your quarterly forecast is off by xyz%. “It was ML’s fault” is not the right answer. But it’s hard for a human to be on the hook for work that a machine did for them. Maybe the ML forecast came from another engineering team, so the final owner of the forecast maybe had no part in creating the forecast. This can make accountability hard.\nIn order to get people on board with transitioning more of their work to ML, you need senior leadership buy in. People at the top need to be invested in the promise of ML and the tradeoffs it might provide. Since you cannot audit a ML forecast like you can with an excel model (tracing cell by cell) there is an essential leap of faith that has to happen.\nIn addition to having senior leadership buy in to improve accountability, being able to adjust the output from ML can also help. Financial analysts can impart their domain expertise about their business by making small manual adjustments to the ML forecast. That way ML can get you 80% of the way to a completed forecast, and a human might make adjustments on that final 20% to finalize the forecast. Having the forecast owners be “humans in the loop” of a ML process adds a sense of ownership, which can then improve accountability.\nA final way to improve accountability is through the Ikea effect. Which states that we value things more if we are involved in making them. The best way for this to work is if the final forecast owners create their own ML forecast through self-serve tools that abstract away the complex of ML and allows analysts to upload data and get back forecasts with a few clicks of their mouse. This is exactly what we did in Microsoft finance and it has worked out well so far.\nBack to Table of Contents\n\n\nBuilding Trust in the ML forecast\nBuilding trust in the ML forecast is the hardest part of using ML. There are three ways that have worked well in helping non-technical financial forecast owners warm up to and fully transition to using ML to forecast.\n\nHistorical Accuracy: The best way to get someone on board with a forecast into the future is to show them how well a similar forecast has performed in the past. It’s not a perfect proxy for future performance but gives the end user a good idea of how well a ML model is performing. Common performance metrics to use are MAPE (mean absolute percentage error) and RMSE (root mean squared error). MAPE is similar to existing variance to forecast calcs already done by financial analysts so it’s a good error metric to convey past performance. When calculating historical performance, aka back testing, it’s a good idea to create hypothetical forecasts for the most recent periods of your data. Making sure you cover enough historical time to ensure your ML model is robust. For example with a monthly forecast. You might want to just forecast the next 3 months into the future. If you have 5+ years of historical data you could have 4 historical back tests where you train a model and produce a 3 month forecast for each of the last 4 quarters in your historical data.\nPrediction Intervals: Understanding the uncertainty of the future forecast can also help build trust. Think of prediction intervals as upper and lower bounds of the future forecast that convey a certain percent of probability that the future value will land in between these bounds. Common prediction intervals are 80% and 95%. For example, you might have a future forecast of $100 for next month, with a 80% prediction interval of $80 and $120. This means there is an 80% chance that the actual value of next month will be between $80 and $120. So the tighter the prediction interval, the better. Having an prediction interval that’s +-5% of the forecasted value gives more comfort to the end user than one that’s +-30%. Just make sure that the end user of the forecast knows that those upper and lower bounds are not a “bull case” and “bear case” of what could happen in the future, like they might be used to in other financial modelling. But instead just a way to capture the uncertainty of the future prediction.\nInterpretability: Finally another good way to build trust is being able to explain how a future ML forecast was created. You will not be able to perfectly explain it like you can an excel model (by tracing through it cell by cell). But you can use methods to poke and prod a model to see what might be going on under the hood.\n\nBack to Table of Contents\n\n\nWho owns the ML creation process\nBeing at Microsoft we are lucky to have engineering teams (sometimes called IT in other companies) that sit inside of finance and report to the CFO. This allows us to have software engineers who are solely focused on improving the lived experience of each of the 5,000+ employees under the CFO.\nOn those engineering teams there are people like myself who work on ML. Most of my time is spent helping produce and evangelize ML forecasting. My team has built two unique ways of how ML gets created and consumed by financial analysts.\n\nStandard API: This allows other engineering teams to call upon ML on demand to get a forecast. Without needing to set up the infrastructure to train and serve models. This process works hand in hand with other forecast centralization efforts we’ve done.\nSelf-Serve UI: On the other end of the spectrum is to have tools that allow any financial analyst to produce their own forecast with a few clicks of their mouse. This allows us to scale ML to every single person in finance, without needing to rely on an engineering team to help produce a forecast. The tool we built for this is called Finn, and you can learn more about it here.\n\nBack to Table of Contents\n\n\nHow to get started with ML\nGetting the ball rolling can seem like an insurmountable task. Thankfully it’s been easier than ever to get started with ML. Check out how we got started back in 2015.\nAnother way to make quick wins is to use something pre-built off the shelf that’s ready to go. Thankfully you can use it for free! We have open-sourced our ML forecasting code into a standard package called finnts. You can have your engineers take this code and quickly get up and running with ML forecasts within a week. It condenses almost 10 years of trial and error in Microsoft finance down to code that works at scale in production. Try it!\nBack to Table of Contents\n\n\nGoing from ML as a triangulation point to replacing manual human forecasts\nThere’s a saying that 80% of ML projects never make it into production. This definitely applies to ML forecasting, but in a different way. Better put, 80% of ML forecast projects never make it past the triangulation point phase.\nThere are three distinct phases of adopting a ML forecast.\n\nInitial Development: Going from initial idea to a first prototype. Trying to see what a ML forecast would look like and if accuracy is good or not.\nTriangulation Point: Using a ML forecast every month or quarter alongside your existing manual forecast process. ML is another data point you “triangulate” with the actual forecast you will be using. So ML is nice to have but not required.\nBaseline Forecast: Using a ML forecast to replace your manual process. Where now the first step in your forecast cycle is to produce the ML forecast and build on top of it. Using the ML forecast as the initial “baseline” foundation. You have now “burned the ships” of your manual process never to look back. You are in it to win it with ML.\n\nIt’s easy to go from phase 1 to phase 2. With tools like finnts you can get a new forecast up and running in less than a day. But most ML forecasts go to the triangulation point stage and die. It’s kind of a bummer but a fact of life. Here are some tips to break through the resistance and get all the way to phase 3.\n\nHave a timeline. Finance teams are busy. There is always more to do in the time allotted to do it. So having deadlines helps move work along. Instead of a “one day we will switch to ML” attitude you need a deadline like “we will switch to ML within the next two quarters”. This instills a sense of urgency and gets things moving. I’ve seen ML projects stall for 2+ years because there wasn’t a timeline set in place. An answer like “we’ll move to ML sometime this fiscal year” is not good enough. Keep pushing until a deadline is agreed upon, and do all you can to stick to it.\nGet leadership buy in. The quickest way to change a financial analysts behavior is to have them hear it from their boss. Getting senior leadership buy in helps with the ongoing issue of prioritization. If the leaders make it a priority then every person on their team has no choice but to make it a priority. Things start to move a lot faster once that happens.\nIf accuracy is a concern, establish a benchmark to beat. If you ask any forecast owner what kind of forecast error do they want, they will probably say something like “less than 1% error”. Meaning 99% accuracy. This is all well and good, but if the previous manual forecast process had a forecast error of 10% then having ML be 9% or less is already a win. Make sure you compare the approaches and agree beforehand on switching once a certain benchmark is beat.\nSqueaky wheel gets the grease. Constant contact with the end ML forecast user helps speed things up. I can’t tell you how many times I’ve seen finance teams come to me, over the moon excited about using ML to revolutionize their team. Only to have that same team fizzle out their ML usage over the next 2 months. ML projects take time, and momentum with business partners can be lost quickly. Something I’ve realized is simply staying in contact with them, even as little as 1-2 times a month, helps move people from ML curious to ML power users rather quickly. It may seem like you might be bugging them too much but keeping ML at the top of their mind will help in prioritization.\n\nBack to Table of Contents\n\n\nBuilding data science talent in finance\nHiring technical talent into the finance org is no easy feat. Here are a few challenges you have to overcome.\n\nLack of defined career path. A data scientist working in finance will most likely not work for a data scientist manager. If they are lucky they might work for a software engineering manager, but I haven’t seen that happen too many times in Microsoft finance. This makes it hard to attract strong data science talent and get them to stick around for many years. If they keep getting promoted there are not many other jobs they can rise up to and take. The more successful they get, the more likely they will have to move on outside of finance to grow their career. And that’s ok. So make sure you cultivate the career path of your data scientists, even if that means leaving your team.\nLack of mentors. Because there aren’t a lot of data scientists in finance, there is often no one they can go to for help. For example, I’ve never had a boss review my code. Not even once. So to know if what I built is any good I needed to go consult the opinions of others. Sometimes these can be other data scientists in the finance org, but often I’ve had to go outside of finance to other mature data science teams to get advice and feedback on my work. So for your data scientists, make sure they are connected to a larger data and AI community at your company. And if there isn’t one, ensure they are finding community outside of the company at conferences and online spaces.\nLack of data engineering. Garbage in, garbage out. If there isn’t good data, no amount of data scientists can create value for you. Even worse, don’t rely on a data scientist to fix your data problems first before they start training models. Building robust data infrastructure in finance should be the job of data engineers, not data scientists. Those two jobs cannot often be combined into one, because they require different skill sets and even different mindsets. So make sure you get your data house in order first before even thinking of getting data scientists.\n\nLack of complexity. Sometimes data scientists can get lost in building the coolest thing possible, instead of building something that helps the business. For example, a data scientist might use the latest hot thing from the world of deep learning to create a solution for the business problem, when in reality a simple linear regression could have taken 1/10th the time and give you the same results. Also when explaining their work to the end user in finance, they might dive right into the technical aspects of the project, and all of the cool statistics they performed to save the day and create something awesome. This kind of explanation works well with other data scientists, but it will scare off every finance person I know. One of the main jobs of a data scientist in finance is to abstract away all of the complexity of the craft and make their work simple to understand for the average finance person. It may feel like the data scientist is “dumbing down” all of their hard work, but this part is crucial. If the end finance user cannot understand what they are being given from the data scientist, then they will not trust it, which means they will not use it. So gone are the fun technical presentations showing off all of the bells and whistles of their work. Now all they need to do is explain in simple terms if what they built worked, maybe how it worked under the hood (simplest explanation possible), and how the finance end user can leverage the work to do their job better.\n\nOk, now that we know the challenges we must overcome to grow strong data scientist talent, here are some more tips around making data scientists successful in finance.\n\nBorrow, rent, then buy. If you are starting from zero. Then I recommend trying to borrow a data scientist from elsewhere in your company. Then once you’ve gotten a few projects off the ground rent some outside vendors/contractors to keep the work going. Then once you’re really kicking butt either hire those vendors as full time employees or go out and hire other data scientists in full time positions.\nStart small. If your team has the budget to hire 5 data scientists, maybe just hire two for now and see how it goes. Then ramp up over time. When new technology is booming, like the current generative AI wave, people have the tendency to overinvest. So start small and grow over time. Having too much work for your 1-2 data scientists to do is way better than having to lay off 1 of your 5 data scientists because of budget cuts that always seem to happen every few years.\nConnect with mentors outside of finance. Make sure your data scientists have a community of like minded people whom they can share their work with and get feedback. This will most likely be with data scientists in other departments like marketing, sales, and product teams.\nAlways answer the “so what” question. Once you hear this concept you cannot unhear it. A boss of mine recently started asking me one simple question after I told him about a project I recently worked on, “so what”? I recently moved our model training infrastructure to a new Azure resource, so what? I’m working on improving the model ensembling techniques used in our ML forecasting models, so what? I’ve been asked to present to some outside customers about the work we do in Microsoft finance, so what? It’s a simple question but also a powerful one. If you cannot answer the so what question, you probably should go work on something else. Often the answer to the so what question boils down to a metric or number to measure impact of the work. If you cannot convey that to your manager, then you might be in trouble of making yourself busy instead of productive. Always be able to answer the so what question with data or customer testimonials.\nHire for more applied ML, not research ML. It’s better to hire someone who has spent their time putting ML solutions into production than someone who just got their PHD in deep learning. There is a difference between applied ML and research ML. If you do hire a PHD to work on ML stuff, they most likely will not be happy, this goes back to the lack of complexity I called out earlier. We’re not building rockets in finance, often times we don’t even need deep learning to get the job done. So this kind of work may not entice the PHD to stick around long. Instead hire someone who can bring data science to business expertise, which brings me to my next point.\nConvert financial analysts into data scientists. The perfect scenario is to take existing financial analysts, who are already experts in the business, and either give them tools that turn them into data scientists without the code. Or actually have them take the red pill and go all the way down the ML rabbit hole, becoming a true data scientists. This fixes most problems. Your data scientist already has strong domain expertise in the business, they can already speak like a normal person to business partners without all of the technical jargon, and they’ve only ever known applied ML so they won’t waste time trying to build custom models from scratch. In my opinion it’s a no brainer and something that should become the standard going forward.\n\nBack to Table of Contents"
  },
  {
    "objectID": "posts/2024-06-28-life-circles/index.html",
    "href": "posts/2024-06-28-life-circles/index.html",
    "title": "Inner Circles of Life",
    "section": "",
    "text": "One of my siblings recently got married and had a baby. As they were living this new life, myself and other family members realized something. They now had less time for us. Instead of coming around to the standard family events like they used to, they now had to “squeeze us in” between other things going on with their life. This become the most apparent around Christmas time. My sibling and their family now only stopped by for a few hours around the Christmas holiday, and on Christmas day this meant only seeing them for 2-3 hours. In previous years we would have been together nonstop. This broke my Mom’s heart. She could no longer be with all of her kids 24/7 during holidays and major life events.\nIt was during this Christmas that I realized something. My sibling had to deprioritize us for their spouse and eventually their new child. And that’s perfectly ok. They now had a new top priority in life, and myself and other family and friends are now lower down on the list. A new inner circle of priorities formed for them, and myself and others were no longer in it. We had to make due with this new reality and understand it could get worse in the future. This kind of deprioritization also happens with friends too. It just gets hard to prioritize people in your life as new people come into it.\nThe list of life priorities is something I like the call the “inner circles of life”. Sounds fancy but it’s just a list of what you truly prioritize and make time for in life. Your inner most circle is your top priority. Then circles form around and outward. With each new circle, your priorities of things in that circle drops. Let’s see how that changes as we get older.\n\n\nAge 0-1\n\nMom\n\nWhen you’re born, the only person that exists is your mother. Dad, who’s Dad? What’s a Dad? You have no idea. The only person you recognize and bond with is your Mom. No one else comes close.\n\n\nAge 1-6\n\nMom\nDad\nGrandparents, Aunt/Uncle\n\nOk, now Dad comes into the picture as the second circle of your life, but Mom still holds the inner most circle. You also now get to know there are other people who love you unconditionally. These people are called Grandparents and they’re awesome. All they do is give you hugs and tasty food, life is great when they are around. Aunts and Uncles come into the picture to. They kind of look like Mom and Dad but smell different and always seem to get out of having to change your diaper. Lucky them.\n\n\nAge 7-12\n\nParents\nFriends\nSiblings\nRelatives\n\nYour parents still keep the top spot, but now they are more like a combined unit. Not just separate people but a singular force. Now they are telling you to do chores and keep an eye on your brothers and sisters. Who the heck are they? These people look like me but have different interests and personalities. And hey, they’re mean! We don’t get along that well. We always fight. So they are definitely farther down on the list. Same goes with Grandparents and other relatives. We still enjoy seeing them, but we’d rather hang out more with these people who go to school with you. People your own age who have the same interests as you. Who you see every day at school. People who give you their pudding cup just because you looked hungry. These people are your friends, and they are the best. They now take a higher spot on the list. Life starts to revolve more around your friends and less around your other family members.\n\n\nAge 13-25\n\nFriends\nRomantic Partners\nSiblings\nParents\nRelatives\n\nThis is what I have come to call the “dark decade”. A time where you kinda suck as a person. You might hate your parents. You might hate school. You might just hate the world. Don’t worry that’s just your emo phase, it’ll pass. Friends become your top priority in life. School might be on that list too but it most likely won’t come until your college years, so I’ve left it off the list for now. There are now others who might have initially looked like friends. But who you now see in a different light. They smell nice. They have shiny hair. They seem cool. You’d like to get to know them more. Maybe even kiss them right on the mouth. Yes, these are people I call romantic partners. Hopefully they do not occupy the top spot on the list. You know the saying, bros before ____ right? Wrong. At times a girlfriend, boyfriend, or someone you admire (who might not know you even exist) could take the top spot. You might even go to a specific college on the other side of the country for them. This dark decade is where mistakes happen. Where you fail a lot. Do dumb things. Thankfully all of this dumb stuff happens when you’re at a school of some sort, so the mistakes are temporary. These mistakes are things you learn from and grow into a better person (hopefully).\n\n\nAge 22-26\n\nJob\nRomantic Partners\nFriends\nParents\nSiblings\nRelatives\n\nNow you’re in the real world. And you need money to live. Work has now become your top priority. Don’t believe me? How many of your friends took jobs in different cities after college graduation? Did you break up with your romantic partner because you both were headed to different cities to start your careers? Yup, it happens. It’s ok to have your job be the top priority. You need to establish yourself at this stage in life. Build a career that’s going somewhere. You also might be in a serious relationship with someone who smells nice and has shiny hair. Lucky you. This person might even move in with you. Your roommate used to be your best friend. Now you’ve kicked them out for a different kind of best friend, one you may want to spend every day of the rest of your life with. Friends are still high up on the list, but they might live in a different city now. You take trips to visit them, but you only have so many vacation days off work. You also need to balance that with time to see your parents and other family members. Now you have too many life balls in the air to juggle, so some might get dropped. When was the last time you called your Grandparents? Call them now.\n\n\nAge 25-35\n\nSpouse\nJob\nParents\nSiblings\nFriends\nRelatives\n\nBy now you might have married your roommate who smells nice. Where you live and sometimes changing jobs are based on this other person. They are now the center of your world. Having a job and good friends are still high up on the list, but those fall by the wayside compared to your new spouse. You are now out of the “dark decade”, so naturally your parents and siblings become fun again. You genuinely enjoy hanging out with them. And miss them when they’re not around. Now that you’re starting to see your family more, and your job responsibilities are heating up, all of a sudden you can’t see your friends +3x a week. Some of your friends might move away, back to their or their spouse’s hometown. It happens, and it kinda sucks. But that’s life. They’re also dealing with their own priorities just like you.\n\n\nAge 28-38\n\nKids\nSpouse\nJob\nParents\nSiblings\nFriends\nRelatives\n\nThe most beautiful thing in the world happens. A baby comes into your life. Everything else is meaningless. The only the thing that matters is making sure this child is happy and healthy. Your kids become your inner most circle. Everything else gets bumped down the list of priorities. You now realize you need baby sitters, because eventually you might have to return to work. This is where parents and siblings come in. Now you’re closer than ever with them. Friends visit you, but that weekly poker game or all night weekend party is now out of the question. You have someone to feed and someone to love. Life is beautiful. Work falls on the list too. Late night and weekend working sessions become harder. Now you have to tradeoff time at work with time with your child. This becomes a hard choice that has been argued thousands of times by smart people. The answer is hard. Thankfully by now you have built up some career capital. Meaning you can use your seniority and expertise at your company to guard your time more. Only work on the biggest impact items instead of the grunt work you did at the start of your career. They say you can have everything in life, just not all at once. Choices have to be made. Just know the tradeoffs of each one. Make sure you define your own definition of success in life. Which can be truly anything. No one has the right answer. No one has it all figured out.\n\n\nAge 35+\n\nKids\nSpouse\nParents\nSiblings\nFriends\nJob\nRelatives\n\nHopefully by your mid to late thirties you are able to find a nice smelling person. Maybe even raise some rugrats. By this time you have also built up a lot of career capital. Maybe this means you can now do your job how and when you’d like. Maybe your job is still demanding most of your time. Good news, things will only get worse. More people will ask for your time. Pull you in a thousand directions. Ask you to do more. Then more. Then once you get all of that work done, your reward is more work. Congrats! Maybe you tell yourself you’ll retire early. That way you can then have more time to spend with your family. Like I said before, I don’t know the right answer. I don’t think anyone does. So again I’ll say that life is all about priorities and tradeoffs. How you define success in life could be different than someone else. And that’s ok. I think in a perfect world your family and friends are still high on the list as you get older. You can spend more time with them, and maybe less time on that job. Or maybe your job fulfills you immensely. You know the work contributes to making the world better. So working more is a worthwhile tradeoff. Do Presidents of nations feel bad that they cannot spend time with their family every day? Maybe they do, maybe not. For me I don’t think retiring to a beach for the rest of my life is any fun. I’d like to be like Charlie Munger, working into his 90s. I assume he wasn’t working nights and weekends in his 90s. Instead he still worked, but also made time for other relationships in his life. Again, there’s no right answer.\n\n\nFinal Thoughts\nWhen we’re on our deathbed. I don’t think any of us will say “I wish I worked harder”. The quality of our life boils down to the quality of our relationships. The quality of your relationship with your kids, parents, siblings, friends, and extended family. Relationships at work can serve a purpose too, but it’s hard for a job to replace these other types of relationships. You can have everything in life, just not all at once. Everything comes with a tradeoff. In the end, define what success looks like to you and have zero f#### for anyone else who tells you how to live your life. Prioritize accordingly."
  },
  {
    "objectID": "posts/2024-06-12-msft-ml-fcst-journey-1/index.html",
    "href": "posts/2024-06-12-msft-ml-fcst-journey-1/index.html",
    "title": "Microsoft Finance ML Forecasting Journey: Part One",
    "section": "",
    "text": "This is a multipart series:\n\nPart One\nPart Two\n\nPart Three\n\nEver wonder how Microsoft Finance got started with machine learning? It didn’t just happen overnight. It started small and grew from calculated steps. In this post and a few others I want to tell the journey of how we got started. Gather round children! It’s story time.\n\nParadigm Shift\nIn the summer of 2015 AI and machine learning (ML) weren’t terms you’d hear every day. Maybe you’d hear the word “big data” being thrown around business circles but no one had a clue what it meant. There was a lot of data being captured about our world. Somehow we could “mine” the data to get some value out of it. No one really knew.\nEarlier that year, something interesting happened at Microsoft. A new product called Azure Machine Learning was officially released. The service allowed anyone to start mining their data up in the cloud. You could train models and serve them through APIs. It was basically magic. Unfortunately in finance, those words meant nothing. To a Microsoft finance worker the term “train a model” meant training the new employee on building excel models. Everything was done by hand and with care. Especially forecasting our financial statements. The CFO of Microsoft, Amy Hood, thought differently. What if we could use the new product to improve some of the manual work we did in finance? Could we have these models be trained to forecast our business? It was a tough question. No one in finance at the time was really qualified to answer it. She had to go ask the expert.\n\n\nGetting The Ball Rolling\nAmy went to the legend himself. The head of Microsoft’s cloud, Scott Guthrie. King of the cloud and wearing red polo shirts. She wanted to see if Scott’s engineering team could help finance build machine learning models. Allowing finance to forecast the business. Thankfully Scott said yes and lent a few data scientists to help the finance org get off the ground with ML.\nThe big ticket item was forecasting revenue. Instead of starting small with one specific area we started very high level. Amy wanted a quarterly global revenue forecast by each of Microsoft’s major products. This forecast could be used internally to compare against the manual forecasts. Which are created by sales finance and product finance teams. The ML forecast could either confirm or contradict these bottoms up forecasts made by humans. Allowing finance to either adjust their forecasts. Or make sure they know why they are different than ML.\nThe results were strong. The ML forecast was around 1%-2% off on average, compared to the manual human forecast error of 2%-4%.\n\n\nKeeping The Ball Rolling\nThe game officially changed. The finance team could now just rely 100% on ML going forward right? Not so fast! Who would keep training these models? What if we wanted to forecast at a more granular level? Scott’s data scientists couldn’t help forever. To fix this Amy had to hire some data science talent. People who knew what they were doing. Like the engineers on Scott’s team.\nHiring your first data scientist is a hard thing to do. Creating a career path for them in a non-technical team like finance makes it harder. As a first step, a team of vendor data scientists were hired. This was enough help to take the work done by Scott’s team and keep it going. Even expand it to other areas. The hope was to eventually turn a vendor data scientist team into a team of full time employees.\n\n\nLessons Learned\nGoing from zero ML work to your first forecast solution takes hard work and perseverance. Here are a few lessons Microsoft finance learned when starting out.\n\nBorrow -&gt; Rent -&gt; Buy\nInitially data scientists were borrowed from other teams at the company. Then they were rented from outside companies as vendors. Then finally once a strong data science practice was established after a few years, full time employees were hired. Many were vendors who turned into full time employees. This process was slow, but allowed finance the time to make sure a data science practice and career path could be built.\n\n\nWhat’s the biggest opportunity?\nThe biggest opportunity to forecast with ML was revenue. We could have spread ourselves thin and tried to do the entire income statement. But we knew revenue was the hardest to forecast. So that’s where we started first.\n\n\nStart at the top, work your way down\nStarting first with worldwide revenue allowed finance to get good results without getting too deep into the weeds first. If we wanted to get an accurate daily forecast down to the sku level, that would have taken forever. Instead we started big and then eventually worked our way down. This process may not initially replace the manual forecast work being done. But it starts to get others in finance comfortable using ML in the decision making process. After finance leaders got used to seeing these ML forecasts, we could then start working on more granular forecasts that could replace more manual work.\n\n\n\nFinal Thoughts\nOk now you know how the ML ball got rolling in Microsoft finance. Before reading this article you might have thought we had this amazing ML kick-off with millions invested in the space. We definitely did not. Instead we started small in areas that had the highest ROI and worked our way from there. If your company is just starting out on your ML journey, I suggest you do the same. Small, incremental change can compound into enormous impact over the long run. That’s the kind of change that lasts."
  },
  {
    "objectID": "posts/2024-05-31-time-series-deep-learning/index.html",
    "href": "posts/2024-05-31-time-series-deep-learning/index.html",
    "title": "Time Series First Principles: Deep Learning Last",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the tenth and final principle of a good time series forecast, deep learning last. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nThe Shiny New Thing\nDeep learning is the latest frontier in the field of machine learning. It’s a subset of machine learning that uses neural networks with many layers (hence “deep”) to model complex patterns in data. These neural networks are built to resemble how human brains work. There are a lot of different types of deep learning models. Even the latest large language models from OpenAI are using deep learning techniques.\nSince deep learning is getting all the hype nowadays, it can be tempting to go straight to training deep learning models when starting a new forecasting project. This is a bad idea. While deep learning can be very effective, there are many reasons I’ll call out in this post that make deep learning hard to use for forecasting projects. You can still use a deep learning model in your forecast, but I recommend exhausting all other avenues before trying deep learning. Let’s dive into why deep learning should be tried last.\n\n\nReasons To Use Deep Learning Last\n\nLack of Quality Data\nDeep learning can work well if you have thousands, or better yet millions, of observations in your historical data. In my job we might be trying to forecast a monthly time series for a single product, but only have the last three years of historical data. That’s 36 data points. This lack of data is a common problem at my company, where new products are released constantly (meaning they have limited data) and our business shifts so often that even historical years from six years ago may not be relevant to where our business is headed. If you don’t have tons of historical data, it becomes very hard to train an accurate deep learning model.\n\n\nExpensive Hardware\nDeep learning requires millions of matrix algebra calculations. Think of it as multiplying two sets of tables together. Regular computers have CPUs (central processing unit), which are designed for sequential processing. Even if you have 10+ CPUs on a computer, it will take a while to crank through the millions of matrix operations needed to train a deep learning model. GPUs on the other hand, are specialized to have thousands of cores and parallelize matrix operations effectively. They were initially built for video game graphics, hence the name graphical processing unit, but in recent years have stumbled across a new use case in training deep learning models. This is why Nvidia is the third most valuable company at the time of this writing, since they are the leading manufacturer of GPUs. These GPUs are hard to build, making them expensive to buy or rent from a cloud provider. Because they are expensive to use, they make it harder for anyone to start using them. With non-deep learning models you can start training them on your local computer, but to train a deep learning model you either have to camp out at Best Buy to purchase a Nvidia H100 or jump through a lot of hoops with a cloud provided to rent one by the minute. The juice may not be worth the squeeze.\n\n\nBigger Black Box\nDeep learning models are often harder to interpret than other machine learning models. This is due to them having up to billions of parameters (model inputs) that are abstracted between multiple layers. This means one layer of parameters feeds into another layer of parameters. There are ways to interpret the inner workings of these models, but they are often just an educated guess. Can anyone explain how a deep learning model like GPT-4 came up with its answer? Not likely.\n\n\n\nWhat to Use Instead\nMaybe I’ve convinced you to not chase after the shiny thing and try something non-deep learning first. What model should I use instead? Here is what to try first. Once you have tried these models and evaluated their performance, you can then see if the juice is worth the squeeze with deep learning.\n\nUnivariate Models\nThese are models that only need one variable, historical values of what you’re trying to forecast. Univariate models are more statistics than machine learning, and are custom built for time series. They train very fast and are tuned for each specific time series in your data. One weakness is some of these models cannot take in outside data in the form of features. With that said they are a terrific starting point for any new forecasting project. Often they can get you the required accuracy needed, and if they don’t they can serve as the benchmark to beat with other models. Here are a few common univariate models to try first.\n\nARIMA: An ARIMA (AutoRegressive Integrated Moving Average) model predicts future values in a time series by combining differencing (modeling the difference between periods), autoregression (using past values), and moving averages (using past forecast errors).\nExponential Smoothing: Forecasts future values in a time series by applying decreasingly weighted averages of past observations, giving more importance to recent data points to capture trends and seasonal patterns.\nSeasonal Naive: Predicts future values by simply repeating the observations from the same season of the previous period, assuming that future patterns will mimic past seasonal cycles.\n\n\n\nTraditional ML Models\nAfter trying univariate models, it’s time to try more traditional machine learning models. These are models built specifically for tabular data, or data that can live in a SQL table or excel spreadsheet. These models are multivariate, which allow them to incorporate outside variables as features to improve their forecast accuracy. They require more handling than a model like ARIMA, since they need feature engineering and proper time series cross-validation. Multivariate models can also learn across multiple time series at the same time, instead of being trained on just a single time series like a univarite model. Here are a few common multivariate models.\n\nLinear Regression: Predicts future values by fitting a line to the historical data, where the line represents the relationship between the dependent variable and one or more independent variables.\nXGBoost: Predicts future values using an ensemble of decision trees, boosting their performance by iteratively correcting errors from previous trees, resulting in a highly accurate and robust prediction model.\nCubist: Predicts future values by combining decision trees with linear regression models, creating rule-based predictions that incorporate linear relationships within each segment of the data for greater accuracy.\n\n\n\n\nReversal\nDo mega retail corporations like Amazon or Walmart only use ARIMA or linear regression models when trying to forecast the millions of product skus in their universe? Probably not. When the stakes are that high, and they can hire hundreds of data scientists to forecast, then they most likely build their own custom deep learning approaches that can learn from billions of data points to produce robust forecasts. With limitless resources and data, deep learning becomes easy. Assume you are not them.\nExciting startups like Nixtla have been doing great work on deep learning transformer models. These are the types of models that power products like GPT-4 from OpenAI. They built something called TimeGPT-1, which is a generative model for time series. They trained this model on billions of publicly available time series, creating the first GPT model tailored to time series. What required special hardware and tons of data to train can now be a simple API call in any programming language. This is a potential game changer and can completely change how forecasting is done, turning it more into a software engineering problem than a data science problem. Keep a close eye on this space as innovations like this can move at light speed.\n\n\nFinal Thoughts\nWhile deep learning holds great promise and can offer high accuracy in certain scenarios, it is often not the best starting point for most forecasting projects. The need for extensive data, expensive hardware, and the complexity of interpreting deep learning models make it a less practical choice compared to more traditional methods. Starting with simpler, well-established models like ARIMA, exponential smoothing, or traditional machine learning models often provides sufficient accuracy with lower costs and greater interpretability. As innovations continue to emerge, especially with models like TimeGPT-1, the landscape of time series forecasting may shift, making deep learning more accessible and practical. However, for now, prioritize simpler, more transparent models and reserve deep learning as a last resort when simpler methods fall short.\n\n\nSeries Wrap Up\nThat’s a wrap on our First Principles in Time Series Forecasting series! My goal was to walk through core concepts of creating a strong time series forecast. Instead of diving deep into code and super technical concepts, I wanted to give timeless knowledge that will serve anyone who builds or consumes time series forecasts. Hopefully you enjoyed the series and learned a lot 🤞."
  },
  {
    "objectID": "posts/2024-05-13-finnts-multistep-horizon/index.html",
    "href": "posts/2024-05-13-finnts-multistep-horizon/index.html",
    "title": "Multistep Horizon Forecasting With finnts",
    "section": "",
    "text": "TL;DR\nI’m excited to announce that we just released a new feature in our machine learning forecast package, finnts, centered around multistep horizon forecasting. It’s a mouthful to say but at a high level it helps improve forecast accuracy by optimizing models to be accurate at each period of a forecast horizon. For example, a 3 month forecast would then be optimized for the forecast in each month (period) of the future forecast.\nLet’s dive in to how multivariate modeling used to work in the package and how multistep horizon forecasting can help.\n\n\nHow It Used to Work\nLet’s use an example of a monthly revenue forecast for our business’s main product. In practice we would want more than a year of data but let’s just keep it simple today.\n\n\n\nDate\nRevenue\n\n\n\n\n2023-01-01\n120\n\n\n2023-02-01\n135\n\n\n2023-03-01\n140\n\n\n2023-04-01\n145\n\n\n2023-05-01\n150\n\n\n2023-06-01\n155\n\n\n2023-07-01\n160\n\n\n2023-08-01\n165\n\n\n2023-09-01\n170\n\n\n2023-10-01\n175\n\n\n2023-11-01\n180\n\n\n2023-12-01\n185\n\n\n\nIf we wanted to forecast the next 3 months of revenue using multivariate machine learning models we would have to do some feature engineering to get our data in good shape. This involves creating lags on our target variable. Let’s try create some lags on this data.\n\n\n\nDate\nRevenue\nRevenue_Lag1\nRevenue_Lag2\nRevenue_Lag3\n\n\n\n\n2023-01-01\n120\n\n\n\n\n\n2023-02-01\n135\n120\n\n\n\n\n2023-03-01\n140\n135\n120\n\n\n\n2023-04-01\n145\n140\n135\n120\n\n\n2023-05-01\n150\n145\n140\n135\n\n\n2023-06-01\n155\n150\n145\n140\n\n\n2023-07-01\n160\n155\n150\n145\n\n\n2023-08-01\n165\n160\n155\n150\n\n\n2023-09-01\n170\n165\n160\n155\n\n\n2023-10-01\n175\n170\n165\n160\n\n\n2023-11-01\n180\n175\n170\n165\n\n\n2023-12-01\n185\n180\n175\n170\n\n\n2024-01-01\n???\n185\n180\n175\n\n\n2024-02-01\n???\n???\n185\n180\n\n\n2024-03-01\n???\n???\n???\n185\n\n\n\nWe added some rows onto the bottom of the data, to allow us to forecast out the next 3 months after our historical data ends. That’s what we have question marks “???” for those values. We also added lags for a 1 month, 2 month, and 3 month lag. But hey, looks like we have a problem. We have more question marks for a few future months for lag 1 and lag 2. If we wanted to forecast the next three months we wouldn’t be able to use those lags, since once we get out further in the forecast horizon we start to have missing lag data.\nThis means that the smallest lag we could use would always be equal to or greater than the forecast horizon. Since our forecast horizon is 3 than the smallest lag we could use to train a model on would be lag 3. This approach can yield good results, but it removes a lot of potential signal in the data. Revenue next month is most likely impacted by how revenue grew in the current month, but if our forecast horizon is long a lot of this insight has to get thrown away before we can train models. Imagine a forecast horizon of 12. For a monthly forecast this limits our lags to 12 months or more, which is a really bummer since our business can drastically change within 3-6 months, and not using that information in our model can hurt forecast accuracy.\n\n\nHow Multistep Horizon Forecasting Works\nMultistep horizon helps fix this issue that allows us to use smaller lags while still being able to have long forecast horizons. In our 3 month forecast horizon example, we can keep the lag 1 and lag 2 features, but how the model gets trained will be different.\nIn the non-multistep horizon approach, a specific model is trained once on the data using lags that are equal or greater than the forecast horizon. When we run a multistep horizon approach, we can actually train multiple sub models under the hood of a specific model. In our 3 month forecast horizon, here’s how one model like linear regression will be trained.\n\nFor the first month in the forecast horizon, we can use all available lags. Lag 1, lag 2, and lag 3 of revenue will all be used to predict the first month.\nIn the second month of the forecast horizon, we will use lag 2 and 3 to predict the second month.\nIn the third month of the forecast horizon, we will use lag 3 to predict the third month.\n\nAre you starting to get the hang of it? With multistep horizon forecasting we can still have one model that under the hood has multiple sub models that are each optimized on forecasting out a specific part of our forecast horizon. This allows us to have greater accuracy in the first few periods of our forecast horizon. In a non-mulitstep horizon approach, we are always optimizing for the last period in a forecast horizon. If the forecast horizon is 12 months, the way we do the feature engineering and train models is optimized for forecasting out the 12th month. When running a multistep horizon approach, we instead optimize for every period of the forecast horizon.\nThis kind of approach is so crucial to forecasters in the corporate finance space. Often these financial analysts are tasked with always forecasting out the rest of the entire fiscal year, even though they might only care about the next 3 months, since they are most likely going to be re-creating a new forecast in the following quarter. Multistep horizon forecasting allows these analysts to still forecast out long forecast horizons like 9 or 12 months, while still being able to optimize for the next 1-3 months. How cool is that!\n\n\nReversal\nIf each specific model can have 2-5 sub models under the hood, the amount of time needed to train these models can multiply by the same amount. Make sure to keep that in mind if run time is a big factor in your forecasting process.\nA multistep horizon forecast may not result in a more accurate forecast for smaller forecast horizons. Some time series may have a strong relationship with a 12 month lag, but less with a 1 month or 2 month lag. This means there is strong yearly seasonality in the data. If there is not a strong relationship with 1 month or 2 month lag, then having multiple sub models optimize for each future month in a multistep horizon approach may not result in more accurate forecasts. Consider doing some exploratory data analysis to see what kind of relationships there are with historical lags of your target variable and other features.\n\n\nFinal Thoughts\nThe new multistep horizon forecasting approach in finnts allows users to create even more accurate forecasts, regardless of their forecast horizon length. If you’d like to learn more check out the official finnts documentation to see how you can use the newest multistep horizon feature!"
  },
  {
    "objectID": "posts/2024-05-03-time-series-simple-models/index.html",
    "href": "posts/2024-05-03-time-series-simple-models/index.html",
    "title": "Time Series First Principles: Simple Models Are Better Models",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the seventh principle of a good time series forecast, simple models are better models. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nOccam’s ML Model Razor\nWilliam of Ockham was a 14th-century English Franciscan friar, philosopher, and theologian. In his work he preached that for most things in life the simplest explanation is the correct one. I’ve learned this inadvertently in my life many times. For example, when I was studying for the ACT in high school a teacher told me that on the english questions it’s usually the shortest answer that is often correct. You could get a decent score just by following this one rule, even if you couldn’t read or speak english. This one tip saved my ass more than I’d like to admit, and I could read and speak english. Or so I thought.\nOften in life, just like the ACT english section, it’s usually the simplest approaches that provide the best results. You can hire a fitness coach and buy all the supplements in the world but you’ll probably get similar results following a handful of simple exercise and eating tips. The same applies in the world of machine learning. The more complexity you add to your data and models, the less likely they are going to be useful in the end. Let’s walk through how simplicity helps in all aspects of machine learning, from the data you use all the way down to models you train.\n\n\nMore Features, More Problems\nIn the world of time series forecasting, there are so many ways we can do feature engineering. Learn more about feature engineering in a previous post. A dataset containing two columns, a date and value column, can be transformed into 100+ new features. This can easily get out of hand once we add external regressors (outside variables like consumer sentiment or inflation data) and create new features from them.\nEach feature you add to a dataset hurts your model in multiple ways.\n\nTrain Time: It can slow down model training, meaning it will take longer to train the model. This may not seem like a big deal with small datasets but once you start having tens of thousands of rows in a dataset, adding a new feature can really slow things down.\nOverfitting: Adding more features can lead to overfitting, meaning your model might be very accurate on the data it was trained on but cannot generalize well to unseen data in the future. Your model will learn from the noise in the data instead of the signal.\nInterpretability: Adding more features makes it harder to explain the model’s predictions. If you can’t explain your forecast to non-technical business partners, then the forecast may not be used by anyone. I’ve seen this countless times in my work. An accurate model doesn’t help anyone if the end user ultimately wants to know how the prediction was created. More on that in this post.\n\n\n\nFeature Selection\nOne way to simplify your data before model fitting is to implement a feature selection process. It’s called selection but it’s more like removal, where we drop any features that do not contribute to a model that can generalize well to new data. Here are a few techniques for feature selection.\n\nDomain Expertise: Remove features that don’t make sense to you as a human. For example, the annual rain fall in Iceland might be perfectly correlated to Coca Cola sales in South America, but it doesn’t pass our smell test of being a factor that impacts the business. When in doubt take it out.\nCorrelation: If a feature has a strong correlation to the target variable (what we want to forecast) then we keep it in, but only after it passes our domain knowledge smell test.\nModel Specific: Some models, like certain flavors of linear regression, have built in feature selection or feature importance. We can use that info to remove features and can then retrain on any kind of ML model.\n\nThere are many other methods for feature selection, but are out of the scope of this post. The ones called out above are a good starting point.\n\n\nSimple Models\nSimplifying our data is helpful, but sometimes simplifying our models is even better. When starting a new forecast project, you might feel tempted to go out and build an advanced deep learning model, using all of the latest bells and whistles. That model may show promising results, but often a simpler model like linear regression can get the same or even better results. Models like linear regression are faster to train and have better model interpretability.\n\nWe can even go one level deeper and not use any features at all. Univariate statistical models like ARIMA or exponential smoothing are classic time series forecasting models that only need one column of data, the historical values of your target variable. That’s what makes them univariate (one variable). They have built in feature engineering under the hood that allows them to learn from historical trends and seasonality in the data, so no additional work is needed to create features. Often in time series forecasting competitions a large team of deep learning researchers can just barely beat a single person team who uses simple models like ARIMA or random forest models. More on that in a future post.\n\n\nfinnts\nMy forecasting package, finnts, has built in feature selection and other techniques to ensure simple models are built in ways that produce accurate forecasts. Check out the package and see for yourself.\n\n\nFinal Thoughts\nUltimately, the goal of any forecasting model is to provide clear, accurate, and quick results. Simpler models often meet these criteria better than complex ones because they’re easier to understand, faster to run, and just as accurate. By focusing on simplicity and minimizing inputs, we ensure that our forecasts are not only effective but also user-friendly. This approach doesn’t just save time; it makes the insights gained from the data accessible to everyone involved in the decision-making process. Simplicity, therefore, isn’t just a principle; it’s a practical strategy for better forecasting."
  },
  {
    "objectID": "posts/2024-04-23-time-series-order/index.html",
    "href": "posts/2024-04-23-time-series-order/index.html",
    "title": "Time Series First Principles: Order Is Important",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the fifth principle of a good time series forecast, order is important. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nBaking Cakes Over Making Smoothies\nMachine learning (ML) is a lot like cooking. You have various ingredients and can combine them together in clever ways to make for a tasty dish. Most machine learning approaches like classification (predicting an outcome) and regression (predicting a number) can follow a similar process to making a smoothie. We can take some data (fruits and veggies) and blend it all together inside of our model blender.\nTime series forecasting is a whole other beast. It still technically falls under the regression family tree but has to be handled very differently. Forecasting is more like baking a cake, where the order in which you do things is very important. For example, you cannot switch when you add the eggs and when you add the frosting. If you do you will certainly not be invited back to your nephew’s birthday party next year. In order to bake something tasty please follow the below guidance.\n\n\nTime Series Training\nTraining any sort of machine learning model often requires two separate historical data sets. One that is used to train the initial model, then another that is set aside to create predictions based on the initial model. We can then see how accurate the predictions were on the test data set. This ensures that our new ML model can generalize well to new and unseen data, making sure our model doesn’t overfit to the training data.\nCommon ML approaches like classification and regression don’t need a lot of sophistication when splitting up the historical data between a training set and a testing set. Often it will be split randomly. This is similar to making a smoothie. You can randomly throw in bananas, apples, spinach, and blueberries. All without having to think about the order of when you do it.\nTake the below housing data. This is a traditional regression problem. Let’s use the total square feet and number of bedrooms to predict how much the house will cost. We can randomly split 80% of the data to train the model, then hold out 20% of the data to test how accurate the model is. Randomly splitting the data ensures we get a healthy mix of different data in each split.\n\nExample fake housing data for a regression model\n\n\nSquare_Feet\nBedrooms\nTotal_Cost\nSplit\n\n\n\n\n3774\n2\n822732\nTraining\n\n\n1460\n1\n245280\nTraining\n\n\n1894\n4\n602292\nTraining\n\n\n1730\n4\n550140\nTraining\n\n\n1695\n4\n539010\nTesting\n\n\n3692\n5\n1358656\nTraining\n\n\n2238\n1\n375984\nTraining\n\n\n2769\n5\n1018992\nTraining\n\n\n1066\n5\n392288\nTesting\n\n\n1838\n1\n308784\nTraining\n\n\n\nA time series has a built in order to it. It’s said right there in the name, time. Ignoring the order based on time can have disastrous consequences, resulting in your final future forecast not being accurate. Just like baking a cake, we need to make sure how we train a model is done in the right order. When splitting a historical time series into a training set and a testing set, splitting not at random but based on time is the proper way to go. Using the oldest data as the training set and the newest data as the testing set makes sure we respect the order of our data based on time. The example table below is a made up time series of the price of one specific house. In reality we would need a lot more data to train a good time series model but just be cool for a minute and go with me on this one. The split column now has the test data set at the very end instead of randomly split across time. We can now use features like interest rates and gdp growth to help us forecast the price of this house over time. The first 9 months of data will train the model, and the final 3 months will be used to test the model’s accuracy.\n\nExample fake time series for the price of a specific house\n\n\n\n\n\n\n\n\n\nDate\nInterest_Rate\nGDP_Growth\nTotal_Cost\nSplit\n\n\n\n\nJanuary 2002\n3.43635\n1.58111\n315052\nTraining\n\n\nFebruary 2002\n4.87679\n0.03085\n314723\nTraining\n\n\nMarch 2002\n4.32998\n-0.04544\n312854\nTraining\n\n\nApril 2002\n3.99665\n-0.04149\n311865\nTraining\n\n\nMay 2002\n2.89005\n0.26061\n309452\nTraining\n\n\nJune 2002\n2.88999\n0.81189\n311106\nTraining\n\n\nJuly 2002\n2.64521\n0.57986\n309675\nTraining\n\n\nAugust 2002\n4.66544\n0.22807\n314681\nTraining\n\n\nSeptember 2002\n4.00279\n1.02963\n315097\nTraining\n\n\nOctober 2002\n4.27018\n-0.15127\n312357\nTesting\n\n\nNovember 2002\n2.55146\n0.23036\n308345\nTesting\n\n\nDecember 2002\n4.92477\n0.41591\n316022\nTesting\n\n\n\n\n\nData Leakage\nWhenever time is involved in machine learning, the probability of shooting yourself in the foot rises. This has to do with the concept of data leakage. Data leakage occurs when information from outside the training dataset is used to create the model, leading it to make overly optimistic predictions. It can also happen when we train with data that may not be available in the future when we need to create new predictions.\nIn time series forecasting we have already discussed one component of data leakage, related to splitting the data correctly based on time. Take the below table, instead of splitting properly by time the data is now split randomly. Our model can now “see ahead in time” when training, and in effect cheat when being evaluated on the testing splits. For example, for the test observation in July 2002 the model can learn from data on either side of that month. Figuring out previous and future trends and seasonality. This makes it easy to predict what the housing cost in July should be, since it has information before and after that month. With this approach our test accuracy will be a lot better than in the previous example where the splits are based on time. Future forecast performance will suffer though, since we have now trained and chosen a model that may only be good at figuring out how to extrapolate between two points, instead of trying to create predictions on unseen data in the future.\n\nIncorrect train and test splits\n\n\n\n\n\n\n\n\n\nDate\nInterest_Rate\nGDP_Growth\nTotal_Cost\nSplit\n\n\n\n\nJanuary 2002\n3.43635\n1.58111\n315052\nTraining\n\n\nFebruary 2002\n4.87679\n0.03085\n314723\nTesting\n\n\nMarch 2002\n4.32998\n-0.04544\n312854\nTraining\n\n\nApril 2002\n3.99665\n-0.04149\n311865\nTraining\n\n\nMay 2002\n2.89005\n0.26061\n309452\nTraining\n\n\nJune 2002\n2.88999\n0.81189\n311106\nTraining\n\n\nJuly 2002\n2.64521\n0.57986\n309675\nTesting\n\n\nAugust 2002\n4.66544\n0.22807\n314681\nTraining\n\n\nSeptember 2002\n4.00279\n1.02963\n315097\nTraining\n\n\nOctober 2002\n4.27018\n-0.15127\n312357\nTraining\n\n\nNovember 2002\n2.55146\n0.23036\n308345\nTraining\n\n\nDecember 2002\n4.92477\n0.41591\n316022\nTesting\n\n\n\nOk, so we know not to split data randomly when training. Another thing to watch out for is how features are used in the model. In our time series housing example, we can use date information (month, quarter, etc) along with our macro features like interest rates and GDP growth. Let’s say we follow the right approach, split the data based on time, and see that we get good results on the test data. We can now take our model into production and try to create a forecast for the future. But wait, what do we do with the future feature values of interest rate and GDP growth? This is another potential data leakage issue, where data used to train the model is not available to create new predictions in the future. You might be thinking, no problem we can just create a forecast of future interest rates and gdp growth right? Wrong. If you can produce accurate interest rate and GDP growth forecasts, then you shouldn’t be reading this post. You should instead be sitting on your own private island, watching the return on your flagship hedge fund skyrocket. See where I’m going here? If you cannot perfectly predict future values of the feature you want to use, then you should consider not using that feature. You might be able to know with 100% certainty when a holiday or special event will take place, but not even expert economists can perfectly predict interest rate fluctuations. Instead you can look at using feature lags. Where instead of using real time interest rates or GDP growth you can instead use lags of them. For example, using a 3 or 12 month lag of each feature. Using lags in this example can help prevent compounding errors in your forecast.\n\n\nfinnts\nLooking for a way to to never worry about the order of your time series data again? Have no fear because finnts is here! Ok enough with the used car salesman talk. The finnts package is something myself and other outstanding team members have built to automate all of the tedious aspects related to time series forecasting. The package can automatically handle the proper splits of your data and has build in data leakage prevention. Check out the package to learn for yourself how easy forecasting can be.\n\n\nFinal Thoughts\nRemember, order is important in forecasting. Make sure you don’t mix up your data when training models, and keep a look out for data leakage. Do this right and you just might get invited back to your nephew’s birthday party next year."
  },
  {
    "objectID": "posts/2024-04-18-time-series-grain/index.html",
    "href": "posts/2024-04-18-time-series-grain/index.html",
    "title": "Time Series First Principles: Higher Grain Higher Accuracy",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the fourth principle of a good time series forecast, the higher the grain the higher the accuracy. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nClear Skies\nWhen planes take off from the ground they climb high into the sky. During that 5-10 minute period passengers have to stay seated with their seatbelt fastened. It’s only after the plane reaches 10,000 feet people can start to get up and move around the plane. Eventually the plane can reach an altitude of 40,000 feet. To compare, the peak of Mount Everest is 29,000 feet off the ground. Planes go up that high because it’s easier to fly the plane and more efficient. If planes flew a few feet off the ground it would be a lot bumpier ride, having to deal with changing weather and turbulence.\nForecasting is similar to flying a plane. Training a machine learning (ML) model at a higher grain of data is akin to a plane climbing in altitude. There is less turbulence (noise) in the data and your forecast has a better chance of being more accurate. You can either climb in altitude at the individual time series grain, or by the date grain. Let’s discuss each of them along with other methods.\n\n\nTime Series Grain\nThink of a higher time series grain as an aggregation of your original data. For example you might have product sales across a bunch of cities, where each city is a time series. Individual cities might have hard to model trend and seasonality, but when combined at a total country-level, it can be easier to model. Take the example charts below. Each city might be noisy but climbing in altitude up to the county level makes it easier to spot trends and seasonality.\n \n\n\nDate Grain\nIn a perfect world we would be able to forecast our businesses down to the day, or even minute, across the next 10 years. This is sadly not the case. The more granular you try to forecast at the date grain, the noisier the data is going to be, and the harder it will be to create accurate forecasts. Unless there is an absolute need to forecast at a certain level, I almost always recommend a higher date grain. Take the example charts below. See how aggregating daily data to a monthly date grain level makes it easier to spot trends and seasonality.\n \nIn the finance org, the most common date grain to forecast at is month. This gives a healthy balance of being able to forecast long periods of time while also being able to update your forecast with new historical data every few weeks.\nHere is another important point to call out. The longer your forecast horizon, the higher the date grain you should forecast at. Here are some recommendations based on your forecast horizon (how many periods you want to forecast). For example, if you’re trying to forecast out the next 6 months at the daily grain, you might get better results if you aggregate up to the monthly grain and forecast by month instead.\n\nDaily Grain: 1-90 day forecast horizon\nWeekly Grain: 1-12 week forecast horizon\nMonthly Grain: 1-18 month forecast horizon\nQuarterly Grain: 1-8 quarter forecast horizon\nYearly Grain: more than 1 year forecast horizon\n\n\n\nHierarchical Forecasting\nA potential “best of both worlds” solution to the data grain issue is to use a hierarchical forecast. This is where you can train models at different grains of the data, then use a statistical process to reconcile each forecast together so they are in sync. Our forecasting godfather, Rob Hyndman, has done a lot of great work in this space. Here is a chapter from his book on hierarchical forecasting.\nLet’s go back to our time series grain example. Using a hierarchical forecast you could train models and create forecasts for each city, then do the same at the total country-level, then finally do the same at a total world wide level across all countries. This is a standard hierarchical approach shown in the chart below. This hierarchical process blends a “bottoms up” forecast of creating predictions at the lowest level by city, with a “tops down” forecast of creating predictions at the highest global level. A statistical process is then used to make the “tops down” forecast equal the “bottoms up” forecast, optimizing for accuracy at all levels of the hierarchy.\n\nThe same idea can be applied at the date grain too. Where you can forecast at the daily level, weekly level, and monthly level. Then use a reconciliation process to get the final forecast at the daily level that is also accurate when summing up by month. This can work well if a monthly forecast is more accurate, but the final forecast needs to be at a daily level.\n\n\nAllocations\nAnother option is to take a forecast at a higher grain and allocate it down to a lower grain using simple allocation logic. This process can replace the more complicated hierarchical forecasting discussed earlier. Simple allocations can be done in two ways.\nThe first is to take historical values and create a percent split to apply to the final forecast. For example we can create a forecast at the country-level, then split that out by city. The split percent by city (allocation percent) can be calculated based on how much each city was the percent of total country over the last few years. This can be broken down by period. So you can get a specific percent split for each month on average in the past. This approach helps maintain historical seasonality across each time series (each city). See the charts below for an example of using two historical years of monthly data to create the final allocation percentages.\n\nHistorical splits by city\n\n\nMonth\nCity A %\nCity B %\nCity C %\n\n\n\n\nJan 2021\n30.87%\n30.08%\n39.05%\n\n\nFeb 2021\n28.11%\n23.90%\n47.99%\n\n\nMar 2021\n23.77%\n47.43%\n28.81%\n\n\nApr 2021\n31.26%\n18.42%\n50.32%\n\n\nMay 2021\n39.50%\n34.08%\n26.42%\n\n\nJun 2021\n50.58%\n30.98%\n18.44%\n\n\nJan 2022\n36.33%\n31.79%\n31.88%\n\n\nFeb 2022\n19.79%\n40.74%\n39.47%\n\n\nMar 2022\n19.95%\n28.99%\n51.06%\n\n\nApr 2022\n20.37%\n26.76%\n52.87%\n\n\nMay 2022\n41.37%\n41.53%\n17.10%\n\n\nJun 2022\n43.86%\n41.10%\n15.04%\n\n\n\n\nAverage of city split by month\n\n\nMonth\nCity A %\nCity B %\nCity C %\n\n\n\n\nJan\n33.60%\n30.93%\n35.46%\n\n\nFeb\n23.95%\n32.32%\n43.73%\n\n\nMar\n21.86%\n38.21%\n39.93%\n\n\nApr\n25.82%\n22.59%\n51.60%\n\n\nMay\n40.43%\n37.81%\n21.76%\n\n\nJun\n47.22%\n36.04%\n16.74%\n\n\n\n\nFinal forecast using the city splits\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\nCountry Forecast\nCity A %\nCity B %\nCity C %\nCity A Forecast\nCity B Forecast\nCity C Forecast\n\n\n\n\nJan 2023\n15000\n33.60\n30.93\n35.46\n5040\n4639.5\n5319\n\n\nFeb 2023\n15200\n23.95\n32.32\n43.73\n3640.4\n4912.64\n6646.96\n\n\nMar 2023\n15400\n21.86\n38.21\n39.93\n3366.44\n5884.34\n6149.22\n\n\nApr 2023\n15600\n25.82\n22.59\n51.60\n4027.92\n3524.04\n8049.6\n\n\nMay 2023\n15800\n40.43\n37.81\n21.76\n6387.94\n5973.98\n3438.08\n\n\nJun 2023\n16000\n47.22\n36.04\n16.74\n7555.2\n5766.4\n2678.4\n\n\n\nThe second approach is to use a future forecast to create the allocation splits. For example we can create future forecasts at the country-level and also at the city-level. Then we can create the split percent for each city by taking the city forecast and summing it up to the country-level, then taking the percent split for each city. These splits can then be applied to the final country-level forecast to get the final forecast by city. This approach uses the more robust country-level forecast, while still trying to capture future changing trends and seasonality by city.\n\nInitial forecast, where country and each city are forecasted separately\n\n\n\n\n\n\n\n\n\nMonth\nCountry Forecast\nCity A Forecast\nCity B Forecast\nCity C Forecast\n\n\n\n\nJan 2023\n10,000\n4,000\n3,500\n2,000\n\n\nFeb 2023\n10,500\n4,200\n3,000\n3,300\n\n\nMar 2023\n11,000\n4,500\n3,200\n3,300\n\n\nApr 2023\n11,500\n4,800\n3,400\n3,300\n\n\nMay 2023\n12,000\n5,000\n3,500\n3,500\n\n\nJun 2023\n12,500\n5,200\n3,800\n3,500\n\n\n\n\nCalculating the percent splits by city\n\n\nMonth\nCity A %\nCity B %\nCity C %\n\n\n\n\nJan 2023\n42.11%\n36.84%\n21.05%\n\n\nFeb 2023\n40.00%\n28.57%\n31.43%\n\n\nMar 2023\n40.91%\n29.09%\n30.00%\n\n\nApr 2023\n41.74%\n29.57%\n28.70%\n\n\nMay 2023\n41.67%\n29.17%\n29.17%\n\n\nJun 2023\n41.60%\n30.40%\n28.00%\n\n\n\n\nFinal forecast after applying the city splits to the country-level forecast\n\n\n\n\n\n\n\n\nMonth\nCity A Final Forecast\nCity B Final Forecast\nCity C Final Forecast\n\n\n\n\nJan 2023\n4,211\n3,684\n2,105\n\n\nFeb 2023\n4,200\n3,000\n3,300\n\n\nMar 2023\n4,500\n3,200\n3,300\n\n\nApr 2023\n4,800\n3,400\n3,300\n\n\nMay 2023\n5,000\n3,500\n3,500\n\n\nJun 2023\n5,200\n3,800\n3,500\n\n\n\n\n\nReversal\nA more granular forecast can sometimes be more accurate, especially if the more detailed grain uncovers more stable trends and seasonality that can be modeled. Take for example a product whose sales are impacted by Chinese New Year. That holiday doesn’t happen on the same day every year, and it can even happen in different months. Sometimes in January, and sometimes in February. Since it happens over multiple days the split between the two months can change drastically from year to year. Creating a forecast at the daily level, adding information around when Chinese New Year is happening, could result in a more accurate forecast. You could also take the approach of a monthly forecast, and have a numeric feature that lists how many days of Chinese New Year falls within each month.\nIf your initial data at the higher grain is noisy or has low forecast accuracy, consider asking the domain expert if there could be more insightful trends and seasonality at a lower grain.\n\n\nfinnts\nHierarchical forecasting is a tricky business, thankfully my open-source package finnts can automatically do hierarchical forecasting. The package can even use external regressors (features) in the hierarchical approach! Today finnts supports hierarchical forecasting at the time series grain. Hopefully one day we will implement hierarchical forecasting at the date grain, stay tuned. This is the same package I use internally at my job, allowing my company to replace hundreds of billions of manual forecasts with machine learning. Check out the package and see for yourself.\n\n\nFinal Thoughts\nJust as pilots navigate to higher altitudes to find smoother skies and better efficiency, so too must we elevate our approach to data granularity in forecasting when needed. By stepping back from the minutiae of daily or city-level data and ascending to monthly or country-level aggregations, we enable our models to capture more coherent patterns and deliver forecasts with improved precision. This strategic shift—from a granular view to a broader perspective—is not just about avoiding turbulence; it’s about leveraging stability to enhance predictability.\nHowever, the real magic often lies in blending these approaches through hierarchical forecasting. This method combines the detailed insights available at lower levels with the clarity and simplicity of higher-level forecasts, ensuring both depth and breadth in our predictive capabilities. As we continue to refine our techniques and tools, like the finnts package, we are paving the way for a future where complex, multi-tiered forecasting is as streamlined as a flight cruising at 40,000 feet.\nIn your journey through data, remember that the right altitude can make all the difference. Rising above the noise can provide not just clearer views, but also far-reaching insights. So, buckle up—we’re about to take forecasting to new heights."
  },
  {
    "objectID": "posts/2024-04-11-time-series-past-future/index.html",
    "href": "posts/2024-04-11-time-series-past-future/index.html",
    "title": "Time Series First Principles: The Future Is Similar To The Past",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the third principle of a good time series forecast, the future is similar to the past. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nHere Comes The Sun\nFor whatever you’re trying to forecast, it will be a lot easier to do with with machine learning (ML) if the future is similar to the past. It’s as simple as that.\nWhen you open the weather app on your phone, have you ever looked at when the sun is expected to rise and set? If you’re on the new human optimization craze about getting morning sunlight, you most likely have. That forecast is down to the minute, most likely even second, and has a high degree of accuracy. Is the forecast accurate because of expert human judgement, or the type of weather related features fed into a ML model? Nope. It’s accurate because the sun has risen and set at relatively the same time, based on time of year, for millions of years. We don’t expect future sun rises and sun sets to change that much going forward, that’s why your weather app gives you an exact time for the sun rise but gives you only a percent probability of rain. Even then, that chance of rain may not even be accurate. It’s almost a joke now how many times in Seattle I’ve seen a dry forecast only to step out of my house and have it immediately start raining. At least I know the exact minute when the sun will set that day.\n\n\nHandling A Changing Future\nYour business is most likely not like the sun. It’s constantly changing, reacting to market forces and industry competitors. The best way to teach a model about your expectations of the future is to give it data about the past and future.\nLet’s use an example of a monthly revenue forecast for a product. If you only use historical sales data to forecast the product, then you are making the assumption that the future of the product will be almost identical to the past, especially the most recent past. For some established products in mature industries this could be totally fine, but often this is not the case.\nOne thing to try is adding features that can explain how outside forces impact the product. For example, how much money consumers have to spend might greatly impact who buys your product. So using an economic feature like consumer sentiment can help a model adjust it’s predictions based on changes in consumer spending habits.\nWe can add features into our data in two ways. The first is to just give historical values of that feature. This will force us to only use historical lags of the feature when training a ML model, since we don’t know what the future value of that feature will be. We can take that original feature and create new features (this is called feature engineering). Ones that are a 3 month lag, 6 month lag, or 12 month lag of the original data. Often macro data like consumer sentiment can be a lagging indicator. Meaning their impact is delayed and takes a while to flow through the economy. Changes in consumer sentiment from 6 months ago can actually have a strong correlation with how customers purchase our product today.\nA second way is to use both historical values and future values. We could use a future forecast of consumer sentiment in our model, in addition to using the historical data. That way a model can learn from any lagged relationships as well as understand how changes in consumer sentiment impact our product in real time. These future values can either come from an expert forecast (like from famous economists) or created by your own ML solution.\n\n\nThe Future Must Always Learn From The Past\nYou might have a ton of ideas for new kinds of future information your can encode as features to train a model. In order to use this data we need to make sure there are historical examples for a model to learn from. The upcoming 2024 presidential election in the US could have a large impact on your business, which will impact your future forecast for the rest of 2024. We know exactly when the election is going to happen, so it’s easy to give that information into a ML model to learn from.\nThe catch is we need to make sure that we show examples from the past to allow the model to learn how previous elections impact our business and how the model should handle similar events in the future. If we only have product sales data from the last three years, then we cannot feed it future election data because we don’t have the data from the 2020 or 2016 US presidential elections.\nIf we know something is going to happen in the future, but we can’t quantify it with historical data for a model, then we need to go old school. Instead we need to use our domain expertise about the business to take the ML output, without knowledge of the future event, and make a manual adjustment to the forecast based on the expected impact of the future. For the election example, maybe your product sales will grow as we get closer to the election, so if you don’t have enough historical data for a model to learn about the election’s impact you can make a manual adjustment to the ML forecast based on your assumption about the election’s impact.\nThis kind of hybrid approach, ML first with a light human touch second, can create a powerful combination. A ML model can do 80-90% of the initial work and a human can make the final manual adjustments based on their domain expertise. This allows a human to add more insight into a forecast that is not easily quantifiable for a ML model to learn from.\n\n\nNew Time Series\nA new product at your company might be exciting, but is harder to forecast accurately with ML models. The lack of historical data will make it hard for any new ML model to learn from. Initial trends and seasonality may not always carry into the future. For example there might be a big spike in initial sales around release but then taper off over time. The new product may not even be on sale yet, so you are now tasked with forecasting something with zero historical data.\nIf the time series in question has some historical data, ideally more than one year of historical observations, a good way to deal with it is to train a ML model with the new time series alongside similar existing products with a lot of historical data. This is sometimes called a “global model”, where a model learns from multiple time series instead of one. Training on one specific time series is sometimes called a “local model”. Training a global model allows the ML model to learn general trends and seasonality patters across similar time series and apply it to the newer time series. This can work well if the other time series are similar to one another.\nIf the product you want to forecast has no historical data, then you are in a tough boat. Traditional time series methods cannot help you, since they all rely on quality historical data. What you can do is take a more traditional machine learning regression approach. This involves taking all historical products that have launched over time and training a model to understand the initial demand of a new product and how it either grows or shrinks over time. For example with iPhone sales, you can train a model on the initial sales of each iPhone model from V1 to V14, then use that model to predict the kind of demand the latest V15 iPhone might have. This type of approach would need a more detailed post to explain fully, but hopefully you get the broader picture.\nTo learn more about forecasting new time series, check out the forecasting bible written by our forecasting godfather Rob Hyndman. The chapter on judgemental forecasts goes deep into forecasting new products and discusses other approaches you can take.\n\n\nReversal\nWhen using future values of a feature, there is a risk of compounding errors. Let’s go back to the consumer sentiment example. If you create your own expectation of future consumer sentiment, or use an economist’s prediction, there is a good chance the forecast will be wrong. If the forecast about consumer sentiment is wrong, and that forecast is fed into a model to predict your product’s sales, then your sales forecast will be even more wrong. The errors compound. Add in other features and you can see how the house of cards can tumble pretty fast. Always be weary about using future values of features where you don’t have 100% confidence in their future value. For example using future holiday features are great because they will always happen on a specific day with 100% certainty, but trying to tell a model where inflation is headed in the future can get you in trouble.\nHaving a human make manual adjustments after the initial ML forecast can add unneeded human bias to the final forecast. This bias can sometimes be wrong and hurt the accuracy of the forecast. It’s good practice to capture these adjustments and always report on the forecast accuracy of the pure ML model and the accuracy for the model + human adjustments. That way you can track how helpful the manual adjustments are, and remember why they were made in the first place.\n\n\nFinal Thoughts\nWhen embarking on the journey of time series forecasting, remember it’s more art than exact science—akin to predicting rain in Seattle. The key takeaway? Use the past as a guide but sprinkle in educated guesses about the future with caution. Whether you’re launching new products or navigating established markets, blending machine learning with a dash of human intuition can create robust forecasts. May your forecasts be as reliable as the sunrise, with just enough flexibility to handle an unexpected downpour."
  },
  {
    "objectID": "posts/2024-04-05-weekend-reads/index.html#videos",
    "href": "posts/2024-04-05-weekend-reads/index.html#videos",
    "title": "Weekend Reads (4/5/24)",
    "section": "Videos",
    "text": "Videos\n\nImproving Focus\nDopamine and Motivation\nOptimal Morning Routine\nSequoia AI Summit\nSBF, Gov Spending, and More on All In"
  },
  {
    "objectID": "posts/2024-04-05-weekend-reads/index.html#podcasts",
    "href": "posts/2024-04-05-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (4/5/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nScott Glenn on Tim Ferriss\nSeth Godin on Tim Ferriss\nDr. Rhonda Patrick on The Knowledge Project"
  },
  {
    "objectID": "posts/2024-04-05-weekend-reads/index.html#tweets",
    "href": "posts/2024-04-05-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (4/5/24)",
    "section": "Tweets",
    "text": "Tweets\n\nSora Goodness\nSatya Nadella Stays Undefeated\nBezos Leadership Principles"
  },
  {
    "objectID": "posts/2024-04-05-weekend-reads/index.html#books",
    "href": "posts/2024-04-05-weekend-reads/index.html#books",
    "title": "Weekend Reads (4/5/24)",
    "section": "Books",
    "text": "Books\n\nGenAI Guidebook"
  },
  {
    "objectID": "posts/2024-03-26-time-series-first-principles-initial/index.html",
    "href": "posts/2024-03-26-time-series-first-principles-initial/index.html",
    "title": "Thoughts on First Principles in Time Series Forecasting",
    "section": "",
    "text": "I’ve been doing time series forecasting with machine learning (ML) most of my career. I believe it’s still the best AI opportunity in corporate finance, even with all of the latest Generative AI developments in recent years. If you work for the CFO, chances are you often create predictions about the future. Those predictions take time and can always be more accurate. Machine learning can help in both areas. Before you build machine learning solutions in your finance org, it’s important to understand the true building blocks of making good forecasts.\nIn this post I will overview each first principle, and have follow-up posts digging deeper into each one. Let’s dive in.\n\nDomain Expertise: Knowing what factors actually influence what you are trying to forecast is more important than which ML model to train.\nGarbage In Garbage Out: Training a model on bad data leads to bad forecasts.\nThe Future Is Similar To The Past: If you expect the future to be drastically different than past data, you will have a hard time training accurate models.\n\nHigher Grain Higher Accuracy: Forecasting by country is often more accurate than forecasting by city. Forecasting by month is often more accurate than forecasting by day.\nOrder Is Important: When time is involved, how your data is ordered makes all the difference.\nThe Magic Is In The Feature Engineering: How you transform your data before model training can transform a mediocre forecast into a world class forecast.\nSimple Models Are Better Models: Like occam’s razor, the best model is often the one with the least amount of inputs.\nCapture Uncertainty: Showing the back testing results and future uncertainty of a model’s forecast builds more trust.\nModel Combinations Are King: Usually a combination of multiple models is more accurate than just one model’s prediction.\nDeep Learning Last: Deep learning isn’t as effective as more traditional ML models.\n\n\nFinal Thoughts\nThis is not an exhaustive list, but instead principles that I find particularly important when creating a time series forecast. Having a firm understanding of these principles is enough to get the ball rolling on any type of forecast you’re working on. Thankfully, the very same approach I use in my job to do forecasting is open source and freely available through my R forecasting package called finnts. Even if you’ve never done data science or used R before, finnts makes it easy to get off the ground fast without shooting yourself in the foot when dealing with the above principles. Stay tuned for more posts about each principle."
  },
  {
    "objectID": "posts/2024-03-16-weekend-reads/index.html#videos",
    "href": "posts/2024-03-16-weekend-reads/index.html#videos",
    "title": "Weekend Reads (3/16/24)",
    "section": "Videos",
    "text": "Videos\n\nThe Algebra of Happiness"
  },
  {
    "objectID": "posts/2024-03-16-weekend-reads/index.html#podcasts",
    "href": "posts/2024-03-16-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (3/16/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nChris Davis On The Knowledge Project\nKimbal Musk On Lex Friedman\nDr. Cal Newport On Huberman Lab"
  },
  {
    "objectID": "posts/2024-03-16-weekend-reads/index.html#tweets",
    "href": "posts/2024-03-16-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (3/16/24)",
    "section": "Tweets",
    "text": "Tweets\n\nExcellent Product Placement\nMeet Devin, Your AI Dev Intern\nSpaceX Launch\nChatGPT Writing Prompt\nInvest For The Long Haul"
  },
  {
    "objectID": "posts/2024-03-16-weekend-reads/index.html#books",
    "href": "posts/2024-03-16-weekend-reads/index.html#books",
    "title": "Weekend Reads (3/16/24)",
    "section": "Books",
    "text": "Books\n\nThe Great Mental Models Volume 1 by Shane Parrish"
  },
  {
    "objectID": "posts/2024-02-23-weekend-reads/index.html#articles",
    "href": "posts/2024-02-23-weekend-reads/index.html#articles",
    "title": "Weekend Reads (2/23/24)",
    "section": "Articles",
    "text": "Articles\n\nSora by OpenAI\nOpenAI Forum\nCompany Fires Employees Who Don’t Embrace New AI Tools"
  },
  {
    "objectID": "posts/2024-02-23-weekend-reads/index.html#videos",
    "href": "posts/2024-02-23-weekend-reads/index.html#videos",
    "title": "Weekend Reads (2/23/24)",
    "section": "Videos",
    "text": "Videos\n\nAswath Damodaran on Prof G Markets\nWhat I Learned From 100 Days of Rejection\nCharlie Houpert on Modern Wisdom"
  },
  {
    "objectID": "posts/2024-02-23-weekend-reads/index.html#podcasts",
    "href": "posts/2024-02-23-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (2/23/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nChris Williamson on Joe Rogan\nLatest AI Developments on All In\nMorgan Housel on Modern Wisdom"
  },
  {
    "objectID": "posts/2024-02-23-weekend-reads/index.html#songs",
    "href": "posts/2024-02-23-weekend-reads/index.html#songs",
    "title": "Weekend Reads (2/23/24)",
    "section": "Songs",
    "text": "Songs\n\nSweet City Woman by Stampeders"
  },
  {
    "objectID": "posts/2024-02-23-weekend-reads/index.html#series",
    "href": "posts/2024-02-23-weekend-reads/index.html#series",
    "title": "Weekend Reads (2/23/24)",
    "section": "Series",
    "text": "Series\n\nUnbreakable Kimmy Schmidt on Netflix"
  },
  {
    "objectID": "posts/2024-02-16-weekend-reads/index.html#articles",
    "href": "posts/2024-02-16-weekend-reads/index.html#articles",
    "title": "Weekend Reads (2/16/24)",
    "section": "Articles",
    "text": "Articles\n\n8 Lessons from “Curb Your Enthusiasm”\nThe Knowledge Economy Is Over. Welcome to the Allocation Economy\nThe Race Is On to Stop Ozempic Muscle Loss\nAll My Thoughts After 40 Hours in the Vision Pro"
  },
  {
    "objectID": "posts/2024-02-16-weekend-reads/index.html#tweets",
    "href": "posts/2024-02-16-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (2/16/24)",
    "section": "Tweets",
    "text": "Tweets\n\nHigh Agency by George Mack\nVision Pro Teardown by Trung Phan"
  },
  {
    "objectID": "posts/2024-02-16-weekend-reads/index.html#books",
    "href": "posts/2024-02-16-weekend-reads/index.html#books",
    "title": "Weekend Reads (2/16/24)",
    "section": "Books",
    "text": "Books\n\nThe Boys in the Boat by Daniel James Brown"
  },
  {
    "objectID": "posts/2024-02-16-weekend-reads/index.html#products",
    "href": "posts/2024-02-16-weekend-reads/index.html#products",
    "title": "Weekend Reads (2/16/24)",
    "section": "Products",
    "text": "Products\n\nMaui Nui Sugar Free Venison Jerky"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#articles",
    "href": "posts/2024-01-12-weekend-reads/index.html#articles",
    "title": "Weekend Reads (1/12/24)",
    "section": "Articles",
    "text": "Articles\n\nScott Galloway’s 2024 Predictions\nThe Psychology of Persuasion\nDeep Dive on the AI Revolution"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#videos",
    "href": "posts/2024-01-12-weekend-reads/index.html#videos",
    "title": "Weekend Reads (1/12/24)",
    "section": "Videos",
    "text": "Videos\n\nDoes Athletic Greens Actually Work?\nDo Electrolyte Powders Actually Work?"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#podcasts",
    "href": "posts/2024-01-12-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (1/12/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\n2024 Predictions on All In\nGeorge Mack on Modern Wisdom"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#tweets",
    "href": "posts/2024-01-12-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (1/12/24)",
    "section": "Tweets",
    "text": "Tweets\n\nDiscipline is Overpriced, Incentives are Underpriced\nRicky Gervais at the Golden Globes"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#movies",
    "href": "posts/2024-01-12-weekend-reads/index.html#movies",
    "title": "Weekend Reads (1/12/24)",
    "section": "Movies",
    "text": "Movies\n\nPoor Things: In Theatres\nYou Are What You Eat (A Twin Experiment): On Netflix"
  },
  {
    "objectID": "posts/2024-01-12-weekend-reads/index.html#books",
    "href": "posts/2024-01-12-weekend-reads/index.html#books",
    "title": "Weekend Reads (1/12/24)",
    "section": "Books",
    "text": "Books\n\nHidden Potential by Adam Grant"
  },
  {
    "objectID": "posts/2023-12-30-naval-book/index.html",
    "href": "posts/2023-12-30-naval-book/index.html",
    "title": "Thoughts on The Almanack of Naval Ravikant",
    "section": "",
    "text": "The Almanack of Naval Ravikant by Eric Jorgenson is something I like to read every few months. Eric was able to take all public content from Naval and put it into one place. I love that idea and think Eric will be successful with many other thought leaders.\nMeet Naval Ravikant: a modern philosopher-entrepreneur and the co-founder of AngelList. Renowned for his deep insights on technology, investing, and the art of living, Ravikant has become a luminary in Silicon Valley. Beyond his success with startups like X (Twitter) and Postmates, he captivates audiences with his profound perspectives on life and happiness, shared on his popular podcast and Twitter feed.\nEnjoy some of Naval’s best quotes and ideas I found most interesting.\n\nTypes of Leverage\n\nLabor means people working for you. It’s the oldest and most fought-over form of leverage. Labor leverage will impress your parents, but don’t waste your life chasing it. Money is a good form of leverage. It means every time you make a decision, you multiply it with money. It’s probably been the most dominate form of leverage in the last century.\nCode and media are permissionless leverage. They’re the leverage behind the newly rich. You can create software and media that works for you while you sleep.\n\nFocus on permissionless leverage. Often people think the next step in their career is managing people. That’s how they get more money, more power, more status. Is Taylor Swift Time’s 2023 person of the year because she is a great manager or because she is the best in the world at media (music, video, merch)? We’re quickly starting to see more one person empires being built. Through media with influencer’s, and through code like Satoshi Nakamoto (inventor of Bitcoin). Instead of trying to grow you career through scaling of people and capital, try the route with no gatekeepers.\nBalaji Srinivasan made an interesting point that once a robot can do something, you’ve turned labor into capital. Meaning you just need the money to purchase the robot and then code to program it. Not long from now we might see managers only overseeing physical robots operating in the world of atoms (doing physical work) or AI agents operating in the world of bits (traditional knowledge work). The skills needed to manage these new digital workers revolve around writing code and coordination. Not traditional management skills.\n\n\nBuild or Sell\n\nLearn to sell. Learn to build. If you can do both, you will be unstoppable.\n\nI believe Bill Gates said this back in his heyday at Microsoft. This is the most powerful piece of advice in the book. Most businesses boil down to a product or service to sell. The most important people at a company either build that product or sell that product. If you do not do work like that today at your company, you might want to consider changing jobs. These roles have the greatest potential for impact on the world, and create the most financial rewards.\n\n\nSpecific Knowledge\n\nSpecific knowledge is knowledge you cannot be trained for. If society can train you, it can train someone else and replace you. When specific knowledge is taught, it’s through apprenticeships, not schools.\n\nThis is how you can prevent AI from taking your job. If what you do can’t be explained in a book, class, or YouTube videos, there is a good chance it will be hard for an AI model to figure out how to do it well.\n\n\nBe You\n\nThe way to get out of the competition trap is to be authentic, to find the thing you know how to do better than anybody. You know how to do it better because you love it, and no one can compete with you. If you love to do it, be authentic, and then figure out how to map that to what society actually wants. Apply some leverage and put your name on it. You take the risks, but you gain the rewards, have ownership and equity in what you’re doing, and just crank it up. Your goal in life is to find the people, business, project, or art that needs you the most. There is something out there just for you. What you don’t want to do is build checklists and decision frameworks built on what other people are doing. You’re never going to be them. You’ll never be good at being somebody else. Technology democratizes consumption but consolidates production. The best person in the world at anything gets to do it for everyone.\n\nNothing to add here, Naval hit the nail on the head.\n\n\nLifelong Learning\n\nThe most important skill for getting rich is becoming a perpetual learner. You have to know how to learn anything you want to learn. The old model of making money is going to school for four years, getting your degree, and working as a professional for thirty years. But things change fast now. Now, you have to come up to speed on a new profession within nine months, and it’s obsolete four years later. But within those three productive years, you can get very wealthy.\n\nChatGPT was released by OpenAI about a year ago. In that time, people have had the opportunity to get up to speed on using generative AI models like GPT-3.5 and now GPT-4. There was no class in using these models, only the documentation on OpenAI’s website and videos on YouTube. For the brave folks who went ahead and learned how to use these models, the world will be theres the next three years while everyone else catches up. Learning how to use GPT-4 in your company to improve a product or become more productive is a form of permissionless leverage. Something of a superpower. The recent AI hype cycle might be winding down, but people with these skills are in demand. A few years from now another technology or idea might come out, another thing that’s not taught in schools. Never stop learning, never stop growing."
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#summary",
    "href": "posts/2023-12-11-ai-specialization/index.html#summary",
    "title": "Is AI the End of Specialization?",
    "section": "Summary",
    "text": "Summary\nAI and large language models (LLM) are slowly taking away value from jobs that used to require a lot of schooling, like medicine and law. It will even come for pure software development jobs. What AI cannot replace in the near term is specialized knowledge, skills that can only be obtained through apprenticeships. In the future, we need to focus more on gaining strong domain expertise across business, science, and engineering. But only in ways where specialized knowledge is needed, not simple skill acquisition that a LLM can do faster and cheaper. Going to a coding bootcamp to learn how to build web apps will help you make money this year at a company, but that same company might kick you to the curb three years from now when GitHub Copilot can code the entire app in a day."
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#more-specialization-more-knowledge",
    "href": "posts/2023-12-11-ai-specialization/index.html#more-specialization-more-knowledge",
    "title": "Is AI the End of Specialization?",
    "section": "More Specialization, More Knowledge",
    "text": "More Specialization, More Knowledge\nWhich jobs require the most schooling? This is a good proxy for how much knowledge someone has to attain before they are qualified to work in a specific job. Doctors and lawyers seem to require the most schooling before they can start their careers. Most likely because of all of the information they have to memorize and load into their head. The more specialized someone’s skills in a domain like medicine, the more money they can make. What’s interesting is that new large language models are becoming very good at the same thing, and can be trained in months, not years. How will this impact these high status, high pay jobs? Specialization will go through a fundamental shift in the age of AI."
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#story-time",
    "href": "posts/2023-12-11-ai-specialization/index.html#story-time",
    "title": "Is AI the End of Specialization?",
    "section": "Story Time",
    "text": "Story Time\nI recently had to get my knee checked out by a doctor. I tweaked it pretty good playing kickball at my grandma’s 90th birthday party (story for a different time). Most of my time spent getting my knee examined was not actually interacting with the doctor I came to see. Getting checked into the system, filling out forms, getting an xray. All things where the doctor was no where to be found. The doctor did finally come in to my room, after waiting a while, and briefly talked to me for two minutes. Yes, two minutes. Almost as if they couldn’t wait to leave and were in a rush to go somewhere else, maybe the golf course. They recommended I go get an MRI, and handed me a phone number to schedule it myself.\nI went to get the MRI, which was even stranger. It was in a different medical building, one with a waiting room the size of a small bedroom. There were no doctors there, just someone to check you in and someone to operate the MRI machine. After getting the MRI, the technician said I had some bruising on my knee. I asked what that meant and they said they were not a doctor and couldn’t tell me anything about my knee. A qualified doctor was required to do that.\nFinally, after another week I was able to see the doctor again. They looked at the MRI and told me some various technical jargon that meant my knee was ok and that I should just take it easy for another month to heal part of my bone that connects to the knee. This entire process took a month, all to see and speak to a doctor for five total minutes over three appointments. The cherry on top was that it cost me hundreds of dollars even with good insurance. Do you see where I’m going here?"
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#rethinking-healthcare",
    "href": "posts/2023-12-11-ai-specialization/index.html#rethinking-healthcare",
    "title": "Is AI the End of Specialization?",
    "section": "Rethinking Healthcare",
    "text": "Rethinking Healthcare\nWhat if instead I could go to a local pharmacy, and say that my knee hurts. There could be someone with high tech equipment who could xray my knee, and take an MRI if needed. An AI system would analyze the scans and flag any concerns. I could then get a final recommendation from the system. If something was wrong with my knee, it could recommend some physical therapy exercises. Or if something was really wrong it could route my request to a hospital that could do the surgery for me. This process could happen in an hour, instead of a month, and could cost me half the price without ever needing to see a real doctor. This kind of technology might be closer to reality then some might guess.\n\nDoctors who go to school most of their life could be replaced by a specially trained large language model that was trained on the history of medical research in a few months. It would already know everything about the human body, including what current therapies and medicines work the best. The best part is once it’s built, it can scale to the entire world. Rural villages in India could get the same healthcare as the rich in Beverly Hills.\nWould doctors still need to go to school? There is still an opportunity for wannabe doctors about to enter school. Instead of going to school for seven years, maybe they just jump to the very end of the training from the get go. Instead of building a strong foundation about everything in the body, they skip and go right to the path of specialization in a specific field of medicine. This is how doctors get paid the most anyways. If someone is interested in oncology (treating cancer), then maybe that’s what they start studying immediately. They can use AI tools to cover the basics on everything else and spend all of their waking moments on learning about cancer. Lowering the barriers to entry might make it easier for more folks to do medical research, at least until AI starts to get good at that too.\nIf doctors can be replaced, what happens to nurses? They already do a decent amount of the work already in hospitals, and are the main connection to a patient. Over time they could be the ones powered by AI who end of making all of the decisions and determining the best treatment options. A nurses role would change into a combined version of what doctors and nurses have done historically. Maybe even spawning a new type of job. Robotics is another space where AI can disrupt medicine. Would someone rather be treated by a human, or a machine that comes with 90% less medical bills? Money might do the talking there, but replacing human connection could be the most resistant to AI and automation."
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#ai-cannot-replace-specialized-knowledge",
    "href": "posts/2023-12-11-ai-specialization/index.html#ai-cannot-replace-specialized-knowledge",
    "title": "Is AI the End of Specialization?",
    "section": "AI Cannot Replace Specialized Knowledge",
    "text": "AI Cannot Replace Specialized Knowledge\nMemorizing all of the parts of the body and the latest medical research is easily done by LLMs. What could be more difficult is what Naval Ravikant calls specialized knowledge. These are skills that cannot be easily learned from a book or class. Instead they usually get developed through apprenticeships. A student learning from a master, Jedi style. My life in corporate finance is a good example of specialized knowledge. My company hires finance employees from all walks of life. Having a finance degree or experience is good but not crucial to succeeding. This is most likely because the work you actually do within the CFO’s org is very specialized, where what classes you took in school is not that helpful. Most of it, in the financial planning and analysis space, revolves around financial rhythms like forecasting and close. There is no class in college related to this work. You cannot watch a few YouTube videos and get up to speed in a weekend. You learn by doing, and mostly watching others do it. This apprenticeship like model is a powerful indicator of what jobs are hard to replace with AI. There have been efforts to automate things like forecasting with machine learning (ML) in finance, but we have a long way to go before that becomes the default for everyone. Even then there will be cases where a ML model cannot capture everything about a business in its training data, and a human will always need to be in a loop to impart their domain expertise.\nDomain expertise will become more valuable as LLMs can learn more technical concepts like coding and memorizing all facts on the internet. What it can’t do (not yet at least) is know how a specific business works inside and out like a human can. A LLM could write all of the code to build your companies website, but it will not know how to piece together logistics, manufacturing, sales, marketing, and design all at the same time like a CEO can. Maybe one day it will get closer, but for now we still need a human at the top directing all of these AI processes. That takes specialized knowledge."
  },
  {
    "objectID": "posts/2023-12-11-ai-specialization/index.html#final-thoughts",
    "href": "posts/2023-12-11-ai-specialization/index.html#final-thoughts",
    "title": "Is AI the End of Specialization?",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIn my current job, I believe strong domain knowledge about finance and our business is going to be way more important than what kind of coding skills you posses. Since LLMs will take over more of the coding tasks, leaving work that requires specialized knowledge for us humans to do. This could mean less specialists in technical domains that historically have paid the most, since that kind of knowledge was rare in society. With more emphasis on specialization for domain specific knowledge is a business instead. Coders who are like restaurant line cooks working their way through a backlog created by someone else might be at the highest risk of getting their jobs removed. Same goes for doctors who just spend five minutes reviewing a patients chart and telling them what to do next without any follow up or long-term investment.\nFinally, if AI can know the foundation of each profession, that makes it easier for anyone to come into the space and do good work. If they are able to obtain specialized knowledge quickly. This opens up opportunities for people to have multiple careers throughout their life, without the need of always going back to school or getting a credential of some type. Over the long term this gives the power to individuals with high agency and entrepreneurial spirit. Ones who can seize opportunities for innovation and make impact across many different domains."
  },
  {
    "objectID": "posts/2023-11-24-llm-in-finance/index.html",
    "href": "posts/2023-11-24-llm-in-finance/index.html",
    "title": "Large Language Model Use in Finance",
    "section": "",
    "text": "How can we use large language models in corporate finance? A lot of great work has been done inside of Microsoft Finance to find and consolidate ways we can leverage generative AI. Below are the common themes we found. Special shout out to the Microsoft modern finance community for their help in this effort."
  },
  {
    "objectID": "posts/2023-11-24-llm-in-finance/index.html#final-thoughts",
    "href": "posts/2023-11-24-llm-in-finance/index.html#final-thoughts",
    "title": "Large Language Model Use in Finance",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThere is no need to go directly into the deep end of the LLM pool with custom solutions. See what you can start using immediately with existing copilots and then build from there. Technology by definition is something that doesn’t quite work yet, so don’t be afraid to experiment and fail."
  },
  {
    "objectID": "posts/2023-02-12-language-wars/index.html",
    "href": "posts/2023-02-12-language-wars/index.html",
    "title": "The ML Language Wars Are Over, Large Language Models Won",
    "section": "",
    "text": "When I started learning data science there was an essential choice that had to be made at the beginning. One that has consequences that can ripple throughout your machine learning journey and even change your career options. I’m talking about the classic debate of R versus Python. Just like any hot issue, you will most likely get a different answer to this question based on who you ask. R is great for statistics, but Python is the best for production. You can’t do deep learning in R. Python is the second best language for everything. Over the years it seems that each side of the debate has only dug their heels in deeper. Whenever people ask me what language they should learn first, I usually give this recommendation. If you can only choose one, learn python, but eventually you will have to learn both to be a good data scientist. Building high quality solutions with data comes down to one thing, picking the right tool for the job.\nEach language has its strengths. Over the years the rise of new open source packages have started to even the playing field. For example R has always had amazing packages for time series, but now python is catching up. The same can be said for the rise of deep learning in R. While I still believe learning both will allow you to thrive in your data career, the rise of large language models (LLM) like OpenAI’s Chat-GPT are going to change the game forever.\nIf you haven’t heard of Chat-GPT yet then you are living under a rock, and congrats on being able to read this blog inside your underground doomsday bunker. These LLM’s can do a lot of things, but the game changer for the open source data space is being able to easily translate one language into another. For example I’m able to easily translate dplyr code in R into perfect pandas code in python. Over time these models will unlock the potential to translate every package from one language into another. Imagine writing a package in R, and have a GitHub action that calls a LLM to automatically convert it into python or vice versa. A data scientist could even write code in one language during a projects development, then automatically convert it into another when they move into production. The possibilities are endless!\nModels like Chat-GPT may not be at this level of language translation today, but like anything in the AI space the pace of innovation moves exponentially, and could be on the horizon sooner than you think. I for one personally can’t wait for this day to come."
  },
  {
    "objectID": "posts/2023-01-01-new-year-resolutions/index.html",
    "href": "posts/2023-01-01-new-year-resolutions/index.html",
    "title": "New Year Resolutions Are Dumb",
    "section": "",
    "text": "New year, new me, right? Today marks the start of many peoples new year resolutions, or big goals they want to start chasing over the next year. For me I think that’s bad advice. Goals like “get in shape” or “travel more” don’t really mean anything and can easily get set aside as your life autopilot takes over. No wonder why most resolutions get dropped by February. Why do you think you have to sign a bunch of contracts when you join a gym? They know how our default settings work and ensure you’re paying for a while regardless if you come in. In reality, they would appreciate it if you didn’t come in at all. Less of your sweat to mop up.\nInstead I focus on two things, being consistent and trying to map out the next few steps in front of me.\nBeing consistent is what matters in anything you pursue in life. People overestimate what they can accomplish in the next few weeks but underestimate what they can accomplish in a year. Let’s change the resolutions we called out above to instead be more consistent. Instead of “get in shape” we can rephrase that as “exercise 30 min every day”, or “travel more” can be changed to “leave the country once a year” or “get out of my town once a quarter”. These are now measurable things that will keep you honest instead of loftier ones that are easier to sweep under the rug.\nHaving long term goals is another trap I try to avoid. People plan, and God laughs. Having it all figured out is a quick way to getting let down when things don’t exactly go as you planned. Instead I focus on the next 1-3 steps in all aspects of my life. I don’t plan to have 12 pack abs, but rather focus on doing core 3-5 times a week. I don’t plan to be some hot shot director or senior engineer at my job, but rather focus on shipping the next few features on the big projects I work on. I still maintain a vision of the long term aspects of the things I work on in my job, but know that those are subject to change as I actually start to build out the next 1-3 steps.\nIf you are someone who enjoys writing goals, then all power to you. To each their own. For me I have found being consistent in a few things can compound into something special. Why not try it yourself and see where it takes you in 2023."
  },
  {
    "objectID": "posts/2022-09-25-quarto-switch/index.html",
    "href": "posts/2022-09-25-quarto-switch/index.html",
    "title": "Switching to Quarto",
    "section": "",
    "text": "I’ve recently switched this website over to quarto, the newest scientific publishing technology developed by RStudio, which recently changed its name to Posit. Lot’s of change going on at Posit, and they are doing some really cool things. One of the most interesting is quarto.\nQuarto builds on top of the already great work of Rmarkdown, which is a way to take code and covert it to other document types like pdf or html. With quarto you can write Python or R code, and create outputs that can be published anywhere, including as web pages.\nI used to leverage some complex Ruby code with Jekyll to produce this site, but with quarto the process was dramatically simplified. The design of “.qmd” documents is very smooth and intuitive, allowing me to fully re-publish the site on a Sunday afternoon. I still leverage github pages, which connects to a “docs” folder in my repo where the quarto files are rendered as html, css, and javascript. Very simple.\nWith great technology like quarto, it begs the question if folks should use quarto for all written communication. From writing proposals, creating slide presentations, publishing papers, and creating technical documentation. All written as code and stored in version control. Exciting stuff!\nIf you’d like to learn more about quarto, check out their website!"
  },
  {
    "objectID": "posts/2021-09-13-intro-finnts/index.html",
    "href": "posts/2021-09-13-intro-finnts/index.html",
    "title": "Time Series Forecasting Simplified with finnts",
    "section": "",
    "text": "I’m excited to announce the release of a new R package called finnts, aka Finn. The package helps simplify the process of creating time series forecasts with statistical and machine learning models. Finn automates the more tedious aspects of machine learning. Things like data cleaning, feature engineering, back testing, and model selection are all handled automatically by Finn while still being flexible to many forecasting scenarios.\nFinn is a perfect solution for people new to machine learning as well as seasoned pros looking for a scalable way to put many forecasts into production. Please take a look at the package site, try it out, and let me know what you think!"
  },
  {
    "objectID": "posts/2021-08-24-probono-stretch-projects/index.html",
    "href": "posts/2021-08-24-probono-stretch-projects/index.html",
    "title": "Pro Bono Work for Stretch Projects",
    "section": "",
    "text": "Want to learn a new skill for your career? There are tons of places to learn the actual skill, from in person classes to free YouTube videos. What is sometimes the hard part is the application of the skill that is often required when trying to transition jobs or career paths. You know, the whole “experience” word employers throw out in job postings. Even entry level jobs ask for so many years of experience. This leads to a chicken or the egg problem. You need experience to get a job, but you need to get a job to gain experience.\nTechnical roles like data science and analytics suffer from this problem in a big way, where you normally see job postings asking for PHD’s or having 10 years of experience. The term data science has barely been around for 10 years, this is crazy."
  },
  {
    "objectID": "posts/2021-08-24-probono-stretch-projects/index.html#final-thoughts",
    "href": "posts/2021-08-24-probono-stretch-projects/index.html#final-thoughts",
    "title": "Pro Bono Work for Stretch Projects",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWhen learning something new, the best way to master it is by applying it in the real world on something that matters. That could be something within your current job, but oftentimes what you are learning are skills you want to grow into a different job or career. In order to gain experience in those skills before making that job switch, consider volunteering for a non-profit. They would be thrilled to have you."
  },
  {
    "objectID": "posts/2021-08-17-screenplays-vscode/index.html",
    "href": "posts/2021-08-17-screenplays-vscode/index.html",
    "title": "Writing Screenplays in VS Code",
    "section": "",
    "text": "Today there are a lot of tools for people to write screenplays. Most of them are paid and require hundreds of dollars. I recently stumbled across a VS Code add called Better Fountain that allows you to turn simple markdown documents into a professionally formatted screenplay. It is built upon an open source tool called Fountain, that is a bare bones screenwriting tool that is free and easy to use.\nWhy the heck would you want to write a screenplay is VS Code? I was asking myself the same thing. If you come from a coding background, this is an easy transition. Heck, I’m writing this post currently in a markdown file within VS Code 😎. What I think is most interesting is the tools we use to code can now be applied to the older art of storytelling with screenwriting.\nThe biggest tool most coders leverage is some sort of version control. VS Code easily integrates with GitHub, so you can push your latest script (screenplay) changes to version control and track their changes with ease. Side note, kinda funny how folks in Hollywood call their screenplays “scripts”. Guess coding and storytelling are more alike then we thought.\nWhat makes the idea of hosting your script on GitHub is how it could be seen by others. It could be a public repository, and essentially open source. An open source movie? That would be crazy. What if others could read the script and make updates through pull requests? It could be the best version of fan fiction. What if others could read the script, upvote it in some way (using GitHub stars) to catch the eyes of top directors and film studios? That way a movie could already be audience approved beforehand.\nThis becomes very interesting when you add in the idea of block chain and smart contracts. If a movie studio wants to pick it up and take it into production, everyone who contributed to the script in the GitHub repo could automatically receive some sort of pre-arranged compensation when the movie gets made. There could even be new types of open source licenses built for these kinds of projects that have specified terms around compensating people who made the project possible.\nThere could also be some kind of crypto coin created for the repo, which would allow movie studios to buy into the project and either fund change requests to the script or set up a subsequent sequel or second season. The possibilities get more exciting the more I think about them.\nTo close this out, let’s list out everything that makes this a horrible idea. The biggest thing that comes to mind is that people don’t want to know what’s going to happen in a movie or tv show. Having the script publicly available somewhere like GitHub could ruin any twists or cliffhangers in a script, but at the same time we run into the same problem today if someone watches a movie anytime after opening day. Everyone knew the plot to Harry Potter before each movie was released, but people lined up around the corner to go watch them anyway. Maybe open source movies are not such a bad thing after all."
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "",
    "text": "Machine Learning (ML) is exploding, we are only limited by our imagination of how to bake ML into every aspect of our organizations. Within corporate finance the opportunities are endless. Most finance activities can be aided by algorithms.\n\nBudgeting and Forecasting\nAccount Reconciliation\nContract Risk Analysis\nA Million Other Things\n\nWhen finance departments start on their modern finance journey with machine learning, they might tackle the highest impact projects that could benefit the business the most. These will probably revolve around forecasting revenue and various balance sheet risk management solutions.\nWho implements these kinds of game changes in the early days of ML adoption? Help usually comes from these areas.\n\nExisting data scientists that work in another department\nConsultants that come in, build something, then leave\nVendors/Contractors you hire for an extended period of time\n\nLeveraging help from outside of the finance org can get the ball rolling quickly for those big ML solutions that have the most immediate impact. This process usually yields great results. Revenue is forecasted more accurately, and financial risk is better handled across various aspects of the business. Everyone gets a pat on the back and a pep in their step. This initial momentum creates a lot of energy, which stirs up new ideas of where ML can be applied next. This is when you fall into a hidden trap around trying to scale ML in a non-technical discipline like finance.\n\nsource"
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html#getting-the-ball-rolling",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html#getting-the-ball-rolling",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "",
    "text": "Machine Learning (ML) is exploding, we are only limited by our imagination of how to bake ML into every aspect of our organizations. Within corporate finance the opportunities are endless. Most finance activities can be aided by algorithms.\n\nBudgeting and Forecasting\nAccount Reconciliation\nContract Risk Analysis\nA Million Other Things\n\nWhen finance departments start on their modern finance journey with machine learning, they might tackle the highest impact projects that could benefit the business the most. These will probably revolve around forecasting revenue and various balance sheet risk management solutions.\nWho implements these kinds of game changes in the early days of ML adoption? Help usually comes from these areas.\n\nExisting data scientists that work in another department\nConsultants that come in, build something, then leave\nVendors/Contractors you hire for an extended period of time\n\nLeveraging help from outside of the finance org can get the ball rolling quickly for those big ML solutions that have the most immediate impact. This process usually yields great results. Revenue is forecasted more accurately, and financial risk is better handled across various aspects of the business. Everyone gets a pat on the back and a pep in their step. This initial momentum creates a lot of energy, which stirs up new ideas of where ML can be applied next. This is when you fall into a hidden trap around trying to scale ML in a non-technical discipline like finance.\n\nsource"
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html#what-got-you-here-wont-get-you-there",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html#what-got-you-here-wont-get-you-there",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "What Got You Here Won’t Get You There",
    "text": "What Got You Here Won’t Get You There\nYou quickly run into issues with the current business model of outside help. The more solutions you want to build, the more resources you need to build them. ML solutions expand on a linear scale. This linear approach runs into a myriad issues.\n\nData scientist teams in other orgs already have a job, and can only offer so much support.\nCosts grow with each new solution. Existing solution work never goes away. There will always be maintenance to constantly update data sources and improve the model as the business evolves. More solutions, more problems.\nPrinciple-Agent problem with vendors. External resources want to make sure they are always needed, which means they will build solutions in a way to ensure new solutions continue to scale linearly. New innovation around scaling solutions or democratization through tools may never come, since that removes them from the picture.\n\nIn addition to scale issues, there is another large problem of domain expertise. Most outside data scientists don’t understand the work of finance people. Those that claim they have experience in building finance solutions do not understand your company and its various nuances. This creates the need for program manager (PM) roles to be created in order to gather the domain expertise of finance users and translate it to these outside resources to build solutions. More layers of abstraction lead to longer dev times, more communication overhead, and just more everything. The linear approach is a short-term solution to a long-term problem.\n\nLeveraging data science talent outside of finance is a short-term solution to a long-term problem.\n\nYou could also entertain the idea of hiring these vendors and other outside talent as full-time employees to sit within the finance department. This might seem to solve some of the problems called out above, but it still doesn’t solve the long-term problem. These outside data scientists turned finance employees may not have finance backgrounds, and in some respects are “data mercenaries”. They could be people that enjoy building ML solutions but do not care who benefits from them or if they are solving the right problems within finance.\nThere’s got to be a better way!\n\nsource"
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html#long-term-strategy",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html#long-term-strategy",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "Long-Term Strategy",
    "text": "Long-Term Strategy\nThe solution has been right under our nose this entire time. The best people are already within your company, and are already experts in finance. Yes, I’m talking about your finance employees! What I’m proposing is to find the data scientists who don’t know they are data scientists yet, the diamonds in the rough.\nThe diamonds I’m talking about usually fit a familiar archetype, which you may have already seen within your finance employees.\n\nLike to build things. They hate repeating themselves, and often take pride in the large financial models they have built in excel. They might already know a little bit of programming, whether that’s excel macros using VBA or having a little python knowledge to break out whenever they feel constrained by the four walls of excel.\nConstantly learning. They are the definition of a growth mindset. Being a “learn it all” is more important than being a “know it all” kind of person.\nMaster communicator. This one is typically harder to find. Having someone who loves to build, and then can turn around and sell it to others is hard to do. Being able to explain complex technical subjects is a super power itself.\n\nIf you send out the bat signal looking for these types of folks, many will come running. Others may be more hesitant to reach out. Imposter syndrome could be the biggest reason these diamonds stay in the rough. They could be a rock star within the org, but they may not see themselves that way. So always be on the lookout for talent, even if they don’t see it in themselves.\nCreating a path from regular finance employee to data scientist is a hard path to build out. This is a problem I have been thinking about for some time, and more details will be provided in a future blog post. At a high level, an apprenticeship program is one that I personally see as a path to building finance centered data science teams. Most of us forgot about this model of learning, and think it’s only for electricians and plumbers today. I think it’s making a comeback in a big way, and look forward to detailing how I plan to implement it within my finance org. The best part is, once I do I will report back with information around how you can do it too!"
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html#final-thoughts",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html#final-thoughts",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIn conclusion, scaling out data science work within large finance orgs is tough. What initially got you started isn’t what will take you to the next level. There are many pitfalls along the way, and it’s not an easy process. What I hope to do is implement a system for growing data science talent that already exist within finance, then turn around and share those insights with the rest of the world. Stay tuned!"
  },
  {
    "objectID": "posts/2021-08-10-bridging-ml-gap/index.html#reversal",
    "href": "posts/2021-08-10-bridging-ml-gap/index.html#reversal",
    "title": "Bridging the ML Talent Gap in Corporate Finance",
    "section": "Reversal",
    "text": "Reversal\n\nFor smaller finance orgs (&lt;50 employees), leveraging outside data science talent may be your best strategy. There are only a few places that ML can be implemented, and having a full time team on-site may not be the best financial decision.\nHiring external data science help as full time employees could work just fine if you find people who are passionate about solving problems within the business. They may not always be data mercenaries and could have existing business backgrounds in addition to data science skills.\nThere has been a lot of innovation recently in no-code tools that enable anyone to produce a machine learning model for a variety of tasks. This democratization leads to more exponential adoption of ML, as opposed to the linear adoption curve. This can post it’s own risks, where cookie cutter solutions don’t fit the need to every problem. Once you do need a more custom solution based on the needs of the business, you are right back at square one in using outside help. Custom solutions and no-code tools will merge into a happy harmony one day, and will intermingle as self-serve solutions need to be transformed into custom ones managed by a data science team.\nIt will take time to grow existing finance talent into data science roles, and it may not work out. There is some downside to this approach, but it comes with potentially unlimited upside when you empower the domain experts to build things they think are useful.\nLack of career path and poor engineering practices are likely to spring up after going down this paths. This is something to think a lot about, and is something I plan to address in a future post around building an apprenticeship program."
  },
  {
    "objectID": "notebooks/2025-02-05-ts-fundamentals-data-cleaning-boxcox.html",
    "href": "notebooks/2025-02-05-ts-fundamentals-data-cleaning-boxcox.html",
    "title": "Python Code",
    "section": "",
    "text": "# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import boxcox\nfrom scipy.special import inv_boxcox\n\n\n# create a simple monthly time series with a linear trend and no noise\nnp.random.seed(0)\nn = 100\nx = np.arange(n)\ny = 0.5 * x\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# plot the time series\nplt.figure(figsize=(10, 6))\nplt.plot(df['x'], df['y'])\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Simple Linear Time Series')\n\n# save the plot\n# plt.savefig(\"chart1\", dpi = 300, bbox_inches = \"tight\")\n\n\n# read data\ndata_raw = pd.read_csv(\"../posts/2024-10-02-ts-fundamentals-whats-a-time-series/example_ts_data.csv\")\n\ndata_raw = (\n    # select columns\n    data_raw[[\"Country\", \"Product\", \"Date\", \"Revenue\"]]\n    # change data types\n    .assign(\n        Date = pd.to_datetime(data_raw[\"Date\"]), \n        Revenue = pd.to_numeric(data_raw[\"Revenue\"])\n    )\n)\n\n# print the first few rows\nprint(data_raw.head())\n\n\n# filter on specific series\ncd_ic_raw = data_raw[(data_raw[\"Country\"] == \"Canada\") & (data_raw[\"Product\"] == \"Ice Cream\")]\n\ncd_ic_raw.set_index(\"Date\", inplace=True)\n\nprint(cd_ic_raw.head())\n\n# plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(cd_ic_raw.index, cd_ic_raw[\"Revenue\"], label=\"Ice Cream Revenue\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.title(\"Ice Cream Revenue in Canada\")\nplt.legend()\n\n# save the plot\n# plt.savefig(\"chart2\", dpi = 300, bbox_inches = \"tight\")\n\n\n# copy data\ncd_ic_bc = cd_ic_raw.copy()\n\n# apply Box-Cox transformation with lambda = 0\ncd_ic_bc[\"Revenue\"]= boxcox(x = cd_ic_bc[\"Revenue\"], lmbda = 0.0)\n\n# plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(cd_ic_bc.index, cd_ic_bc[\"Revenue\"], label=\"Ice Cream Revenue\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Transformed Revenue\")\nplt.title(\"Box-Cox Transformed Data (Lambda = 0)\")\nplt.legend()\n\n# save the plot\n# plt.savefig(\"chart3\", dpi = 300, bbox_inches = \"tight\")\n\n\n# copy data\ncd_ic_bc = cd_ic_raw.copy()\n\n# apply Box-Cox transformation with lambda = 0.5\ncd_ic_bc[\"Revenue\"]= boxcox(x = cd_ic_bc[\"Revenue\"], lmbda = 0.5)\n\n# plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(cd_ic_bc.index, cd_ic_bc[\"Revenue\"], label=\"Ice Cream Revenue\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Transformed Revenue\")\nplt.title(\"Box-Cox Transformed Data (Lambda = 0.5)\")\nplt.legend()\n\n# save the plot\n# plt.savefig(\"chart4\", dpi = 300, bbox_inches = \"tight\")\n\n\n# copy data\ncd_ic_bc = cd_ic_raw.copy()\n\n# set random seed for reproducibility\nnp.random.seed(100)\n\n# apply Box-Cox transformation with lambda = None\ncd_ic_bc[\"Revenue\"], lambda_ = boxcox(x = cd_ic_bc[\"Revenue\"])\nprint(lambda_)\n\n# plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(cd_ic_bc.index, cd_ic_bc[\"Revenue\"], label=\"Ice Cream Revenue\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Transformed Revenue\")\nplt.title(\"Box-Cox Transformed Data (Estimated Lambda = -0.0017)\")\nplt.legend()\n\n# save the plot\n# plt.savefig(\"chart5\", dpi = 300, bbox_inches = \"tight\")\n\n\n# inverse Box-Cox transformation\ncd_ic_bc[\"Revenue\"] = inv_boxcox(cd_ic_bc[\"Revenue\"], lambda_)\nprint(cd_ic_bc.head())\n\n# plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(cd_ic_bc.index, cd_ic_bc[\"Revenue\"], label=\"Ice Cream Revenue\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.title(\"Ice Cream Revenue in Canada (Original Scale)\")\nplt.legend()"
  },
  {
    "objectID": "notebooks/2024-11-26-ts-fundamentals-missing-outliers.html",
    "href": "notebooks/2024-11-26-ts-fundamentals-missing-outliers.html",
    "title": "Python Code",
    "section": "",
    "text": "# import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom statsmodels.tsa.seasonal import STL\nfrom statsmodels.tsa.deterministic import Fourier\nfrom scipy.interpolate import interp1d\n\n\n# read data\ndata_raw = pd.read_csv(\"../posts/2024-10-02-ts-fundamentals-whats-a-time-series/example_ts_data.csv\")\n\ndata_raw = (\n    # select columns\n    data_raw[[\"Country\", \"Product\", \"Date\", \"Revenue\"]]\n    # change data types\n    .assign(\n        Date = pd.to_datetime(data_raw[\"Date\"]), \n        Revenue = pd.to_numeric(data_raw[\"Revenue\"])\n    )\n)\n\n# print the first few rows\nprint(data_raw.head())\n\n\n# filter on specific series\nus_ck_raw = data_raw[(data_raw[\"Country\"] == \"United States\") & (data_raw[\"Product\"] == \"Cookies\")]\n\nus_ck_raw.set_index(\"Date\", inplace=True)\n\nprint(us_ck_raw.head())\n\n\n# randomly replace some revenue values with NaN\nnp.random.seed(123)\n\nus_ck_mv = us_ck_raw.copy()\n\nus_ck_mv.loc[us_ck_mv.sample(frac=0.03).index, \"Revenue\"] = np.nan\n\nprint(us_ck_mv.head())\n\n\n# plot the series\nplt.figure(figsize=(10, 6))\nplt.plot(us_ck_mv.index, us_ck_raw[\"Revenue\"], label = \"Cookie Revenue\", color = \"blue\")\nplt.title(\"US Cookie Revenue - with Missing Values\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.grid(True)\n\n# save the plot\n# plt.savefig(\"chart1\", dpi = 300, bbox_inches = \"tight\")\n\n\n# Function to interpolate missing values\ndef na_interp(ts, freq=None, lambda_=None, use_linear=None):\n    # Ensure ts is a pandas Series\n    if not isinstance(ts, pd.Series):\n        raise ValueError(\"Input `ts` must be a pandas Series.\")\n\n    # Handle cases where there are no missing values\n    if ts.isna().sum() == 0:\n        return ts\n\n    # Frequency determination\n    if freq is None:\n        freq = pd.infer_freq(ts.index) or 1\n\n    # Automatic linear interpolation decision\n    if use_linear is None:\n        use_linear = freq &lt;= 1 or ts.notna().sum() &lt;= 2 * freq\n\n    # Apply Box-Cox transformation if lambda_ is provided\n    def boxcox_transform(series, lmbda):\n        if lmbda is None:\n            return series\n        if lmbda == 0:\n            return np.log(series)\n        return (series ** lmbda - 1) / lmbda\n\n    def inv_boxcox_transform(series, lmbda):\n        if lmbda is None:\n            return series\n        if lmbda == 0:\n            return np.exp(series)\n        return (series * lmbda + 1) ** (1 / lmbda)\n\n    ts_original = ts.copy()\n    ts = ts.astype(float)\n    if lambda_ is not None:\n        ts = boxcox_transform(ts, lambda_)\n\n    if use_linear:\n        # Linear interpolation\n        ts_interpolated = ts.interpolate(method='linear', limit_direction='both')\n    else:\n        # Seasonal interpolation\n        ts_filled = ts.interpolate(method='linear', limit_direction='both')  # Pre-fill gaps for STL\n        stl = STL(ts_filled, seasonal=freq, robust=True).fit()\n        \n        # Interpolate seasonally adjusted series\n        sa = stl.trend + stl.resid  # Seasonally adjusted component\n        \n        # Convert DatetimeIndex to numeric values for interpolation\n        idx_numeric = ts.index.astype(int) / 10**9  # Convert to seconds since epoch\n        sa_interp_func = interp1d(\n            idx_numeric[~ts.isna()], \n            sa[~ts.isna()], \n            bounds_error=False, \n            fill_value=\"extrapolate\"\n        )\n        sa_interp = sa_interp_func(idx_numeric)\n\n        # Add back the seasonal component\n        ts_interpolated = pd.Series(sa_interp + stl.seasonal, index=ts.index)\n\n    # Back-transform if Box-Cox was applied\n    if lambda_ is not None:\n        ts_interpolated = inv_boxcox_transform(ts_interpolated, lambda_)\n\n    # Check for stability (fallback to linear if unstable)\n    if not use_linear:\n        ts_range = ts_original.dropna().max() - ts_original.dropna().min()\n        if (ts_interpolated.max() &gt; ts_original.max() + 0.5 * ts_range or \n            ts_interpolated.min() &lt; ts_original.min() - 0.5 * ts_range):\n            return na_interp(ts_original, freq=freq, lambda_=lambda_, use_linear=True)\n\n    return ts_interpolated\n\n\n# get the series with missing values and convert to float\nus_ck_missing = us_ck_mv[\"Revenue\"]\nus_ck_missing = np.array(us_ck_missing, dtype=float)\n\n# get the monthly date range, using first day of month\ndate_range = pd.date_range(start=\"2019-01-01\", periods=60, freq=\"M\")  # Monthly frequency\n\n# Create a pandas Series\nts = pd.Series(us_ck_missing, index=date_range)\n\n# Interpolating missing values\nts_interpolated = na_interp(ts, freq=11)\n\n# Plot the results, with origin on top\nplt.figure(figsize=(10, 6))\nplt.plot(ts_interpolated, label=\"Interpolated\", color=\"red\", linestyle=\"--\")\nplt.plot(ts, label=\"Original\", color=\"blue\")\nplt.legend()\nplt.grid(True)\nplt.title(\"US Cookie Revenue - with Missing Values Interpolated\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\n\n# save the plot\n# plt.savefig(\"chart2\", dpi = 300, bbox_inches = \"tight\")\n\n\n# STL decomposition\nstl = STL(us_ck_raw[\"Revenue\"], seasonal = 13, period = 12)\nres = stl.fit()\n\n# plot the decomposition in a stacked chart\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize = (10, 8))\n\n# original data\nax1.plot(res.observed, label = \"Original\", color = \"blue\")\nax1.set_title(\"Original Data\")\nax1.set_ylabel(\"Revenue\")\nax1.grid(True)\nax1.legend()\n\n# trend\nax2.plot(res.trend, label = \"Trend\", color = \"red\")\nax2.set_title(\"Trend\")\nax2.set_ylabel(\"Revenue\")\nax2.grid(True)\nax2.legend()\n\n# seasonal\nax3.plot(res.seasonal, label = \"Seasonal\", color = \"green\")\nax3.set_title(\"Seasonal\")\nax3.set_ylabel(\"Revenue\")\nax3.grid(True)\nax3.legend()\n\n# residual\nax4.plot(res.resid, label = \"Residual\", color = \"orange\")\nax4.set_title(\"Residual\")\nax4.set_xlabel(\"Date\")\nax4.set_ylabel(\"Revenue\")\nax4.grid(True)\nax4.legend()\n\n# Formatting and final touches\nplt.xlabel(\"Date\")\nplt.tight_layout()\n\n# save the plot\n# plt.savefig(\"chart3\", dpi = 300, bbox_inches = \"tight\")\n\n\n# calculate outliers based on residuals\nresiduals = res.resid\nresiduals_mean = residuals.mean()\nresiduals_std = residuals.std()\n\n# calculate the z-score\nz_score = (residuals - residuals_mean) / residuals_std\n\n# identify the outliers\noutliers = z_score[abs(z_score) &gt; 3]\n\n# plot the residuals\nplt.figure(figsize=(10, 6))\nplt.plot(residuals, label = \"Residuals\", color = \"orange\")\n# plt.axhline(y = 0, color = \"black\", linestyle = \"--\")\nplt.axhline(y = residuals_mean + 3 * residuals_std, color = \"red\", linestyle = \"--\")\nplt.axhline(y = residuals_mean - 3 * residuals_std, color = \"red\", linestyle = \"--\")\n# plt.scatter(outliers.index, outliers, color = \"red\", label = \"Outliers\")\nplt.title(\"Residuals with Outliers\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.legend()\nplt.grid(True)\n\n# save the plot\n# plt.savefig(\"chart4\", dpi = 300, bbox_inches = \"tight\")"
  },
  {
    "objectID": "notebooks/2024-11-06-ts-fundamentals-decomposition.html",
    "href": "notebooks/2024-11-06-ts-fundamentals-decomposition.html",
    "title": "Python Code",
    "section": "",
    "text": "# import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# read data\ndata_raw = pd.read_csv(\"../posts/2024-10-02-ts-fundamentals-whats-a-time-series/example_ts_data.csv\")\n\ndata_raw = (\n    # select columns\n    data_raw[[\"Country\", \"Product\", \"Date\", \"Revenue\"]]\n    # change data types\n    .assign(\n        Date = pd.to_datetime(data_raw[\"Date\"]), \n        Revenue = pd.to_numeric(data_raw[\"Revenue\"])\n    )\n)\n\n# print the first few rows\nprint(data_raw.head())\n\n\n# filter on specific series\nus_cookie_raw = data_raw[(data_raw[\"Country\"] == \"United States\") & (data_raw[\"Product\"] == \"Cookies\")]\n\nus_cookie_raw.set_index(\"Date\", inplace=True)\n\nprint(us_cookie_raw.head())\n\n\n# plot the data\nplt.figure(figsize=(10,6))\nplt.plot(us_cookie_raw.index, us_cookie_raw[\"Revenue\"], label = \"Cookies Revenue\", color = \"blue\")\nplt.title(\"US Cookie Revenue\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.grid(True)\n\n\n# calculate the 2x12 month moving average\nus_cookie_ma = us_cookie_raw.copy()\n\nus_cookie_ma[\"2x12_MA\"] = us_cookie_ma[\"Revenue\"].rolling(window = 12, center = True).mean().rolling(window=2, center = True).mean()\n\nprint(us_cookie_ma.head())\n\n\n# plot the moving average\nplt.figure(figsize=(10,6))\nplt.plot(us_cookie_ma.index, us_cookie_ma[\"Revenue\"], label = \"Original Revenue\", color = \"blue\")\nplt.plot(us_cookie_ma.index, us_cookie_ma[\"2x12_MA\"], label = \"2x12 Moving Average\", color = \"red\")\n\nplt.title(\"United States - Cookie Revenue Trend\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.grid(True)\nplt.legend()\n\nax = plt.gca()\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n\n# save the plot\n# plt.savefig(\"chart3\", dpi = 300, bbox_inches = \"tight\")\n\n\n# detrend the data\nus_cookie_detrend = us_cookie_ma.copy()\n\nus_cookie_detrend[\"Detrended\"] = us_cookie_detrend[\"Revenue\"] - us_cookie_detrend[\"2x12_MA\"]\n\nprint(us_cookie_detrend.head(20))\n\n\n# plot the detrended data\nplt.figsize=(10,6)\nplt.plot(us_cookie_detrend.index, us_cookie_detrend[\"Detrended\"], label = \"Detrended Revenue\", color = \"purple\")\nplt.title(\"United States - Cookie Revenue Detrended\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.grid(True)\nplt.legend()\n\nax = plt.gca()\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n\n# make date easier to read\nplt.gcf().autofmt_xdate()\n\n# save \nplt.savefig(\"chart4\", dpi = 300, bbox_inches = \"tight\")\n\n\n# calculate the seasonal component\nus_cookie_seasonal = us_cookie_detrend.copy()\nus_cookie_seasonal[\"Seasonal\"] = us_cookie_seasonal[\"Detrended\"].groupby(us_cookie_seasonal.index.month).transform(\"mean\")\n\n# divide by mean across all months to have it sum to zero\nus_cookie_seasonal[\"Seasonal\"] = us_cookie_seasonal[\"Seasonal\"] / us_cookie_seasonal[\"Seasonal\"].mean()\n\n\n# plot the seasonal component\nplt.figure(figsize=(10,6))\nplt.plot(us_cookie_seasonal.index, us_cookie_seasonal[\"Seasonal\"], label = \"Seasonal Component\", color = \"green\")\nplt.title(\"United States - Cookie Revenue Seasonality\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.grid(True)\nplt.legend()\n\nax = plt.gca()\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n\n# save\n# plt.savefig(\"chart5\", dpi = 300, bbox_inches = \"tight\")\n\n\n# calculate the residual component\nus_cookie_residual = us_cookie_seasonal.copy()\nus_cookie_residual[\"Residual\"] = us_cookie_residual[\"Detrended\"] - us_cookie_residual[\"Seasonal\"]\n\n# plot the residual component\nplt.figure(figsize=(10,6))\nplt.plot(us_cookie_residual.index, us_cookie_residual[\"Residual\"], label = \"Residual Component\", color = \"orange\")\nplt.title(\"United States - Cookie Revenue Residual\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.grid(True)\nplt.legend()\n\nax = plt.gca()\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n\n# save\n# plt.savefig(\"chart6\", dpi = 300, bbox_inches = \"tight\")\n\n\n# STL decomposition\nfrom statsmodels.tsa.seasonal import STL\n\nstl = STL(us_cookie_raw[\"Revenue\"], seasonal = 13, period = 12)\nres = stl.fit()\n\n# plot the decomposition in a stacked chart\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize = (10, 8))\n\n# original data\nax1.plot(res.observed, label = \"Original\", color = \"blue\")\nax1.set_title(\"Original Data\")\nax1.set_ylabel(\"Revenue\")\nax1.grid(True)\nax1.legend()\n\n# trend\nax2.plot(res.trend, label = \"Trend\", color = \"red\")\nax2.set_title(\"Trend\")\nax2.set_ylabel(\"Revenue\")\nax2.grid(True)\nax2.legend()\n\n# seasonal\nax3.plot(res.seasonal, label = \"Seasonal\", color = \"green\")\nax3.set_title(\"Seasonal\")\nax3.set_ylabel(\"Revenue\")\nax3.grid(True)\nax3.legend()\n\n# residual\nax4.plot(res.resid, label = \"Residual\", color = \"orange\")\nax4.set_title(\"Residual\")\nax4.set_xlabel(\"Date\")\nax4.set_ylabel(\"Revenue\")\nax4.grid(True)\nax4.legend()\n\n# Formatting and final touches\nplt.xlabel(\"Date\")\nplt.tight_layout()\n\n# save\n# plt.savefig(\"chart7\", dpi = 300, bbox_inches = \"tight\")"
  },
  {
    "objectID": "health.html",
    "href": "health.html",
    "title": "How I Try to Stay Healthy",
    "section": "",
    "text": "Diet\n\nWhole foods, no processed foods. Limit seed oils.\n0.7-1 gram of protein per ideal body weight, so around 150 grams per day.\nEating window between 8am and 6pm, but will also eat later for social events.\n\n\n\nSupplements\n\nEssentials for Everyone\n\nGlucose Control Probiotic by Pendulum\nCreatine Monohydrate by Momentous\nOmega 3 Fish Oil by Momentous\nVitamin D3 by Momentous\nMethylated Multivitamin by 10X Health\nMagnesium Breakthrough by BIOptimizers\nPerfectAmino by BodyHealth\nPrebiotic Fiber by Supergut\nPrebiotic Fiber by Bulletproof\nMineral Sea Salt by Baja Gold\nWhey Protein by ProMix\n\nSpecific to My Biomarkers\n\nTMG by 10X Health\n5-MTHF by 10X Health\nRed Yeast Extract by Blueprint\nBerberine by Thorne\n\n\n\n\nWater Filtration\n\nCountertop Reverse Osmosis Filter by AquaTru\nShower Head Filter by Jolie\n\n\n\nHealth Tracking\n\nBlood Panel Testing by Function\nMicrobiome Testing by Viome\nGenetic Methylation Testing by 10x Health\nBlood Glucose Monitor by Dexcom\nWhoop\nEight Sleep\n\n\n\nExercise\n\n10k steps every day\nZone 2 cardio 3x per week\n\nPeloton cycle on Tuesday/Thursday/Saturday\n\n25 min of moderate intensity\n5 min of HIIT, 30 seconds max then 30 seconds rest\n\nRucking once during the week with 45 pound weight for 1-2 miles\n\nStrength training 4x per week\n\n4 week mesocycle\n\nFirst 3 weeks of progressive overload\nLast week of deload with half the weight, half the reps, and half the sets\n\nUpper body on Sunday/Wednesday\n\nPull ups\nPush ups\nOverhead tricep extensions\nBicep curls\nBent over row\nChest fly\nArnold press\nShoulder fly\nShoulder Shrugs\nBicycle Kicks\nV-ups\nCrunches\n\nLower body on Monday/Friday\n\nWeighted knees over toes front lunges\nTibialis raises\nRomanian deadlift\nFront squat\nStanding single leg calf raise\nClamshell raise\nGlute bridges\nStanding clamshell band and band step back\nSide plank\nBicycle kicks\nV-ups\n\n\nAll training tracked via Whoop\n\n\n\nMorning Routine\n\nWake up at 5:00am\nOral care\n\nToothpaste\nTongue Scraper\nMouthwash\n\nMake bullet proof coffee\n\nDecaf and mold free coffee, FYI I try to limit caffein until 90-120 minutes after waking and stop 10-12 hours before bed\n2 tablespoons of grassfed better\n2 tablespoons of C8 coconut MCT oil\nAll mixed together in a blender for 20 seconds (will not have the same effect if not blended)\n\nTake perfect aminos and creatine\nGet into bathtub with water at 50 degrees and stay for 3 minutes\nPerform exercise of the day\nConsume another serving of perfect aminos after exercise\nShower and get ready for work\nMoisturize\n5 minutes of Wim Hoff style breathwork using an app called State\nJournal using an app called Day One, answering the following prompts\n\nI am grateful for…\nMorning exercise\nWhat’s the most important question for today?\nWhat will you have a sense of urgency about today?\nWhat will you have a bias for action about today?\nWhat would make today great?\nDaily affirmation\n\nWrite for 30-60 minutes about anything that interests me\nEat first meal at 8am and take rest of supplements (excluding supplements taken during evening routine)\nKick ass the rest of the day\n\n\n\nEvening Routine\n\n6:30pm put on blue light blocking glasses\n7:30pm dim all lights, stop looking at screens, and take the following supplements\n\n5-MTHF\nMagnesium Breakthrough\n\n7:40pm do the following cognitive work\n\n10 minutes of parasympathetic inducing breathwork using an app called State\n10 minutes of meditation using an app called The Way\n\n8:30pm take a contrast shower\n\nFew minutes on very hot\nLast minute on very cold\n\nOral care\n\nFloss\nToothpaste\nTongue Scraper\nMouthwash\nCoconut Oil Pulling\n\nMoisturize\nFall asleep by 9:00pm"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi there, my name’s Mike Tokic. I currently work at Microsoft where I help build things with machine learning. Here are my thoughts….on things. These thoughts are my personal opinions and do not reflect the ideas/beliefs of my employer. There, now I can say whatever I want."
  },
  {
    "objectID": "about.html#origin-story",
    "href": "about.html#origin-story",
    "title": "About Me",
    "section": "Origin Story",
    "text": "Origin Story\nIn the summer of 2015, I was your run of the mill finance intern at Microsoft. Bright eyed and bushy tailed, I knew a little bit about business and almost everything about technology. Just kidding I knew nothing.\nWithin a span of two weeks my life changed forever.\nFirst, I saw a demo of Azure Machine Learning, and second our CFO Amy Hood told all the interns that AI will become very important in Finance one day. That was all I needed to hear to dedicate my life to figuring this AI and machine learning thing out, starting my quest to get data superpowers.\nOver the next few years, I taught myself how to use data and AI to help our business make better decisions. Successfully transitioning from a finance to engineering career track. I’ve helped democratize AI so everyone in finance can have data superpowers, no coding required. Nothing gets me more excited than building technology that fundamentally changes how people live their lives and giving other people data superpowers."
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "About Me",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nMicrosoft\n\nSenior Software Engineer\nOctober 2018 – Present\n\nLead advanced analytics efforts, building machine learning solutions for finance.\n\nCreated a self-serve machine learning forecasting tool, replacing $450 billion of manual forecasts and saving $3 million annually.\n\nDeveloped and maintained machine learning APIs and pipelines using Azure ML and Synapse.\n\nManaged and mentored both interns and full-time employees, often upskilling direct reports with business backgrounds in data analytics and machine learning.\n\nDesigned and ran an apprenticeship program for machine learning, mentoring over 75 early-career employees.\n\nRecognized as a top expert in AI and machine learning, presenting to CFOs and winning Modern Finance awards: Inspiring Teacher (2022) and Outstanding Accelerator (2023).\n\n\n\nFinance Rotation Analyst\nJuly 2016 – October 2018 (2 years 4 months)\n1st Rotation: Windows Finance COGS\n\nOwned reporting for Windows royalties COGS, managing budgets, forecasts, and close analysis.\n\nBuilt a long-range planning model for multi-year business discussions.\n\nDeveloped an intellectual property model used in mobile phone licensing deal negotiations by the senior legal team.\n\n2nd Rotation: Worldwide Commercial Solutions Finance\n\nBuilt Power BI reports used by senior leaders, training team members on the platform.\n\nLed analysis of a new data source revealing customer purchase patterns and developed a transition plan for its adoption.\n\n3rd Rotation: Finance Business Intelligence Services\n\nProject managed the onboarding of a new centralized field forecast model for subsidiary finance leaders, saving $2.4 million annually.\n\n4th Rotation: Office/Dynamics/Bing Finance Business Intelligence\n\nPartnered with a sales data science team to develop machine learning models predicting deal closures for sales pipeline forecasting.\n\nBuilt an R-based driver machine learning model to forecast Search revenue.\n\nImplemented natural language processing in Python to analyze United Nations policy PDFs for the Microsoft Airband initiative."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nThe University of Kansas\nBachelor’s Degree in Finance (2012 – 2016)\nMicrosoft Professional Program for Data Science\nData Science Certification (2017 – 2018)\nDataCamp\nData Scientist with Python Certification (2017)"
  },
  {
    "objectID": "about.html#awards-honors",
    "href": "about.html#awards-honors",
    "title": "About Me",
    "section": "Awards & Honors",
    "text": "Awards & Honors\n\nMost Outstanding Senior in Finance Award\n\nCFA Institute Research Challenge Champion\n\nFinance Scholars Program\nBusiness Honors Program\n\nPhi Kappa Psi Endowment Scholarship"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books That Changed My Life",
    "section": "",
    "text": "Better Life\n\nThe Alamanack of Naval Ravikant\n\nReview\n\nTools of Titans\nThe Essential Wooden\nThe Anthology of Balaji\nSame as Ever\nThe Psychology of Money\nThe 4-Hour Workweek\nTribe of Mentors\nDie With Zero\nScarcity Brain\nThe Comfort Crisis\nClear Thinking\nBe Useful\nHow to Win Friends and Influence People\nThe Art of Learning\nVagabonding\nTuesday’s With Morrie\nRange\nMastery\nFour Thousand Weeks\nThe Great Mental Models\nExtreme Ownership\nThe Dichotomy of Leadership\nThe Coddling of the American Mind\nThe Anxious Generation\nExcellent Advice for Living\nStillness is the Key\nThe Obstacle is the Way\nEgo is the Enemy\nThe Algebra of Happiness\nEssentialism\nPrinciples\nWalkable City\nDiscipline Equals Freedom\nAtomic Habits\nThe 7 Habits of Highly Effective People\nA Mind for Numbers\nDrive\nThe Algebra of Wealth\nStoryworthy\nMake it Stick\nSave the Cat\n\n\n\nBusiness and Technology\n\nZero to One\nLoonshots\nWhat’s The Future\nHow Google Works\nTrillion Dollar Coach\nHow Innovation Works\n\nRead This Before Our Next Meeting\nBuild\nThe Inevitable\nThe New One Minute Manager\nCreativity Inc\nGood to Great\nRework\n\n\n\nHistory\n\nGuns, Germs, and Steel\nCoders\nKitchen Confidential\nHatching Twitter\nElon Musk\n\nOne\nTwo\n\nLet My People Go Surfing\nDon’t Tell Me I Can’t\nThe Promise of a Pencil\nTotal Recall\nEmpire of the Summer Moon\nSurely You’re Joking Mr. Feynman!\nNatural Born Heroes\nEat & Run\nThe New Jim Crow\nPermanent Record\nIn the Weeds\nFear and Loathing in Las Vegas\nSapiens\nHomo Deus\nSteve Jobs\nGreenlights\nNot Fade Away\nInto the Wild\n\n\n\nHealth\n\nBorn to Run\nBreath\nThe Four Hour Body\nMagic Pill\n\n\n\nFiction\n\nHarry Potter\nThe Alchemist\nReady Player One\n\nShantaram\nRed Rising\nThe Chronicles of Narnia\nThe Graveyard Book"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thoughts on Things",
    "section": "",
    "text": "Univariate Models For Time Series\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nFoundational models in forecasting\n\n\n\n\n\nMar 24, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nData Cleaning: Stationary\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nRemoving trends from your time series\n\n\n\n\n\nMar 21, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nYour Genes Are Making You Sick\n\n\n\n\n\n\nlife\n\n\nhealth\n\n\n\nUnderstanding genetic methylation can change your life\n\n\n\n\n\nMar 15, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary Learnings\n\n\n\n\n\n\nlife\n\n\nlearning\n\n\n\nRandom stuff I learned in February 2025\n\n\n\n\n\nMar 1, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nLife Principles\n\n\n\n\n\n\nlife\n\n\n\nHow I try to live my best life\n\n\n\n\n\nFeb 24, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nData Cleaning: Box-Cox Transformations\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nCreating better trendlines in your time series\n\n\n\n\n\nFeb 5, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nData Cleaning For Time Series\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nClean data makes for accurate forecasts\n\n\n\n\n\nFeb 3, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary Learnings\n\n\n\n\n\n\nlife\n\n\nlearning\n\n\n\nRandom stuff I learned in January 2025\n\n\n\n\n\nFeb 1, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Item Would You Bring to a Deserted Island?\n\n\n\n\n\n\nAI\n\n\nlearning\n\n\nschool\n\n\n\nWhy LLMs Are the Most Important Technology Ever Created\n\n\n\n\n\nJan 31, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis: External Regressors\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nWhat outside forces impact our business?\n\n\n\n\n\nJan 24, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\n80/20 SaaS: Why AI-Driven Customization Is the Future\n\n\n\n\n\n\nAI\n\n\n\nMaking software personalized to every user\n\n\n\n\n\nJan 2, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nDecember Learnings\n\n\n\n\n\n\nlife\n\n\nlearning\n\n\n\nRandom stuff I learned in December 2024\n\n\n\n\n\nJan 1, 2025\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWho Ya Gonna Call? ChatGPT!\n\n\n\n\n\n\nAI\n\n\n\nIf my grandpa can now use it, AI is officially mainstream\n\n\n\n\n\nDec 19, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT Pro is Cheap\n\n\n\n\n\n\nAI\n\n\nwork\n\n\n\nIf you think it’s expensive, you’re using AI wrong\n\n\n\n\n\nDec 6, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nNovember Learnings\n\n\n\n\n\n\nlife\n\n\nlearning\n\n\n\nRandom stuff I learned in November 2024\n\n\n\n\n\nDec 1, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nDIY Ozempic For 90% Less Cost\n\n\n\n\n\n\nlife\n\n\nhealth\n\n\n\nA supplement stack that creates GLP-1 Naturally\n\n\n\n\n\nNov 27, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis: Missing Data and Outliers\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nFixing issues in your time series data\n\n\n\n\n\nNov 26, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nHacking My Blood Sugar With Fiber\n\n\n\n\n\n\nlife\n\n\nhealth\n\n\n\nEating donuts while having my body think it’s a salad\n\n\n\n\n\nNov 15, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis: Autocorrelation\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nUnderstanding memory in time series\n\n\n\n\n\nNov 12, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I Cancelled All My Streaming Apps\n\n\n\n\n\n\nlife\n\n\n\nEngineering more boredom into my life\n\n\n\n\n\nNov 8, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis: Time Series Decomposition\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nUnderstanding trend and seasonality patterns\n\n\n\n\n\nNov 6, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nNewsletters Stink\n\n\n\n\n\n\nmachine-learning\n\n\nlearning\n\n\n\nWe need a YouTube for blogs\n\n\n\n\n\nOct 30, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis: Shape of the Data\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nStart to understand how your time series data looks\n\n\n\n\n\nOct 15, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis For Time Series\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nUnderstanding your data creates better forecasts\n\n\n\n\n\nOct 3, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s A Time Series?\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nOverview of AI and How Time Series Fits In\n\n\n\n\n\nOct 2, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts On Time Series Forecasting Fundamentals\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\n\nUnderstanding Core ML Concepts in Forecasting\n\n\n\n\n\nSep 25, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nMy Path from Finance Intern to Senior Software Engineer at Microsoft\n\n\n\n\n\n\nlife\n\n\nfinance\n\n\ncareer\n\n\n\nHow I transitioned into a technical engineering role with zero coding knowledge\n\n\n\n\n\nSep 4, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (8/30/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nAug 30, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (8/23/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nAug 23, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nFAQ on Machine Learning Forecasting\n\n\n\n\n\n\nfinance\n\n\nmachine-learning\n\n\nforecasting\n\n\n\nStraightforward answers to all of your ML forecast questions\n\n\n\n\n\nAug 12, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft Finance ML Forecasting Journey: Part Three\n\n\n\n\n\n\nfinance\n\n\nmachine-learning\n\n\nforecasting\n\n\n\nDemocratizing machine learning to everyone in finance\n\n\n\n\n\nJul 15, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nInner Circles of Life\n\n\n\n\n\n\nlife\n\n\n\nHow priorities change as we get older\n\n\n\n\n\nJun 28, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft Finance ML Forecasting Journey: Part Two\n\n\n\n\n\n\nfinance\n\n\nmachine-learning\n\n\nforecasting\n\n\n\nCentralizing our largest forecast process with machine learning\n\n\n\n\n\nJun 26, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft Finance ML Forecasting Journey: Part One\n\n\n\n\n\n\nfinance\n\n\nmachine-learning\n\n\nforecasting\n\n\n\nGoing from 0 to 1 with machine learning\n\n\n\n\n\nJun 12, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nHow To Master Storytelling\n\n\n\n\n\n\npodcast\n\n\nlearning\n\n\n\nNo one remembers charts and numbers. They only remember stories. The best story always wins. Just don’t tell stories about your vacation. No one wants to hear that.\n\n\n\n\n\nJun 4, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Deep Learning Last\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nDeep learning isn’t as effective as more traditional ML models\n\n\n\n\n\nMay 31, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Model Combinations Are King\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nUsually a combination of multiple models is more accurate than just one model’s prediction\n\n\n\n\n\nMay 28, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nMultistep Horizon Forecasting With finnts\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\nfinnts\n\n\n\nNew feature that improves forecast accuracy\n\n\n\n\n\nMay 13, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Capture Uncertainty\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nShowing the back testing results and future uncertainty of a model’s forecast builds more trust\n\n\n\n\n\nMay 7, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Simple Models Are Better Models\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nLike occam’s razor, the best model is often the one with the least amount of inputs\n\n\n\n\n\nMay 3, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: The Magic Is In The Feature Engineering\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nHow you transform your data before model training can transform a mediocre forecast into a world class forecast\n\n\n\n\n\nMay 1, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Order Is Important\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nWhen time is involved, how your data is ordered makes all the difference\n\n\n\n\n\nApr 23, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (4/19/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nApr 19, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Higher Grain Higher Accuracy\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nForecasting by country is often more accurate than forecasting by city. Forecasting by month is often more accurate than forecasting by day\n\n\n\n\n\nApr 18, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (4/12/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nApr 12, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: The Future Is Similar To The Past\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nIf you expect the future to be drastically different than past data, you will have a hard time training accurate models\n\n\n\n\n\nApr 11, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Garbage In, Garbage Out\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nTraining a model on bad data leads to bad forecasts\n\n\n\n\n\nApr 8, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (4/5/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nApr 5, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series First Principles: Domain Expertise\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nKnowing what factors actually influence what you are trying to forecast is more important than which ML model to train\n\n\n\n\n\nApr 2, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on First Principles in Time Series Forecasting\n\n\n\n\n\n\ntime-series\n\n\nmachine-learning\n\n\nfinance\n\n\n\nBuilding blocks of creating a great forecast\n\n\n\n\n\nMar 26, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (3/22/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nMar 22, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (3/16/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nMar 16, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on Power\n\n\n\n\n\n\nlife\n\n\n\nMost desires are attempts to gain power, make sure you choose the right kind\n\n\n\n\n\nMar 13, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (2/23/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nFeb 23, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on My Journey in Microsoft’s Finance Rotation Program\n\n\n\n\n\n\ncareer\n\n\nfinance\n\n\n\nMy path through the FRP and advice to aspiring and current analysts\n\n\n\n\n\nFeb 19, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (2/16/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nFeb 16, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (1/26/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nJan 26, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (1/12/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nJan 12, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (1/5/24)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nJan 5, 2024\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on The Almanack of Naval Ravikant\n\n\n\n\n\n\nbooks\n\n\n\nKey takeaways from the book\n\n\n\n\n\nDec 30, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (12/22/23)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week\n\n\n\n\n\nDec 22, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nIs AI the End of Specialization?\n\n\n\n\n\n\nAI\n\n\nLLM\n\n\n\nHow AI will change high-skill professions like medicine and law\n\n\n\n\n\nDec 11, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Reads (12/9/23)\n\n\n\n\n\n\nweekend-reads\n\n\n\nInteresting things I found on the internet this week.\n\n\n\n\n\nDec 9, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Language Model Use in Finance\n\n\n\n\n\n\nllm\n\n\nfinance\n\n\n\nHow we can start using Generative AI technology within corporate finance\n\n\n\n\n\nNov 24, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTotal Addressable Market for Advanced Analytics in Finance\n\n\n\n\n\n\nmachine-learning\n\n\nfinance\n\n\n\nUsing new technology to reinvent how we look forward, look backward, and make the next best decision in finance\n\n\n\n\n\nOct 19, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThe ML Language Wars Are Over, Large Language Models Won\n\n\n\n\n\n\nmachine-learning\n\n\nopen-source\n\n\n\nPicking the right tool for the job has never been easier with things like Chat-GPT\n\n\n\n\n\nFeb 13, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nThree Levels of Machine Learning Adoption in Finance\n\n\n\n\n\n\nmachine-learning\n\n\ntime-series\n\n\nfinance\n\n\n\nBeating the three levels on your way to machine learning nirvana\n\n\n\n\n\nFeb 11, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nNew Year Resolutions Are Dumb\n\n\n\n\n\n\ngeneral\n\n\n\nDon’t do new year resolutions, be consistent instead\n\n\n\n\n\nJan 1, 2023\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nSwitching to Quarto\n\n\n\n\n\n\ngeneral\n\n\n\nTransitioning my site to Quarto\n\n\n\n\n\nSep 25, 2022\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nFree Book on Advanced Analytics in Corporate Finance\n\n\n\n\n\n\nmachine-learning\n\n\ncode\n\n\nlearning\n\n\nopen-source\n\n\n\nFree resources on building advanced analytics talent that thrives\n\n\n\n\n\nDec 3, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting Simplified with finnts\n\n\n\n\n\n\ncode\n\n\ntime-series\n\n\nml\n\n\n\nNew R package for time series\n\n\n\n\n\nSep 13, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal User Manual\n\n\n\n\n\n\ngeneral\n\n\nwork\n\n\n\nQuick facts about my working style\n\n\n\n\n\nAug 26, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nPro Bono Work for Stretch Projects\n\n\n\n\n\n\ncode\n\n\nlearning\n\n\n\nGetting experience before you’re qualified to do the work\n\n\n\n\n\nAug 24, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Online Courses Stink\n\n\n\n\n\n\ncode\n\n\nlearning\n\n\n\nPitfalls of technical courses\n\n\n\n\n\nAug 19, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Screenplays in VS Code\n\n\n\n\n\n\ncode\n\n\nmovies-tv\n\n\nopen-source\n\n\n\nOpen sourcing screenwriting\n\n\n\n\n\nAug 17, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nfreeCodeCamp Appreciation Post\n\n\n\n\n\n\ncode\n\n\n\nThis site rocks\n\n\n\n\n\nAug 12, 2021\n\n\nMike Tokic\n\n\n\n\n\n\n\n\n\n\n\n\nBridging the ML Talent Gap in Corporate Finance\n\n\n\n\n\n\nml\n\n\nfinance\n\n\n\nFinding the diamonds in the rough\n\n\n\n\n\nAug 10, 2021\n\n\nMike Tokic\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/2024-11-12-ts-fundamentals-autocorrelation.html",
    "href": "notebooks/2024-11-12-ts-fundamentals-autocorrelation.html",
    "title": "Python Code",
    "section": "",
    "text": "# import libraries\nimport pandas as pd\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\n\n\n# read data\ndata_raw = pd.read_csv(\"../posts/2024-10-02-ts-fundamentals-whats-a-time-series/example_ts_data.csv\")\n\ndata_raw = (\n    # select columns\n    data_raw[[\"Country\", \"Product\", \"Date\", \"Revenue\"]]\n    # change data types\n    .assign(\n        Date = pd.to_datetime(data_raw[\"Date\"]), \n        Revenue = pd.to_numeric(data_raw[\"Revenue\"])\n    )\n)\n\n# print the first few rows\nprint(data_raw.head())\n\n\n# filter on specific series\nus_ic_raw = data_raw[(data_raw[\"Country\"] == \"United States\") & (data_raw[\"Product\"] == \"Ice Cream\")]\n\nus_ic_raw.set_index(\"Date\", inplace=True)\n\nprint(us_ic_raw.head())\n\n\n# plot the series\nplt.figure(figsize=(10, 6))\nplt.plot(us_ic_raw.index, us_ic_raw[\"Revenue\"], label = \"Ice Cream Revenue\", color = \"blue\")\nplt.title(\"US Ice Cream Revenue\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.grid(True)\n\n# save the plot\n# plt.savefig(\"chart1\", dpi = 300, bbox_inches = \"tight\")\n\n\n# plot the autocorrelation\nplt.figure(figsize=(10, 6))\nplot_acf(us_ic_raw[\"Revenue\"], lags=24, alpha=0.05)\nplt.title(\"Autocorrelation of US Ice Cream Revenue\")\n\n# save the plot\n# plt.savefig(\"chart2\", dpi = 300, bbox_inches = \"tight\")\n\n\n# plot the partial autocorrelation\nplt.figure(figsize=(10, 6))\nplot_pacf(us_ic_raw[\"Revenue\"], lags=24, alpha=0.05)\nplt.title(\"Partial Autocorrelation of US Ice Cream Revenue\")\n\n# save the plot\n# plt.savefig(\"chart3\", dpi = 300, bbox_inches = \"tight\")"
  },
  {
    "objectID": "notebooks/2025-01-24-ts-fundamentals-eda-xregs.html",
    "href": "notebooks/2025-01-24-ts-fundamentals-eda-xregs.html",
    "title": "Python Code",
    "section": "",
    "text": "# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.feature_selection import mutual_info_regression\n\n\n# read data\ndata_raw = pd.read_csv(\"../posts/2024-10-02-ts-fundamentals-whats-a-time-series/example_ts_data.csv\")\n\ndata_raw = (\n    # select columns\n    data_raw[[\"Country\", \"Product\", \"Date\", \"Revenue\"]]\n    # change data types\n    .assign(\n        Date = pd.to_datetime(data_raw[\"Date\"]), \n        Revenue = pd.to_numeric(data_raw[\"Revenue\"])\n    )\n)\n\n# print the first few rows\nprint(data_raw.head())\n\n\n# filter on specific series\nus_ck_raw = data_raw[(data_raw[\"Country\"] == \"United States\") & (data_raw[\"Product\"] == \"Cookies\")]\n\nus_ck_raw.set_index(\"Date\", inplace=True)\n\nprint(us_ck_raw.head())\n\n# plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(us_ck_raw.index, us_ck_raw[\"Revenue\"], label=\"Cookies Revenue\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.title(\"Cookies Revenue in the United States\")\nplt.legend()\n\n\n# Set a random seed for reproducibility\nrandom.seed(15)\n\nus_ck_corr = us_ck_raw.copy()\n\n# Generate a new variable with strong correlation\ncorrelation_target = 0.9\nn = us_ck_corr.shape[0]\nnoise = np.random.randn(n)\n\n# Orthogonalize noise to the original variable to ensure independence\nnoise = noise - np.dot(noise, us_ck_corr[\"Revenue\"]) / np.dot(us_ck_corr[\"Revenue\"], us_ck_corr[\"Revenue\"]) * us_ck_corr[\"Revenue\"]\n\n# Scale the orthogonalized noise to match the desired correlation\nnoise = noise / np.linalg.norm(noise) * np.sqrt(1 - correlation_target**2) * np.linalg.norm(us_ck_corr[\"Revenue\"])\n\n# Create the new variable\nus_ck_corr[\"xreg1\"] = correlation_target * us_ck_corr[\"Revenue\"] + noise\n\n# Verify the correlation\ncorrelation = us_ck_corr[\"Revenue\"].corr(us_ck_corr[\"xreg1\"])\nprint(f\"Correlation between Original and New_Var: {correlation:.4f}\")\n\n\n# create a variable that has a weak correlation to the revenue column, create random values between 1 and 100\nus_ck_corr[\"xreg2\"] = random.sample(range(1, 100), us_ck_corr.shape[0])\n\n# print the first few rows, formatting numbers to 2 decimal places\nprint(us_ck_corr.head(10).round(2))\n\n# plot the data with the new variables as dotted lines\nplt.figure(figsize=(10, 6))\nplt.plot(us_ck_corr.index, us_ck_corr[\"Revenue\"], label=\"Cookies Revenue\")\nplt.plot(us_ck_corr.index, us_ck_corr[\"xreg1\"], label=\"xreg1\", linestyle = \"dotted\")\nplt.plot(us_ck_corr.index, us_ck_corr[\"xreg2\"], label=\"xreg2\", linestyle = \"dotted\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.title(\"Cookies Revenue in the United States\")\nplt.legend()\n\n# save the plot\n# plt.savefig(\"chart1\", dpi = 300, bbox_inches = \"tight\")\n\n\n# calculate the correlation between the target variable and the new variables, dropping the date, country, and product columns\ncorrelation = us_ck_corr.drop(columns=[\"Country\", \"Product\"]).corr()\n\n# create a simple table to display the correlation values\ncorrelation_table = correlation.stack().reset_index()\ncorrelation_table.columns = [\"Variable 1\", \"Variable 2\", \"Correlation\"]\ncorrelation_table = correlation_table[correlation_table[\"Variable 1\"] == \"Revenue\"]\ncorrelation_table = correlation_table[correlation_table[\"Variable 2\"] != \"Revenue\"]\n\n# print the correlation table, rounding the values to 2 decimal places\nprint(correlation_table.round(2))\n\n\n# calculate lags of the xreg1 and xreg2 columns. Create 1, 2, 3, 6, 9, 12 lags\nus_ck_corr[\"xreg1_lag_1\"] = us_ck_corr[\"xreg1\"].shift(1)\nus_ck_corr[\"xreg1_lag_2\"] = us_ck_corr[\"xreg1\"].shift(2)\nus_ck_corr[\"xreg1_lag_3\"] = us_ck_corr[\"xreg1\"].shift(3)\nus_ck_corr[\"xreg1_lag_6\"] = us_ck_corr[\"xreg1\"].shift(6)\nus_ck_corr[\"xreg1_lag_9\"] = us_ck_corr[\"xreg1\"].shift(9)\nus_ck_corr[\"xreg1_lag_12\"] = us_ck_corr[\"xreg1\"].shift(12)\n\nus_ck_corr[\"xreg2_lag_1\"] = us_ck_corr[\"xreg2\"].shift(1)\nus_ck_corr[\"xreg2_lag_2\"] = us_ck_corr[\"xreg2\"].shift(2)\nus_ck_corr[\"xreg2_lag_3\"] = us_ck_corr[\"xreg2\"].shift(3)\nus_ck_corr[\"xreg2_lag_6\"] = us_ck_corr[\"xreg2\"].shift(6)\nus_ck_corr[\"xreg2_lag_9\"] = us_ck_corr[\"xreg2\"].shift(9)\nus_ck_corr[\"xreg2_lag_12\"] = us_ck_corr[\"xreg2\"].shift(12)\n\n# calculate the correlation between the target variable and the new variables, dropping the date, country, and product columns\nrelation = us_ck_corr.drop(columns=[\"Country\", \"Product\"]).corr()\n\n# create a simple table to display the correlation values\nlag_table = relation.stack().reset_index()\nlag_table.columns = [\"Variable 1\", \"Variable 2\", \"Correlation\"]\nlag_table = lag_table[lag_table[\"Variable 1\"] == \"Revenue\"]\nlag_table = lag_table[lag_table[\"Variable 2\"] != \"Revenue\"]\n\n# print the correlation table, rounding the values to 2 decimal places\nprint(lag_table.round(2))\n\n\n# create mutual information function\ndef calculate_mutual_information(x, y, n_neighbors=3, discrete_features=False, random_state=123):\n    \"\"\"\n    Calculates the Mutual Information (MI) score between two variables using mutual_info_regression.\n\n    Parameters:\n    x (array-like): First variable (time series or feature).\n    y (array-like): Second variable (time series or feature).\n    n_neighbors (int): Number of neighbors to use for density estimation (default is 3).\n    discrete_features (bool): Whether the features are discrete (default is False).\n\n    Returns:\n    float: Mutual Information (MI) score.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    x = np.array(x).reshape(-1, 1)  # Reshape x to 2D as required by mutual_info_regression\n    y = np.array(y)\n\n    # Calculate Mutual Information\n    mi_score = mutual_info_regression(x, y, n_neighbors=n_neighbors, discrete_features=discrete_features, random_state=random_state)\n\n    return mi_score[0]  # Return the MI score (a single value since x has one feature)\n\n\n# copy the data\nus_ck_mi = us_ck_corr.copy()\n\n# drop NaN values\nus_ck_mi.dropna(inplace=True)\n\n# Define variables and their lags\nvariables = [\"xreg1\", \"xreg2\"]\nlags = [0, 1, 2, 3, 6, 9, 12]\n\n# Initialize results\nmi_results = []\n\n# Calculate MI dynamically for variables and their lags\nfor var in variables:\n    for lag in lags:\n        col_name = f\"{var}_lag_{lag}\" if lag != 0 else var\n        if col_name in us_ck_mi.columns:\n            mi_score = calculate_mutual_information(us_ck_mi[col_name], us_ck_mi[\"Revenue\"])\n            mi_results.append({\"Variable 1\": \"Revenue\", \"Variable 2\": col_name, \"Mutual Information\": mi_score})\n\n# Create a DataFrame\nmi_table = pd.DataFrame(mi_results)\n\n# Print the table rounded to 2 decimal places\nprint(mi_table.round(2))\n\n\n# create a binary variable that is 1 in the year 2020 and 0 otherwise\nus_ck_binary = us_ck_corr.copy()\nus_ck_binary[\"COVID_Flag\"] = us_ck_binary.index.year == 2020\nus_ck_binary[\"COVID_Flag\"] = us_ck_binary[\"COVID_Flag\"].astype(int)\n\nprint(us_ck_binary[[\"Country\", \"Product\", \"Revenue\", \"COVID_Flag\"]].head())\n\n# calculate the mutual information between the binary variable and the revenue column\nmi_score = calculate_mutual_information(us_ck_binary[\"COVID_Flag\"], us_ck_binary[\"Revenue\"])\n\nprint(f\"Mutual Information between COVID Flag and Revenue: {mi_score:.4f}\")\n\n# create a randrom variable that is 1 with a probability of 0.5 and 0 otherwise\nus_ck_random = us_ck_corr.copy()\nus_ck_random[\"Random_Flag\"] = np.random.choice([0, 1], us_ck_random.shape[0], p=[0.5, 0.5])\n\nprint(us_ck_random[[\"Country\", \"Product\", \"Revenue\", \"Random_Flag\"]].head())\n\n# calculate the mutual information between the random variable and the revenue column\nmi_score = calculate_mutual_information(us_ck_random[\"Random_Flag\"], us_ck_random[\"Revenue\"])\nprint(f\"Mutual Information between Random Flag and Revenue: {mi_score:.4f}\")"
  },
  {
    "objectID": "notebooks/2025-03-21-ts-fundamentals-data-stationary.html",
    "href": "notebooks/2025-03-21-ts-fundamentals-data-stationary.html",
    "title": "Python Code",
    "section": "",
    "text": "# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# read data\ndata_raw = pd.read_csv(\"../posts/2024-10-02-ts-fundamentals-whats-a-time-series/example_ts_data.csv\")\n\ndata_raw = (\n    # select columns\n    data_raw[[\"Country\", \"Product\", \"Date\", \"Revenue\"]]\n    # change data types\n    .assign(\n        Date = pd.to_datetime(data_raw[\"Date\"]), \n        Revenue = pd.to_numeric(data_raw[\"Revenue\"])\n    )\n)\n\n# print the first few rows\nprint(data_raw.head())\n\n\n# filter on specific series\nus_ic_raw = data_raw[(data_raw[\"Country\"] == \"United States\") & (data_raw[\"Product\"] == \"Ice Cream\")]\n\nus_ic_raw.set_index(\"Date\", inplace=True)\n\nprint(us_ic_raw.head())\n\n# plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(us_ic_raw.index, us_ic_raw[\"Revenue\"], label=\"Ice Cream Revenue\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue\")\nplt.title(\"Ice Cream Revenue in United States\")\nplt.legend()\n\n# save the plot\n# plt.savefig(\"chart1\", dpi = 300, bbox_inches = \"tight\")\n\n\n# copy the data \nus_ic_diff = us_ic_raw.copy()\n\n# difference the revenue data \nus_ic_diff[\"Revenue\"] = us_ic_diff[\"Revenue\"].diff()\n\n# plot the differenced data\nplt.figure(figsize=(10, 6))\nplt.plot(us_ic_diff.index, us_ic_diff[\"Revenue\"], label=\"First Order Difference\", color='orange')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Differenced Revenue\")\nplt.title(\"Differenced Ice Cream Revenue in United States\")\nplt.legend()\n\n# save the plot\n# plt.savefig(\"chart2\", dpi = 300, bbox_inches = \"tight\")\n\n\n# copy the data again for second order differencing\nus_ic_diff2 = us_ic_raw.copy()\n\n# take a second order difference \nus_ic_diff2 = us_ic_diff2[\"Revenue\"].diff().diff()\n\n# plot the differenced data\nplt.figure(figsize=(10, 6))\nplt.plot(us_ic_diff2.index, us_ic_diff2, label=\"Second Order Difference\", color='orange')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Second Order Difference\")\nplt.title(\"Second Order Differenced Ice Cream Revenue in United States\")\nplt.legend()\n\n# save the plot\n# plt.savefig(\"chart3\", dpi = 300, bbox_inches = \"tight\")\n\n\n# copy the data again for seasonal differencing\nus_ic_seasonal_diff = us_ic_raw.copy()\n\n# take a seasonal difference (12 months for monthly data)\nus_ic_seasonal_diff[\"Revenue\"] = us_ic_seasonal_diff[\"Revenue\"].diff(12)\n\n# plot the seasonal differenced data\nplt.figure(figsize=(10, 6))\nplt.plot(us_ic_seasonal_diff.index, us_ic_seasonal_diff[\"Revenue\"], label=\"Seasonal Difference\", color='orange')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Seasonal Differenced Revenue\")\nplt.title(\"Seasonally Differenced Ice Cream Revenue in United States\")\nplt.legend()\n\n# save the plot\n# plt.savefig(\"chart4\", dpi = 300, bbox_inches = \"tight\")\n\n\n# apply a seasonal difference of 12 months to the original data and then a first order difference\nus_ic_seasonal_diff_first = us_ic_raw.copy()\nus_ic_seasonal_diff_first[\"Revenue\"] = us_ic_seasonal_diff_first[\"Revenue\"].diff(12).diff()\n\n# plot the seasonal differenced and first order differenced data\nplt.figure(figsize=(10, 6))\nplt.plot(us_ic_seasonal_diff_first.index, us_ic_seasonal_diff_first[\"Revenue\"], label=\"Seasonal + First Order Difference\", color='orange')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Seasonal + First Order Differenced Revenue\")\nplt.title(\"Seasonally and First Order Differenced Ice Cream Revenue in United States\")\nplt.legend()\n\n# save the plot\n# plt.savefig(\"chart5\", dpi = 300, bbox_inches = \"tight\")\n\n\n# test if original time series is stationary using unit root test\nfrom statsmodels.tsa.stattools import kpss\n\ndef kpss_test(timeseries, regression='c'):\n    statistic, p_value, lags, critical_values = kpss(timeseries, regression=regression)\n    \n    print(f'KPSS Statistic: {statistic}')\n    print(f'p-value: {p_value}')\n    print(f'Num Lags Used: {lags}')\n    print('Critical Values:')\n    for key, value in critical_values.items():\n        print(f'   {key}: {value}')\n    \n    if p_value &lt; 0.05:\n        print(\"\\nResult: Series is non-stationary (reject the null hypothesis)\")\n    else:\n        print(\"\\nResult: Series is stationary (fail to reject the null hypothesis)\")\n\n# apply the KPSS test to the original time series\nkpss_test(us_ic_raw[\"Revenue\"], regression='c')\n\n\n# apply the KPSS test to the first order differenced time series, dropping the first row (NaN value)\nus_ic_diff = us_ic_diff.dropna()\nkpss_test(us_ic_diff[\"Revenue\"], regression='c')\n\n\n# apply the KPSS test to the second order differenced time series, dropping the first two rows (NaN values)\nus_ic_diff2 = us_ic_diff2.dropna()\nkpss_test(us_ic_diff2, regression='c')\n\n\n# apply the KPSS test to the seasonal differenced time series, dropping the first 12 rows (NaN values)\nus_ic_seasonal_diff = us_ic_seasonal_diff.dropna()\nkpss_test(us_ic_seasonal_diff[\"Revenue\"], regression='c')\n\n\n# apply the KPSS test to the seasonal and first order differenced time series, dropping the first 13 rows (12 for seasonal and 1 for first order)\nus_ic_seasonal_diff_first = us_ic_seasonal_diff_first.dropna()\nkpss_test(us_ic_seasonal_diff_first[\"Revenue\"], regression='c')"
  },
  {
    "objectID": "posts/2021-08-12-freecodecamp-appreciation/index.html",
    "href": "posts/2021-08-12-freecodecamp-appreciation/index.html",
    "title": "freeCodeCamp Appreciation Post",
    "section": "",
    "text": "Nowadays there are a ton of places to learn programming skills online. From books, to videos, all the way to bootcamps. A lot of these methods work great, but they usually cost money. The free content just teaches the basics and is a hook for you to continue into a paid course or site. freeCodeCamp to the rescue!\nFCC is a free site to learn many aspects of coding. From web development to cybersecurity. They have courses that teach you in depth about programming, with in the browser coding exercises to help hone your skills. To get a certification for a course or topic, all you need to do is complete the project list on the site. This makes it great for complete newbies, as well as people with some experience who would benefit from the project work and certification.\nMy favorite part of FCC is not the site itself, but instead their youtube page. This in a gold mine of useful tutorials and classes taught by experts in their field. The youtube has so much more content than their site. Safe to say it would take years to go through and watch everything 😍.\nOne last final thing I wanted to call out is the newsletter that Quincy Larson, founder of freeCodeCamp, sends out each week. It’s a fantastic list of learning resources curated from the FCC blog and youtube site. Definitely worth subscribing to."
  },
  {
    "objectID": "posts/2021-08-19-why-courses-stink/index.html",
    "href": "posts/2021-08-19-why-courses-stink/index.html",
    "title": "Why Online Courses Stink",
    "section": "",
    "text": "Nowadays there are thousands of courses on the internet, teaching everything from cooking to machine learning with R. Being a self-taught coder myself I started off taking these online courses, but overtime have realized they may not be the best way of learning.\n\nCertifications Over Knowledge\nFor me, traditional schooling techniques do not ensure I master the material or learn anything that will live in my long term memory. What it does ensure is that I will do anything to “check the box” of a certain class and try to get a good grade. Getting a good grade and actually learning the material are two separate things in these environments.\nGetting a good grade entails learning only what is required to pass some test to prove you learned. This is a dangerous game because there are various ways for students to learn as little as possible and still ace a test or exam. What do these tests prove? Mostly that you can regurgitate facts and equations on command. I think we have all been in a situation where we studied just enough to cover what’s going to be on the exam, then a week after the test all of that knowledge seeps out through our ears. Forever lost.\nThe same can be said about online courses that offer “certifications”. This creates the same attitude as regular classes you may have already taken in high school and college. Knowing just enough to get by and get the class credit, or in this case, a certification. The crazy thing about certifications though is that they don’t show how well you did in the course, only that you passed. That’s like seeing a college transcript with only pass/fail grades instead of a GPA.\n\n\nPrioritizing Just in Time Learning\nGetting out of the “certification learning mode” has been critical to my development in programming and data science. What I have realized is the importance of applying whatever I’m learning to real world projects as soon as possible. Here is the learning loop I follow.\n\nLearn enough foundational concepts to be dangerous\nImmediately apply the knowledge to a real world project\nRefer back to learning materials as needed to reinforce concepts\nGo further down the learning rabbit hole with advanced concepts to improve project\nRinse and repeat above steps to constantly reinforce topics and improve project\n\nThis kind of just in time learning has been amazing for me. It’s allowed me to quickly ramp up on the basics and reinforce them in my brain by working on things in my job that actually matter. Not just some fake project within a course, but something that actually effects my job where the stakes are higher. Then after I get the hang of the basics I go back to the learning well to learn more advanced concepts to then come back and apply within the same real world project.\n\n\nReversal\nThe downside to approaching skill acquisition like this is that there is no fancy diploma or certification you can hang on your wall or post on LinkedIn. What you do have is real work on projects you can hang your hat on. These projects may be specific to your job, meaning you may not be able to share them publicly as proof that you know something. This is where the challenge arises. Sites like LinkedIn are working towards creating ways to show that you have a particular skill via online skill assessments, but this still falls back into the learning enough to check the box approach. Building a solution portfolio that lives publicly on places like GitHub is a great way to show employers you have certain skills, but takes extra time since the solutions you build would have to be outside of your job work.\nOne potential solution that’s the best of both worlds is to write blogs about what you’re working on, without spilling any company secrets, and publishing open source software that allows your work to scale to others. That way others can see your growth and commitment to learning, while also reaping the benefits of your work. Truly a win-win."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html",
    "href": "posts/2021-08-26-personal-user-manual/index.html",
    "title": "Personal User Manual",
    "section": "",
    "text": "Below are some basic operating instructions for myself. It details how I work best personally and with others. I got the idea from a great book, called How Google Works. In the book they mention some higher up at the company writing a “how to fix me if broken” post that I found very informative. A lot of times we have to learn how best to work with others the hard way, through trial and error to see what works. Sometimes your coworker might tell you how they prefer to work together, but I cringe thinking about a lot of employees who work through gritted teeth in ways that go against how they work best. So in the below sections I detail how I personally work best throughout the week with myself and others."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#work-hours",
    "href": "posts/2021-08-26-personal-user-manual/index.html#work-hours",
    "title": "Personal User Manual",
    "section": "Work Hours",
    "text": "Work Hours\nIdeally I like to work from 7:30am - 4pm PST. I’m based out on the West Coast (Seattle) and like to work earlier in the day. This allows me to have a lot of focused deep work time in the first few hours of the work day, before any meetings are scheduled.\nMy mornings are sacred. That’s when I get my best thinking done, and the code flies out of my little meat fingers. I try to have multiple 90 to 120 min blocks of uninterrupted time for deep work. This is crucial to me getting any substantial work done in a day. I guard this time with my life."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#how-to-contact-me",
    "href": "posts/2021-08-26-personal-user-manual/index.html#how-to-contact-me",
    "title": "Personal User Manual",
    "section": "How To Contact Me",
    "text": "How To Contact Me\nBecause of my love for morning deep work, I often have my email and teams app closed on my computer, and my phone is set on do not disturb. This means any urgent email or teams message may not get seen until the early afternoon. If there truly is an emergency and you need to get a hold of me, please call me through teams directly. That way I have to answer and break my deep work flow. Just make sure it’s a true emergency that cannot wait.\nUsually email and IM are the best ways to contact me. For email I try to respond within 1-3 days, and for IM within 24 hours (usually same day). Sending me an IM with just “hey” is usually not my favorite message to receive. If you have a question or request, then say it right out of the gate when sending a message. This will help me prioritize what you’re asking over other things I’m working on that day.\nSomething I’m trying to find good solutions for is the dreaded “do you have a min for a quick call”? I know sometimes just asking a question out loud or sharing your screen is the quickest way to figure something out, but it can rob any flow or attention from the person being asked. So I only try to do this sparingly with others on my team and hope others do the same with me."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#dev-vs-meeting-days",
    "href": "posts/2021-08-26-personal-user-manual/index.html#dev-vs-meeting-days",
    "title": "Personal User Manual",
    "section": "Dev vs Meeting Days",
    "text": "Dev vs Meeting Days\nI love the idea from Paul Graham about a maker vs manager schedule. Basically it states that even a single meeting can throw off any momentum or flow one might have when building things. It could take more than an hour to get ramped up into a specific coding task, and one 30 minute meeting in the middle of the afternoon can totally derail and developer productivity after the meeting.\nThis has caused me to segment my days in two ways. First, on Monday/Wednesday/Friday I block out my calendar entirely. These are my dev days, where I only focus on writing code and deep thinking. Then my Tuesday/Thursdays are wide open for anyone to schedule meetings with me. I have found this to work very well. Usually when I meet a new person and hear the words “I’ll set up some time to chat”, I immediately tell them that Tuesday/Thursday are usually the best days for me. It’s been hard to keep this kind of schedule, especially for meetings that have a lot of people attached to them where finding open calendar spots is next to impossible. There are cases where I make an exception and meet with people on my dev days, but this is only after asking the organizer if it’s imperative that I come to the meeting. I have learned that a lot of the times I can skip and catch up with folks later.\nSaying no to meeting requests has been a long journey for me, and I’m now getting to the point where I don’t feel like a total jerk declining meetings where I would just be a fly on the wall. What a weight off my shoulders."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#my-meeting-checklist",
    "href": "posts/2021-08-26-personal-user-manual/index.html#my-meeting-checklist",
    "title": "Personal User Manual",
    "section": "My Meeting Checklist",
    "text": "My Meeting Checklist\nI try to work against the default setting of having a meeting for everything. So here is my checklist of reasons I will attend a meeting, if a meeting doesn’t meet any point below, I have to decline it. Granted there are exceptions to the rule but most of the time I will say no.\nReasons I Will Join A Meeting\n\nPre-defined agenda send ahead of time\nMain discussion point is around making a decision or coordinating next steps on a work item (either with prep work from #5)\nA business partner is having issues with something I built for them, and we cannot fix via IM/email\nShowcasing a new solution, that requires a QnA or broader discussion\nSome sort of prep work was done ahead of time, usually in the form of a meeting pre-read document or email\n\nReasons I Will Not Join A Meeting\n\nNo agenda\nMain discussion point is a status update (should have been an email), sharing information (should have been an email) or brainstorming without any previous work done\nNo prep work was done ahead of time\nIf there are more than 7 people invited (more people don’t make a meeting better)\n\nIf the meeting falls more into the second list instead of the first, I will simply decline the meeting and ask for it to be transcribed via teams so I can catch up via Teams Copilot. If the meeting is about sharing information, I will promise to send a written write up of my update."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#asynchronous-by-default",
    "href": "posts/2021-08-26-personal-user-manual/index.html#asynchronous-by-default",
    "title": "Personal User Manual",
    "section": "Asynchronous by Default",
    "text": "Asynchronous by Default\nI love the idea of asynchronous communication. Less meetings, more documentation and long form writing. I think a lot of times people schedule meetings to think through something rather than reflecting on their own and coming up with a solution before sharing it with others. Scheduling a meeting to “think” about a problem or project is a huge waste of everyone who attended the meeting. It’s also selfish.\nHaving everyone come together to discuss something should not be the first option in my opinion. I think personal reflection and writing your thoughts down before sharing with others is much better. It helps clarify your thinking and prevents groupthink from seeping into decisions. Thinking about a problem, writing down possible solutions with thorough analysis, then sharing the results with team members asynchronously is a much better use of everyone’s time then just having a meeting where no preparation is done beforehand. You could still have a meeting to discuss the written document but usually I only like to do that if there are certain components you’d like to debate about what was written of if a final decision needs to be made.\nAs more teams realize the power of distributed teams, asynchronous communication and writing skills will become an even bigger superpower. Doing this well not only makes better use of everyone’s time, but allows for better decisions to be made. More people have the opportunity to voice their opinion, especially introverts. More people have the freedom to work when they want to work, preventing the dreaded 4pm Friday meeting 😪."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#best-times-to-meet",
    "href": "posts/2021-08-26-personal-user-manual/index.html#best-times-to-meet",
    "title": "Personal User Manual",
    "section": "Best Times to Meet",
    "text": "Best Times to Meet\nThe best times I like to meet are at the edges of existing components of my day. For example, I’d rather meet right after my lunch break around 1:00/1:30pm then having a meeting a 2:30pm. Having a meeting in the middle of the afternoon splits that part of the day in half, which can limit the amount of deep work I get get in a day since it takes a while to get in the zone or flow state. That’s ok because I have set up days (Tue/Thu) where I batch all my meetings together. So if I have a bunch of meetings, all with half hour gaps in between, I don’t worry because I know the next day I’ll have a lot of unstructured time for deep work.\nWith that being said, normally I like meetings to start after 10:30am, take a break over the noon lunch hour, then end by 4pm. Thankfully I work on a team and company that respects those hours, but on occasion I have a 4pm meeting I just can’t avoid. Meeting with a CVP for example. Not the end of the world."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#how-to-fix-me-if-broken",
    "href": "posts/2021-08-26-personal-user-manual/index.html#how-to-fix-me-if-broken",
    "title": "Personal User Manual",
    "section": "How to Fix Me if Broken",
    "text": "How to Fix Me if Broken\nI had to do some hard thinking about this one. In the end I settled on a simple answer, just leave me alone to think. If I even get flustered or seem super stressed, normally time to myself is the cure. Either just some quiet reflection at my desk or a quick walk around the block.\nI’m an introvert so sometimes too many meetings and deadlines can get the best of me and stress me out too much, solitude is the brain reboot I need to come back down to normal."
  },
  {
    "objectID": "posts/2021-08-26-personal-user-manual/index.html#final-thoughts",
    "href": "posts/2021-08-26-personal-user-manual/index.html#final-thoughts",
    "title": "Personal User Manual",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI think more people should write about how they work best. Obviously not everyone will have the same working style as other team members, but knowing how someone honestly likes to get their work done helps find common ground and make compromises to get the best collaboration.\nI could even see HR products like Microsoft Viva coming into the mix soon with some type of employee bio or baseball card with high level stats on who they are and how they do their work. Aggregating this kind of info across teams could lead to some interesting insights. If everyone on the team likes to email and IM vs having a meeting to discuss, then maybe the boss should stop scheduling weekly in person update meetings. The same goes for a team that is mostly extroverted and likes to meet in person to work things through. If a new introvert who enjoys asynchronous work joins the team, maybe the team members could create ways to accommodate their working style to find the best of both worlds. Like recording a meeting and allowing them to contribute on their own time instead of joining synchronously. By doing this more teams will become more inclusive, and hopefully more productive. Which is a buzzword every management team loves to hear."
  },
  {
    "objectID": "posts/2021-12-03-finance-aa-book/index.html",
    "href": "posts/2021-12-03-finance-aa-book/index.html",
    "title": "Free Book on Advanced Analytics in Corporate Finance",
    "section": "",
    "text": "I’m excited to announce the release of a new R package called finnts, aka Finn. The package helps simplify the process of creating time series forecasts with statistical and machine learning models. Finn automates the more tedious aspects of machine learning. Things like data cleaning, feature engineering, back testing, and model selection are all handled automatically by Finn while still being flexible to many forecasting scenarios.\nFinn is a perfect solution for people new to machine learning as well as seasoned pros looking for a scalable way to put many forecasts into production. Please take a look at the package site, try it out, and let me know what you think!"
  },
  {
    "objectID": "posts/2023-02-11-three-levels-of-ml-adoption/index.html",
    "href": "posts/2023-02-11-three-levels-of-ml-adoption/index.html",
    "title": "Three Levels of Machine Learning Adoption in Finance",
    "section": "",
    "text": "Growing machine learning adoption in your finance org is tough. There are many levels to clear, with final bosses to beat. This happens most in aspects of finance that are already being done manually by humans, like forecasting (time series). Below are the three questions or “levels” you need to clear before any company can truly leverage machine learning to its full potential in finance for forecasting.\n\nIs the ML forecast as or more accurate than the current process?\nCan you explain the number generated by the black box?\nIf ML forecasted $100, but actuals came in at $110, how do we explain the forecast variance?\n\nEach level needs to be fully cleared before you can tackle the next. Let’s dive in.\n\nFirst Level\nThe first level is pretty straight forward, you know what the accuracy bar is for the existing process, and in most cases you can create a machine learning forecast that can beat it. Even if you reach a similar level of accuracy with the ML process, ML can run in 95% less time than the manual process. Even then that can be a win for your finance team.\n\n\nSecond Level\nNow you’ve built a machine learning process with great results. This is often where you will face the most resistance from your finance team. They will say “oh great the forecast is super accurate, but how do I know how it came up with its number?”. This is tougher than just creating an accurate forecast, since there is often a trade off between building the most accurate model and building the most interpretable one. Thankfully this is a hot area of research right now, with lots of great open source tools being released. The big key to interpretable forecasts for finance tasks is to understand the seasonality and trend of your forecast, and also how outside drivers (macro, internal KPIs) effect your forecast. Your CFO might not care that your model is mostly driven by historical growth rates over the last 2-3 years but telling her that a key driver in your model is the rise of interest rates and lowering consumer sentiment is a great way to tell a story around the forecast and get more people bought in.\n\n\nThird Level\nThis is the final boss. This in uncharted territory because no data scientist or researcher is thinking about how their models impact the FP&A process at a company. This is a process specific to finance and one that doesn’t have a clear answer. If algorithms can be used to look into the future, then they could also be used to look into the past. Knowing how to automatically reconcile what was initially forecasted versus what actually happened would be a game changer. Allowing you to potentially automate much of the traditional close process in your finance team. Being able to explain the initial ML forecast in level two will allow you to eventually see the factors that contribute to the forecast variance during close.\n\n\nFinal Thoughts\nI think about each one of these questions every single day. Open source tools that my team are working on, like Finn, are being actively improved to answer each one of these questions. If you’re interested in using these tools at your company or helping to make them even better, please reach out to me on LinkedIn."
  },
  {
    "objectID": "posts/2023-10-19-aa-tam/index.html",
    "href": "posts/2023-10-19-aa-tam/index.html",
    "title": "Total Addressable Market for Advanced Analytics in Finance",
    "section": "",
    "text": "What would you say, you do here? I often have to explain the buzzword term “advanced analytics” and why exactly does a team like that exist in finance. Shouldn’t we just be called a business intelligence team or data team? The reason my advanced analytics team exists can be boiled down to one sentence. To reinvent how we use new technology to look forward, look backward, and make the next best decision in finance. This one phrase has a lot of surface area, so let’s peel back the onion and dive deeper.\nThe first two buckets cover most activities related to rhythm of business (RoB) work that is so crucial to finance, from budgeting and forecasting to the close process. This contains the bulk of the opportunity or addressable market. The final frontier is helping finance teams make the next best decision, which takes up every other activity outside of the traditional RoB activities.\n\nLooking Forward\nBy definition finance tries to make decisions about the future. What will out revenue look like? How is headcount going to change over the next 12 months? Are we going to actually hit our budget numbers for Q3? Most of looking forward is about some sort of time series forecasting. This is where I think the biggest bang for buck lies in the finance space. Most finance teams need some sort of prediction about the future, and most predictions are time related.\n\n\nLooking Backward\nAfter making predictions about the future, we have to measure what actually happened in the business. This is the “close process” you might hear finance pros discuss (sometimes with distaste). At the end of each month, each quarter, each fiscal year the financial numbers for that period need to lock. Journal entries need to complete, allocations have to be allocated, and every metaphorical “i” is dotted and “t” is crossed. This is probably the most manual process in finance today that hasn’t changed much since the inception of spreadsheets and powerpoint. Human beings have to look at reports, see how the month closed and if that number was different then what was initially forecasted, and then go tell their CFO why we were off $100 or $1,000,000,000 that month. As we use algorithms to look into the future, we can use algorithms to look into the past. If ML can be adopted in the forecast process, then there is a good chance we can use it during close. This is a unique problem to solve though. Currently we can get decent interpretability of the future forecast, but understanding the error between the forecast and what actually happened is not an exact science. I also think it’s a problem most data scientists may not think about today since it might only be important in small niches like finance where explaining that forecast error is crucial and prevents adoption of ML solutions. Large language models I think can unlock this problem, where we can build agents that can peel back the layers of forecasts and run the variance analysis to create the explanations we need. This takes a very manual process and could eventually speed up the story gathering part of close from a few days to a few minutes. Very exciting.\n\n\nMaking the Next Best Decision\nThis is the final frontier for advanced analytics in finance. Outside of the financial RoB processes like forecast and close, finance people do so much other kinds of work that mostly revolve around making data informed decisions about what to do in the business. How should we price our new product? Will the new product hurt sales of our older products? What’s the lifetime value of our customer? Should we buy back shares or double down on R&D? All of these questions cannot fit into a standard time series forecasting bucket, or any standard ML bucket. Each question has its own nuance and needs to be approached in a different way. One way to solve this with technology it to add armies of data & AI professionals who can answer these non-standard questions on demand, but this method doesn’t scale. Finance is notorious for running lean and being reluctant to add large technical teams that don’t do standard BI work like data pipelines and reporting. So again this is a space that is ripe for disruption through large language models. Again it’s agents to the rescue. Agents that can piece together various tools that can find data, pull data, and then analyze that data to help a human make a decision faster and with more precision. We are still so early in this space.\n\n\nFinal Thoughts\nAdvanced analytics in finance still has so much promise yet to be realized. Expectations are high and skeptics are always there. Brick by brick we get closer to making these ideas become a reality. If you are working on machine learning in your finance org, I salute you! It’s often nerve racking work that doesn’t always pay off immediately. Stay at it my friends, and the future will be yours."
  },
  {
    "objectID": "posts/2023-12-09-weekend-reads/index.html#articles",
    "href": "posts/2023-12-09-weekend-reads/index.html#articles",
    "title": "Weekend Reads (12/9/23)",
    "section": "Articles",
    "text": "Articles\n\nHow AI agents will impact the world: great post from Bill Gates exploring agents and their new place in our world.\nLangchain expands collaboration with Microsoft: so cool to see that open-source LLM tech like langchain can coexist with tech giants like Microsoft."
  },
  {
    "objectID": "posts/2023-12-09-weekend-reads/index.html#videos",
    "href": "posts/2023-12-09-weekend-reads/index.html#videos",
    "title": "Weekend Reads (12/9/23)",
    "section": "Videos",
    "text": "Videos\n\nHow large language models work: perfect intro to LLMs from an AI legend\nAI Engineering Summit: interesting videos about a new discipline\n\nOpen questions about AI engineering\nClimbing the ladder of abstraction\nThe intelligent interface"
  },
  {
    "objectID": "posts/2023-12-09-weekend-reads/index.html#podcasts",
    "href": "posts/2023-12-09-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (12/9/23)",
    "section": "Podcasts",
    "text": "Podcasts\n\nDr. Andrew Huberman on The Tim Ferris Show: Dr. Huberman is a gift to humanity. So much great insight per minute of listening."
  },
  {
    "objectID": "posts/2023-12-09-weekend-reads/index.html#books",
    "href": "posts/2023-12-09-weekend-reads/index.html#books",
    "title": "Weekend Reads (12/9/23)",
    "section": "Books",
    "text": "Books\n\nCurrently Reading\n\nThe Anthonolgy of Balaji Srinivasan: Need to get excited about the future? Learn from one of the best in Balaji.\n\n\n\nRe-Reading\n\nTools of Titans: Re-reding this once a year is almost mandatory for me now. So much great insight that stands the test of time."
  },
  {
    "objectID": "posts/2023-12-22-weekend-reads/index.html#articles",
    "href": "posts/2023-12-22-weekend-reads/index.html#articles",
    "title": "Weekend Reads (12/22/23)",
    "section": "Articles",
    "text": "Articles\n\n4 Types of Professional Time: Always make time for consumption and ideation time.\nMicrosoft Releases Phi-2: Small models are so hot right now.\nNo Meetings, No Deadlines, No Full Time Employees: One meeting a quarter at Gumroad. Yep, that’s the dream. One can only imagine."
  },
  {
    "objectID": "posts/2023-12-22-weekend-reads/index.html#videos",
    "href": "posts/2023-12-22-weekend-reads/index.html#videos",
    "title": "Weekend Reads (12/22/23)",
    "section": "Videos",
    "text": "Videos\n\nMaking LLMs Uncool Again: Hear thoughts from an AI legend, Jeremy Howard.\nA16Z AMA: Various topics tackled by the top dogs at Andreesen Horowitz.\nAngeList QnA with Naval: You can never go wrong hearing from Naval."
  },
  {
    "objectID": "posts/2023-12-22-weekend-reads/index.html#podcasts",
    "href": "posts/2023-12-22-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (12/22/23)",
    "section": "Podcasts",
    "text": "Podcasts\n\nGeorge Mack on Modern Wisdom: Powerful insights on the role memes play in society."
  },
  {
    "objectID": "posts/2023-12-22-weekend-reads/index.html#books",
    "href": "posts/2023-12-22-weekend-reads/index.html#books",
    "title": "Weekend Reads (12/22/23)",
    "section": "Books",
    "text": "Books\n\nAnthology of Naval Ravikant: Don’t think, just buy this book and re-read once a month.\nPoor Charlie’s Almanac: Timeless wisdom from Charlie Munger, may he rest in peace."
  },
  {
    "objectID": "posts/2024-01-05-weekend-reads/index.html#articles",
    "href": "posts/2024-01-05-weekend-reads/index.html#articles",
    "title": "Weekend Reads (1/5/24)",
    "section": "Articles",
    "text": "Articles\n\nSahil Bloom: The Best Ideas of 2023\nSam Altman: What I Wish Someone Had Told Me"
  },
  {
    "objectID": "posts/2024-01-05-weekend-reads/index.html#videos",
    "href": "posts/2024-01-05-weekend-reads/index.html#videos",
    "title": "Weekend Reads (1/5/24)",
    "section": "Videos",
    "text": "Videos\n\nLast Lecture Series: How to Live an Asymmetric Life"
  },
  {
    "objectID": "posts/2024-01-05-weekend-reads/index.html#podcasts",
    "href": "posts/2024-01-05-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (1/5/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nAli Abdaal on Modern Wisdom\nBill Perkins on Modern Wisdom"
  },
  {
    "objectID": "posts/2024-01-05-weekend-reads/index.html#books",
    "href": "posts/2024-01-05-weekend-reads/index.html#books",
    "title": "Weekend Reads (1/5/24)",
    "section": "Books",
    "text": "Books\n\nThe Daily Laws by Robert Greene\nThe Daily Stoic by Ryan Holiday\nA Calendar of Wisdom by Leo Tolstoy"
  },
  {
    "objectID": "posts/2024-01-26-weekend-reads/index.html#articles",
    "href": "posts/2024-01-26-weekend-reads/index.html#articles",
    "title": "Weekend Reads (1/26/24)",
    "section": "Articles",
    "text": "Articles\n\nA Techno-Trad Take on Fertility\nThis is Your Reminder to Say No\nSigns and Portents: Hints about the next year of AI\nMicrosoft Releases Copilot Pro"
  },
  {
    "objectID": "posts/2024-01-26-weekend-reads/index.html#podcasts",
    "href": "posts/2024-01-26-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (1/26/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nSal Khan on the Future of Education\nPeter Attia on Huberman Lab\nMarkel CEO on The Knowledge Project"
  },
  {
    "objectID": "posts/2024-01-26-weekend-reads/index.html#tweets",
    "href": "posts/2024-01-26-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (1/26/24)",
    "section": "Tweets",
    "text": "Tweets\n\nPeacock Boosting Their App Downloads\nSurface Area in Your Life"
  },
  {
    "objectID": "posts/2024-01-26-weekend-reads/index.html#books",
    "href": "posts/2024-01-26-weekend-reads/index.html#books",
    "title": "Weekend Reads (1/26/24)",
    "section": "Books",
    "text": "Books\n\nAtomic Habits by James Clear"
  },
  {
    "objectID": "posts/2024-01-26-weekend-reads/index.html#products",
    "href": "posts/2024-01-26-weekend-reads/index.html#products",
    "title": "Weekend Reads (1/26/24)",
    "section": "Products",
    "text": "Products\n\nTuo Circadian Lightbulb"
  },
  {
    "objectID": "posts/2024-02-19-frp-journey/index.html",
    "href": "posts/2024-02-19-frp-journey/index.html",
    "title": "Thoughts on My Journey in Microsoft’s Finance Rotation Program",
    "section": "",
    "text": "On new years day of 2015, I got an email from a Microsoft recruiter to schedule a phone screen for Microsoft’s Finance Rotation Program (FRP). A week after the phone screen, I was reading the book The Alchemist during a study break and started to think about my personal legend. I thought Microsoft would be a good opportunity and hoped for another interview. That’s right when my phone rang, it was the recruiter informing me that I was about to be chosen for the final super day in February. The weekend before the big interview in Redmond, my brother got me tickets to see the Oklahoma City Thunder take on the LA Clippers, a present for my 21st birthday. I told him I couldn’t go, because I needed time to prepare for the biggest interview of my life. Laughing, he said I had nothing to worry about, I would get the job, and we can use this weekend to celebrate the new job. His confidence in me, and the uncanny coincidence with the Alchemist book, were the “omens I’d follow” into my own personal legend. Thankfully the interview went well, I got the offer, and the rest is history.\nThe Finance Rotation Program at Microsoft is a two year program where recent college graduates complete four rotations across various groups in finance, each lasting six months. It’s a great opportunity to learn about different parts of Microsoft’s business, while also being a world class “try it before you buy it” experience for finding your optimal career path. You get to find what your destined to do, and Microsoft gets a well rounded finance professional that is tuned to find the exact job they want long term. A true win-win.\nI often get asked to talk about my experience from would be FRP’s still in school and existing FRP’s in the program. My career path in the FRP is nontraditional in nature, but is hopefully one anyone with enough high agency can do as well. I write this to those high agency folks. People who want to do interesting things in the world and help others. What follows is my path in the FRP and what I recommend for today’s FRP’s."
  },
  {
    "objectID": "posts/2024-02-19-frp-journey/index.html#rotations",
    "href": "posts/2024-02-19-frp-journey/index.html#rotations",
    "title": "Thoughts on My Journey in Microsoft’s Finance Rotation Program",
    "section": "Rotations",
    "text": "Rotations\n\nInternship: Venture Integration\nI came into the FRP as an intern in the summer of 2015, the same summer Microsoft launched Windows 10. It was a fun time to join finance at Microsoft. Satya Nadella was only installed as CEO a few years back, same as Amy Hood the CFO. The stock price was in the 40s and no one thought Microsoft was doing anything cool or exciting. Safe to say most students at my college had MacBooks instead of Surfaces.\nThe Venture Integration team was responsible for helping acquired companies integrate successfully into Microsoft. While Corporate Development teams work on closing the deal, the Venture Integration team does everything else. From doing due diligence on the companies financial statements, digging through the products source code, all the way to determining if the acquired CEO would move to Redmond. It was a cool space to be in. Code names were used for each deal, and the deal flow was strong. Often we started a new deal every two weeks.\nMy role on the team didn’t involve running deals, although I did get to help with modeling the integration costs for one deal. I was tasked with running the quarterly scorecard process. This involved a lot of cat herding. For each recently closed deal, I would track down metrics to see if we were on track for a successful integration into Microsoft. This was more project management and less about financial modeling or analysis. The cool part was on the last day of my internship I completed the scorecard powerpoint and personally sent it to the CFO for review, who would then share it with the board of directors. So, in a way, I got to work on something that Bill Gate’s got to see. Most likely it was buried in the appendix of the board materials but I know my guy Bill loves to read! So I’m sure he saw it and was blown away by my excellence. At least that’s what I tell myself.\nThe life changing moment for me happened towards the end of the summer. The tech team responsible for reviewing the code of acquired companies hosted a brown bag lunch session about a new service called “Azure Machine Learning Studio”. They demoed a drag and drop tool that could build a machine learning model to predict if an event was going to happen or not. With a few clicks of their mouse they had a working model up and running in a self-serve UI. Only a few lines of code were written, with most of the magic happening in predefined lego blocks they pieced together. My mind was blown. It seemed like a magic trick to me, and I had to figure out how they did it. In school I always loved building three statement financial models to value a companies stock price. The biggest assumption we had to make was around future revenue growth, and that number was always made up. You could say xyz company’s revenue will continue to grow 10% because of xyz reason, but in the end it was a guess. The fact that you could build a machine learning model to create a more accurate prediction of the future was astounding to me, and I had to learn more.\nWithin a week after that presentation, all of the FRP interns got to meet with the CFO. A lot of questions were asked. I’m not even sure who asked it but a question around AI was brought up. Amy said that AI and machine learning will become very important in finance one day, and it will be an important skill for everyone to know. There are few moments of pure clarity in a lifetime where everything comes together and makes perfect sense. This was one of those moments for me. I had to figure out this AI thing. I had absolutely no clue how to start. I was only a finance major with zero technical skills. What I lacked in hard skills I made up for with enthusiasm to go figure it out any way possible.\nThe internship ended and thankfully I got the return offer. I remember the head of the FRP looking at my manager during the final review meeting. She said, “should you tell him or should I tell him”. My heart dropped, I thought I messed up and wasn’t getting the offer. Thankfully that confusion was cleared up real fast and I had a return offer in my hand. How can I say no to an opportunity to come back to Microsoft? Within a week back at school I accepted the offer and had total peace of mind going into my final year of undergrad.\n\n\nFirst Rotation: Windows COGS\nMy first rotation was a classic rhythm of business (ROB) rotation in the world of Windows, Microsoft’s oldest business. It was a great place to earn my sea legs in traditional FP&A work. I was responsible for owning the royalty portfolio for “codec” payments. Basically there is special software needed to read/write disks that are inserted into an optical drive on a computer. That software comes from companies like Dolbe, and Microsoft had to pay them to license the software on Windows OS. Wow, that makes me sound old. Working on royalties for optical drives on computers. Safe to say no one even thinks about optical drives anymore.\nIt wasn’t the prettiest work, but it helped me learn the basics of closing the books each month and forecasting the future each quarter. I had to submit journal entries and make manual adjustments to our payments when needed. Once you got the hang of pulling the data and making journal entries, the work wasn’t all that hard. So most of my time toward the end of the rotation was spent trying to automate every single part of the ROB process. What was initially a complex refresh process handed to me at the start of my rotation was transformed into a simple click to refresh excel that did all of the heavy lifting for you. It was my pride and joy.\nMy manager gave me a powerful piece of feedback towards the end of the rotation. He said that I liked building things. Some people like to start things, others like to keep a process going or optimize it. I was someone who liked to build new things. That idea stuck with me. I eventually became the guy on the team who could build any complex workflow as a model in excel. For example building out the new forecast consolidation file that all team members could add their component forecasts into. While it was fun to build this kind of stuff, it wasn’t AI work. For that I had to continue on my data hero’s journey.\n\n\nSecond Rotation: Worldwide Commercial Solutions Finance\nFor my second rotation I wanted to leave the world of ROB behind and get my hands dirty with data. I went to a team that did two things. First was analytics around discounts and customer renewals across Microsoft’s commercial business, and the second was around supporting the financing of customer purchases. Back in the day a customer could finance a large software purchase so they didn’t have to pay for most of it up front. We would partner with a bank to finance the payment or even do the financing ourselves through our Treasury department.\nThis rotation was all around analytics, which was exactly what I needed. I was able to analyze patterns around the type of discounts we give to customers, and how those discounts compound as a customer continues to renew their software purchase contracts. Another big ticket analytics item was around tracking special azure financing deals. This was a key metric we reported directly to the CFO, so a lot of eye balls were on it. I was the guy to track down this data and create interesting ways to understand it.\nPower BI was recently released. No one on the team knew how to use it, but understood how powerful it could be when visualizing data. So I became the Power BI guy on the team. The one to figure out how to use it, then teach it to everyone else. Thankfully the previous FRP started them on the Power BI journey by building some initial dashboards. My job was to improve those dashboards, then get everyone else up to speed on how they could build their own. It was a solid experience in the world of analytics. One where no one could show you how to solve certain problems. You were on your own and had to figure everything out yourself.\nThe analytics experience was great, but still no AI. While on the team I tried to find ways we could apply AI and machine learning to specific areas but couldn’t find a good opportunity. At this point I started to learn more on my own through books and courses on the internet. I was flying blind, with no one to guide me to ensure I was headed in the right direction. That’s when the power of serendipity came and altered the course of my FRP career.\nAnother FRP analyst in my class got her last pick on her list of ranked rotations. Like choice number 32 out of 32 options. Right before she started on her team for the second rotation, there was a large re-org and she was able to now chose wherever she’d like to go in the broader org. A true choose your own adventure. She had an information systems background and found a team that was just starting to get into the world of machine learning. I heard about this and knew exactly where I wanted to go for my third rotation.\n\n\nThird Rotation: Finance Business Intelligence Services\nWorking for the FBI, what a fun thing to say. The FBI Services team was almost an in house consultancy team, who supported most folks in finance. If someone wanted a special dashboard, custom tool, or even machine learning model they could reach out to this team and get the help they needed. The team consisted mostly of program manager (PM) type roles for full time employees, and various technical roles for vendors.\nComing to this team was being at the right place at the right time. In the summer of 2015 (during my internship) our CFO reached out to the head of the cloud engineering team, asking him to help get some initial machine learning solutions off the ground. Eventually this work was transferred to the FBI Services team, who hired a team of data science vendors to keep the work going. Most of the ML solutions centered around time series forecasting, since that is easily the biggest bang for buck ML work in the corporate finance space. Almost everyone under the CFO is responsible for some sort of forecast, so the impact with ML is enormous.\nI came on the team as a PM who would partner with the data science vendors to build ML solutions. It was a dream come true. Finally I had people I can talk to about ML. Pick their brain. See how they think about ML problems. I also found a data science certification offered through Microsoft (on the EdX learning platform). It had a full learning track for python, no coding experience required. Thankfully as a Microsoft employee I could take the courses for free. While on the team I was able to make the learning part of my job, and even devote part of each day to taking the courses. It was a dream come true.\nThe big project on the team was around creating a centralized tool for our finance teams in the field, teams who support sales and sit all around the world, to use for their quarterly forecast of the commercial business. We were tasked with building a tool that could combine machine learning methods with existing sales pipeline based methods (like taking what’s in the sales pipeline and multiplying it by how many deals we have closed on average historically). One tool that everyone in the field could use instead of building their own manual excel model that eats up weeks of their time every forecast cycle.\nIt was a fun project to work on. The stakes were high, and no one had done anything like it before. It wasn’t just a machine learning project, but also a centralization and automation project. A crazy ven diagram of categories of technology. Over the course of my six month rotation we (and I say that lightly since I was the rookie on this project) were able to go from initial concept to working plugin in excel that could automatically calculate the forecast for the user, allow them to select what forecast methods they’d like to use, make any manual adjustments, and save that data back to a data cube. A herculean effort that had a few late nights toward the end of the project. It was a fantastic learning experience, with so many different opportunities to learn in one project. How to work with technical and non-technical people. Getting the most from ML models, and knowing their limitations. The change management required to get to people to want to use the new tool and move away from what they’ve always done.\nI was also able to knock out the entire Microsoft certification in data science. I could now write python code to analyze data, train models, and create predictions. Everything that astounded me as an intern was now in my tool kit. I had data super powers and it felt awesome. The only problem is I had no idea what came next. Towards the end of the rotation I wanted to stay on the team and graduate early. Unfortunately the previous FRP was able to secure a full time spot on the team before I asked. Now I was in a tough spot. Knowing what I wanted to do, but not knowing where to do it. Thankfully serendipity came to my rescue.\nAt a FRP manager round table, my manager noted to the group that I was looking for a home for my fourth and final rotation. I wanted to work on machine learning but had no clue what team to rank highly. Another manager spoke up, saying they were just starting to dip their toes in the ML space and could have me on the team. That chance encounter lead me to ranking that rotation as my number one pick. Other FRP’s thought I was crazy. Usually that rotation went to a first year FRP, often as someone’s first rotation. I saw it as an opportunity to work on a technical team and carve out a path to working full time on ML. Safe to say after I joined the team every FRP who has worked on the team since has been a second year analyst.\n\n\nFourth Rotation: Office/Dynamics/Bing Business Intelligence\nMy final rotation was a BI team that supported finance teams in the Office, Dynamics, and Bing product spaces. Previous FRP’s worked on an executive scorecard that would get sent to senior leaders. I had my fill of scorecards while an intern so I was relieved that they were changing the rotation to allow me to work on machine learning problems. I felt like the luckiest guy in the world. I could actually write code to build machine learning models and get paid for it, all without having a degree in the subject.\nI worked on two main projects while on the team. One was around forecasting search traffic volume for our Bing business partners, and another was focused on trying to predict if a potential customer purchase in our sales pipeline was going to close or not. The search volume project was exciting because I could write code from scratch to create the forecasts. It was my first true ML project that I was responsible for coding. Building something from nothing, it was exhilarating. Just like my first rotation manager said.\nThe second project was a tough one. Being able to create a classification model to predict the likelihood of a deal in our customer sales pipeline closing was a tricky project. One where getting high quality historical data was tough to get. I knew this was something analytics teams in sales had to of tried to solve before, so I sent out the bat signal through a few distribution groups and got a hit. A data science team in India had done the exact work I needed and could send me the results of their models prediction. What should have taken months of work was now widdled down to a few weeks to pull the prediction data and display it in a Power BI. Leveraging the work of others proved a powerful lesson for me, and allowed me to finish my project that much faster.\nMy time on the team flew by and I was quickly approaching the end of my FRP tenure, meaning I needed to find a full time job ASAP. Most of my time quickly shifted to checking internal job boards and emailing managers in other BI teams, hoping they had a spot for me. I wanted this ML train to keep rolling. My current team said that they could potentially offer me a job, but I had to wait a little longer. This did little to remove any fear of not finding a job post FRP graduation. FRP’s were given some flexibility in finding a full time job. After graduating the program in September, Microsoft gave you about 4-6 weeks more to settle into a final job. You would still get paid, even if you didn’t have a team to call home. Being in this corporate limbo sounds nice in theory but in reality is awful. Like your days are numbered.\nThe waiting ended in a re-org of another BI team joining ours. So there was a hiring freeze in the meantime while both teams were being combined. After that, a job was offered, and the rest is history. I’m still on that team today. We’ve been through a lot of re-orgs since 2018, but I still get to work on ML every single day. I consider myself lucky."
  },
  {
    "objectID": "posts/2024-02-19-frp-journey/index.html#advice",
    "href": "posts/2024-02-19-frp-journey/index.html#advice",
    "title": "Thoughts on My Journey in Microsoft’s Finance Rotation Program",
    "section": "Advice",
    "text": "Advice\nHere is my advice for anyone interested in the FRP, currently in the program, or FRP’s who are about to graduate and start the next phase of their career. I have to say that this advice may not stand the test of time, hiring practices constantly change. So take these next words with a grain of salt.\n\nAspiring FRP’s\nRecruiting for the FRP used to be done at target schools. Places like Texas, Penn, and the University of Washington. Kids from these schools normally had the best shot at getting an interview, because Microsoft would come onto campus and do interviews. Thankfully this started to change when I applied for the internship. Now Microsoft finds FRPs from any school in the country, even recently expanding to other parts of the world. This opens up the opportunity of joining the FRP to almost anyone in the world, but because of this it becomes harder to get recognized in a sea full of applications.\nBefore you even apply, you need to put yourself in a strong position by building a portfolio of experiences and skills that make you a perfect match for the FRP. Here is what I recommend.\n\nHigh Agency: Having good grades is a fantastic accomplishment, but nowadays everyone has good grades. It becomes more of a check box than something that differentiates yourself from others. Grades show that you can follow instructions, keep deadlines, and demonstrate some type of understanding of your study area. What companies want though are people who go out into the world and make things happen. People who are proactive instead of reactive, or better put have high agency. The best definition I’ve heard of high agency is “a person you would call to bail you out of a third world prison”. Someone who can do that can most likely figure out how to do any kind of job, and become indispensable to their company. The best way to showcase high agency to recruiters at Microsoft is to show how you spend your time outside of the classroom. If you have a 4.0 but only play video games when you’re not studying, your resume will most likely get thrown away. Don’t get me wrong, you could be a fantastic hire but a recruiter has no idea how you get along with others, your communication skills, or anything that’s not related to reading a textbook and taking a test to prove you read the textbook. The best way to showcase high agency is with leadership experience. Running your own business, being the vice president of your sorority, treasurer of your finance club, running your own charity, working a full time job to pay for school. These are all ways to demonstrate that you are someone who goes out and makes things happen. That you indeed have high agency.\nPrevious Work Experience: Having already done the job makes it easy to get a similar job. This can sometimes be a chicken or the egg situation, where needing a previous job to get your first job isn’t possible. That’s why I think you should start small. For me, it started the summer after 8th grade. I got a job in the concession stand of my local city pool. It wasn’t fun work, but lead to my second job working for the famous Jack Stack BBQ in the catering department. That job lead to multiple summers working at two different golf courses as a cart boy. All to say that it eventually lead to my first real business job working as an intern on Fridays during the school year at a financial advisor company. I didn’t do much, but I showed up every Friday ready to help. Having that experience gave me the opportunity to work in FP&A at HR&R Block in Kansas City as a legit intern. That experience then opened the door to work at Microsoft as an FRP intern. Small beginnings lead to massive results. Having a crap job at the beginning can lead to your dream job later down the line. Start this path as soon as you can, because hard work compounds.\nData Superpowers: In this modern age of AI, knowing how to work with data is critical. I’m not just talking excel skills. Being able to pull, manipulate, visualize, and tell stories with data is most of what you do in Finance at Microsoft. Having some programming or low code skills in data tools like python or Power BI can go a long way. This gets supercharged with tools like ChatGPT, where a little knowledge of coding can quickly make you on par with an entry level data scientist or software engineer. Imagine not learning email or Microsoft Office in the 90s. Not knowing email today is basically a firable offense. Using AI powered tools might have the same path in knowledge work. Get ahead and get data superpowers.\n\nYou might have all of those experiences and skills called out above, but that’s only half the battle. You still have to get noticed somehow. Either by a recruiter or someone else who works at the company. Here is what I recommend.\n\nConferences: Microsoft has pivoted away from on campus interviews and now does a lot of recruiting at conferences. This is how the first round of interviews get offered. These conferences are usually meetings of large student organizations like Association of Latino Professionals for America (ALPFA), Management Leadership for Tomorrow (MLT), Out for Undergrad (O4U), and National Association of Black Accountants (NABA).\nEmployee Referral: Getting your resume referred by a current or FRP alum can go a long way, but it’s no guarantee of an interview. The best way to get a referral is by actually knowing someone who works at the company, or having a shared interest with them. Blindly messaging Microsoft employees on LinkedIn could work, but you’d have better success finding someone who went to your same high school or served in the same branch of the military. Having a shared connection around a place or thing helps.\nIntern First: The best way to get a full time offer is to be an intern the summer before. 50-90% of full time FRP’s were an intern the previous summer. This is the path with the greatest opportunity of a job. Don’t go work at a bank in the summer, assuming you’ll get a shot at Microsoft in the fall if you don’t like your bank job. Be in it to win it from the start.\n\n\n\nCurrent FRP’s\nFor folks currently in the FRP, I hope you’re having just as much fun as I did. Normally when working with FRP’s today I have to fight the urge to grab them around the shoulders and beg them to cherish being in the FRP (just like Billy Madison). In order to get the most out of the program you should do the following.\n\nAlways ask for what you want. The biggest unlock I learned in the FRP is to just be up front with what you want in the rotation. Telling your manager at the beginning of the rotation what you’d hope to get out of the next six months can unlock doors you never thought possible. In my fourth rotation, they literally changed the entire rotation because I simply asked if I could solely work on building ML solutions. Don’t forget I’m still working in that job six years later. The best approach is to ask for what you want before you rank your next rotation. Bear in mind you can’t just say something like “I want to work with the CFO directly”. I’ve seen interns do this in the past, and it doesn’t end well for them. Know what you want, have some sort of skill or experience doing it, then ask for it.\nAlways see who else is working on something first. It’s always fun to build something from scratch. An important skill to learn in the FRP though is building on the work of others. This is a key component of how impact gets measured at the company. In my fourth rotation, I was able to have impact 10x as fast on the deal pipeline analysis project because I found another data scientist team who already had a deal pipeline conversion model I could leverage and start using immediately. If I didn’t ask I would have spent most of my time trying to build one, which may not have been useful at all. Always ask around before building something new.\nEmbrace serendipity. Your career path is never going to work out 100% exactly like you planned it, and you don’t want it to. You need to brace serendipity, keeping an open mind for making the most of new opportunities. The best example was the FRP in my class who got their dead last choice of rotation, but ended up working on that team full time post FRP. You never know where a road is going to take you. Don’t be afraid to explore different paths or see things through on an existing path.\n\n\n\nGraduating FRP’s\nMy last piece of advice comes to those who are ending their FRP tenure and now look on to the rest of their career. It can be a scary feeling, so here’s what I recommend.\n\nCast a wide net. Don’t just apply to one job, hoping you’re going to get it and end up with the perfect job. You need to apply to a crap ton of jobs. Don’t be picky. Even apply to jobs that don’t look exciting on paper, because that could change once you meet with the hiring manager. Serendipity could strike in your favor.\nThe best jobs go to people with high agency. Your perfect job is out there, but it may not be listed on the internal company job board. So have high agency and chase down opportunities that don’t even exist yet. This starts months before graduation. You need to network with teams you are interested in, meeting with potential hiring managers who could one day give you a job. For me, I went to every business intelligence team in finance. I emailed the director level leader, gave my short sales pitch saying I wanted do work on ML solutions for their team, and asked if we could chat to discuss any openings on their team. I’d even ask them if they knew any other team I should go talk to. Most didn’t offer me a job. Some continued to offer me interviews years after the FRP. You just never know. If you are talking to teams that aren’t hiring, you are basically in your own talent pool. Looking for opportunities that might materialize later once you graduate. If a job does open on that team, who is their first call? Probably the person who proactively reached out and made a connection well before a job was ever created. Use this to your advantage.\nJoin for the team, not the job. The job will eventually come. Being in the right position is important more than what you do. Most jobs after graduating may not be the most fun. They will be work, maybe even grueling. Someone needs to do the work and congrats it’s your turn to eat a plate of crap for the company. Going to the right team is definitely more important that getting the dream job you wanted on a crap team. For me, I took a job on my fourth rotations team. They said I could do ML work, but most of my time would be spent as a program manager (PM). I hated PM work, but knowing that part of my time could be doing ML work was all I needed to hear. Eventually my job morphed into 100% ML work after I was able to prove to my manager that I could have more impact on the ML side than the PM side of my job. Being on the right team or org allows you more opportunity to work on things you eventually want your career to morph into. The team I joined post FRP has now morphed into a 400+ person engineering team. One that’s quickly becoming the defacto engineering team in finance at Microsoft. Being on this team over the years has created much more opportunity compared to doing similar work on a different team. Thankfully I was well positioned, even though I didn’t have the ideal job initially. See what I mean? So don’t be too picky on the job, but more picky on the team."
  },
  {
    "objectID": "posts/2024-02-19-frp-journey/index.html#closing-thoughts",
    "href": "posts/2024-02-19-frp-journey/index.html#closing-thoughts",
    "title": "Thoughts on My Journey in Microsoft’s Finance Rotation Program",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nBeing an FRP teaches you a lot. How to ramp up quickly on new skills, being able to juggle many things at a time, effective communication and teamwork, and countless other skills. All things that make you a well rounded person both professionally and personally. Some of my favorite memories come from the summer internship or hanging out with my FRP coworkers in building 26 (before they tore it down). I think it’s a great idea for anyone who is interested in technology and business. If you’d like to apply to the program, please do so on the Microsoft career website. Whether you’re interviewing for the FRP or trying to land your post FRP role, I wish you good luck!"
  },
  {
    "objectID": "posts/2024-03-13-power-pursuits/index.html",
    "href": "posts/2024-03-13-power-pursuits/index.html",
    "title": "Thoughts on Power",
    "section": "",
    "text": "A few years ago I was at dinner with a few friends. Out of the blue one of them asked me “what is the meaning of life?”. Without thinking I blurted out “love”. Thinking that in the end, everything comes down to love. It sounded cheesy and cliche coming out of my mouth. Over the years though, I continued to ponder the question and now think more than ever that it’s the right answer.\nWhat most people want in life can mostly be boiled down to the pursuit and gaining of power. Some types of power are good for you, but the most common ones always leave you unfulfilled. You know what does fulfil you though? Love. It fills you up better than power ever could. Let’s review how most things we chase in life are just hidden forms of power.\n\nTypes of Unfulfilling Power\nStarting with unfulfilling power. These are things that can be attained in ok ways, but doing them just for the pursuit of having it is a good way to waste your life.\n\nMoney: The most popular form of power and game we play in society. Granted you want to get to a certain level of money to have a good life, but after that the more money you have the less fulfilled it makes you. Mo money, mo problems.\nStatus: This is power over how people in society look at you. There are many ways to get status. Going to an elite college or working at a prestigious job are ways to gain status. You also gain status around how you spend your money. Living in a fancy house in a fancy neighborhood punches a ticket to a different level of status than someone living in a trailer park. It’s easy for people to see what you own, and most people own certain things to convey status. Does a Rolex tell more accurate time than your phone? No but it shows that you are the type of person who can spend $10,000 minimum on an accessory you don’t really need. Increases in status normally feed your ego, which is not good for anyone.\nBeauty: Having power over other’s attention and sexual desires. This is a weird one because it can either be gained naturally (just being born) or through external means (botox, plastic surgery). If you hit the genetic lottery, congrats, but the external route is a slippery slope that usually doesn’t end well.\nFame: Having power over people’s attention. Will Smith says the process of becoming famous is fun, but maintaining fame is just ok, and losing fame is hell. You don’t know how important having a private life is until you become a famous person. Can Will Smith casually go to a local park with his family on a Sunday? Nope. His life is forever inconvenienced by his fame. How much would he pay to have a quiet afternoon on a park bench with no interruptions? It might be more than you think.\nLegacy: Having power after you’re dead. How many people donate tens of millions to a university anonymously? Almost none. Most donate and in return get their name on a building. Maybe they’re stewards of higher education but maybe they just want their name to be remembered after they’re dead. Who cares if people remember your name 100 years from now, you will be dead. I repeat, you will be dead. Has your grandpa ever talked to you about his grandpa while growing up? Probably not. People are forgotten and that’s ok.\nLeadership Roles: Having power over people’s careers. Most people think the further they progress in their careers, the more likely they will end up managing a large team or company. While that’s true for a lot of jobs, it may not be everything you expected. Maybe instead of the management promotion you actually just wanted to work on higher impact projects and have more autonomy over how you get your work done. Think about a band like the Rolling Stones. They are very good at their job, but it’s not like after their second album their record label said “you have been doing such a good job that we think you should be promoted and start overlooking other bands”. That would be crazy. Instead they were given every opportunity to write more songs and perform to bigger audiences. I think knowledge work will move more towards that in the future. Corporate rock-stars will forgo the management path and truly become the best in the world at something. With scalable technology the best person in the world at a task can literally do it for everyone else in the world.\n\n\n\nTypes of Fulfilling Power\nI’m not saying that we should all shave our head, sell our possessions, and live as a monk. It’s ok to do things in life that increase your power, but I recommend trying to increase types of power that can actually fulfill you instead of leaving you wanting more. There are types of power that compound as you continue to increase them. These are true super powers that should be prioritized in life. Here are the types of power worth pursuing.\n\nFreedom: Having power over how you spend your time. Why does America rock? People will say the freedom they have, which is another way to say they can truly do whatever they want. Having control over your time is the best power you can have.\nHealth: Having power over your own body. You might have a million thoughts and worries bounce around your head in a typical day, but when you’re sick or injured you only think about one thing. Improve your sleep, diet, and exercise and see if your life doesn’t improve by an order of magnitude.\nLearning: How does that old saying go again? Oh yeah, knowledge is power. Some would argue it’s potential power, waiting to be applied. Noticed I listed this as learning and not knowledge. Having knowledge is great but there is never a reason to stop acquiring knowledge throughout life. This power compounds as you learn more. Lean into the eighth wonder of the world and keep learning.\nReputation: Having power over other’s perceptions of your character. Warren Buffet can do billion dollar deals over the phone, no contract needed, because of his stellar reputation he built over a lifetime. A good reputation can increase your luck surface area in life. A good reputation takes time to build, but can be gone in an instant.\n\nRelationships: Having power over other people’s time. Loving friends, an amazing spouse, and children running around your house are some of the most rewarding things in life. Not having strong relationships is literally the equivalent to smoking when it comes to lifespan. I would choose these relationships wisely though. Once when I was in college my business class went to visit a local bank in Kansas City. The bank’s founder, a hunched over man in his 80s, came out to speak to us students. When asked for advice for living a good life, his response was simple. He said, “be around good people”. The older I get the more this advice makes more sense. You truly are the average of the five people you hang out with most. Choose wisely.\n\n\n\nLove Over Power\nLove what you do, love who you spend your time with, and most importantly love yourself. The most important things in life boil down to love. Truly a first principle when it comes to living a good life. Call me a hippie but it’s the truth. Stay away from the most popular desires in life that ultimately are unfulfilling power, double down on power that does fulfill you, and most importantly optimize for more love in your life."
  },
  {
    "objectID": "posts/2024-03-22-weekend-reads/index.html#articles",
    "href": "posts/2024-03-22-weekend-reads/index.html#articles",
    "title": "Weekend Reads (3/22/24)",
    "section": "Articles",
    "text": "Articles\n\nDHH On Coding AI Agents\nWhen Confidence Backfires"
  },
  {
    "objectID": "posts/2024-03-22-weekend-reads/index.html#videos",
    "href": "posts/2024-03-22-weekend-reads/index.html#videos",
    "title": "Weekend Reads (3/22/24)",
    "section": "Videos",
    "text": "Videos\n\nSahil Bloom on Building Life Systems\nSam Altman on Lex Friedman\nJensen Huang at Stanford GSB\nTikTok Ban, AI, and more on All In"
  },
  {
    "objectID": "posts/2024-03-22-weekend-reads/index.html#podcasts",
    "href": "posts/2024-03-22-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (3/22/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nMorgan Housel on Modern Wisdom"
  },
  {
    "objectID": "posts/2024-03-22-weekend-reads/index.html#tweets",
    "href": "posts/2024-03-22-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (3/22/24)",
    "section": "Tweets",
    "text": "Tweets\n\nGrateful that Kansas won their first round game\nHow Momentum Rules Your Life\nCognitive Biases"
  },
  {
    "objectID": "posts/2024-03-22-weekend-reads/index.html#books",
    "href": "posts/2024-03-22-weekend-reads/index.html#books",
    "title": "Weekend Reads (3/22/24)",
    "section": "Books",
    "text": "Books\n\nThe Numbers Game by Chris Anderson"
  },
  {
    "objectID": "posts/2024-04-02-time-series-domain-expertise/index.html",
    "href": "posts/2024-04-02-time-series-domain-expertise/index.html",
    "title": "Time Series First Principles: Domain Expertise",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the first principle of a good time series forecast, domain expertise. Check out the initial post to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nIntroduction\nAny data scientist worth their salt can create a time series forecast for you. They can pull some data, train some machine learning (ML) models, and give you a forecast. All with you out of the loop. If that’s the case at your company, run! This is a big red flag. While that can sometimes yield good results, often the most important ingredient is missing, which is strong domain expertise about what you’re trying to forecast. This is where strong understanding of the business and market forces come into play. You know, the stuff that finance people excel at. Pairing robust ML models with strong domain expertise about the area being forecasted always yields the most accurate forecast. It also increases trust in that forecast, since the humans using that forecast know the model took into account important factors that influence the business. In this post we’ll use a hypothetical example of a company’s real estate spending to showcase the importance of domain expertise.\n\n\nTranslating Domain Expertise Into Features\nHow does domain expertise change how a ML model is created? This can manifest in many forms. The most common is changing the kind of data used in training a model. Variables that a model learns from are called “features”. Let’s apply this to our real estate spend forecast example. In the last few years, COVID and the work from home revolution have changed how people come into work. This changes how many people drink coffee, use the copier, and even which buildings stay in operation for a company. Simply pulling historical building expense data and training a model could get you ok results, but to get to peak performance you need domain expertise around what actually moves the needle for building expenses. Example features could be the square footage of a building, how many people actually badge into that building each month, even the periods where COVID was at its worse and a work from home mandate was in effect. All of these things are custom knowledge, most likely kept inside the heads of the finance workers who oversee the real estate space within a finance org.\n\n\nIteration is Key\nThrowing all of your ideas as features into a model from the start is usually not a good idea. Instead having multiple rounds of iteration is key. In the real estate example, it’s best to start out with no external features. Just use historical spend to forecast future spend. Starting with this simpler approach can sometimes get you 90% of the accuracy you need, maybe even 100% if there are stable trends and seasonality that carries into the future. Run this first to see what the initial accuracy is, and if it doesn’t meet your requirements that when we can refine by adding new data.\nOnce you have the baseline, you can look deeper into the accuracy results to see where the forecast is performing poorly. This is where domain knowledge kicks in. Poor initial forecast performance can be fixed by asking the domain expert if there is a difference between what the model knows and what a human knows. If there is a gap, can that be quantified as data to teach a model? This kind of insight can be added into a model with easy to find numeric data, or even as binary yes or no values (1 or 0) to denote when a specific one off event happened. This iterative process is where the magic happens.\nFor the real estate forecast, maybe there was a period where expenses jumped sharply in one month and stayed at that new level for the rest of the year. This will be hard for a ML model to understand or even anticipate, but the domain expert of the real estate space knows that in that specific month there were two new building openings. So the expenses of course jumped up a significant degree and stayed like that going forward. Knowing this, we can get historical square footage information and add it into our model. We can even incorporate future buildings that might be removed or added going forward. This will help a model understand how changes in total buildings impact spend.\nSo we added total square footage to our model and the results improved compared to our initial baseline of no external features. But it didn’t move the needle that much. Even though our company might be adding more buildings, in recent years the spend may not have a perfect correlation with added square footage. Knowing this, the domain expert recommends using anonymous badge in data to see who is actually coming into work. Pre-covid this data may not have been useful, since most buildings were always at max capacity with everyone coming to work each day. Now in a post-covid world this has changed forever. Some teams might only be in their assigned building 2-3 days a week. Or maybe they never returned in person, deciding instead to buy ranches in Wyoming with fast WiFi. Combining the square footage and badge in data into the model yielded fantastic results, much better than the initial baseline.\nAfter reviewing the improved results with the domain expert, the future forecast still seems a little low compared to the domain experts expectations. The domain expert has one last idea, trying to teach the model how COVID impacted spending. This can be quantified as a binary variable, where in all rows of the data we add a 1 if COVID was impacting the world, and 0 when it wasn’t. This means from early 2020 - early 2022 we have values of 1 and every period before and after we give a value of 0. A model can now understand that what happened over those two years was mostly a one off situation that is not expected going forward. After the ML model is trained with this new insight the back testing now looks great and the future forecast matches the expectations of the domain expert.\n\n\nReversal\nGetting high quality data to use as features in a model is always a good idea. There are times though where the amount of historical feature data might be lacking. For example, we may not be able to get more than 3 years of historical square footage data for our real estate expense forecast, even though we can get 5 years of historical spend data. What should we do? We can shorten the historical spend data to the last 3 years to match the square footage data, but having less data can sometimes degrade model performance. So in some cases choosing to use the full 5 years of historical spend without the square footage data is the best approach that yields the best accuracy.\nWhen facing this dilemma, try both approaches and see how accuracy is effected. I’ve seen many times that using more historical data of what you’re trying to forecast is often more accurate than shortening that data to combine it with external features.\n\n\nFinal Thoughts\nStarting any ML process without a business domain expert in the room is always a bad mistake. They are the cheat code in the video game that gets you to level 20 in half the time. Involving them early and often while also adopting a quick iteration approach can create a world class forecast that is trusted by the ultimate end users, which often are the domain experts themselves. At the end of the day most ML forecasts come down to trust by the end user. That’s why domain expertise is the first principle in building quality time series forecasts."
  },
  {
    "objectID": "posts/2024-04-08-time-series-garbage/index.html",
    "href": "posts/2024-04-08-time-series-garbage/index.html",
    "title": "Time Series First Principles: Garbage In, Garbage Out",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the second principle of a good time series forecast, garbage in garbage out. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nYou’re Not a Wizard, Harry\nA common I see people in finance make when trying to use machine learning (ML) is around approaching it like a magic wand. Thinking as long as they bring in some data and throw it over the fence to a ML process, a perfect forecast will come back to them. All shiny and clean. ML should be able to find all of the patterns in the data and do things we humans can’t fathom right? No, wrong. ML is not a cure all thing. Having a good ML forecast starts with having quality historical data for a model to learn from. Without good data, you won’t get a good forecast. It’s as simple as that. Let’s dive into ways data can be stinky and how we can sanitize it before training models. To help illustrate each point we’ll use an example of a monthly sales forecast.\n\n\nAmount of Historical Data\nIdeally you want to get as much historical data as possible. If we want to forecast the next quarter of sales, it’s a bad idea to only use the last 12 months of historical data to train a model. Usually I try to get 5+ years of historical data before training any model. This allows for enough year over year observations for a model to learn from. For monthly forecasts, I won’t even start a forecast project if there is less than 3 years of historical data.\nHaving sufficient historical data creates the opportunity to have sufficient model back testing. Where we can see how models performed over the last few months of the data. We can then use that back testing accuracy as a proxy for what to expect for the future forecast. The less historical data you have, the less you can back test.\nThe more data you have, the longer you can forecast into the future. If I only have 3 years of historical data, it’s a bad idea to try and forecast the next 2 years. A good heuristic is to cap your forecast horizon (periods you want to forecast out) to less than 50% of the historical data. So if you want to forecast out the next 3 years of monthly sales, you need at least 6 years of historical data. When in doubt always get more data.\nWhat if I have tons of historical monthly sales data, but a cool new feature I want to use around sales pipeline is only available for the last 2 years? Most of the time, stick with using more historical data even if it’s at the expense of using highly correlated features but less historical data. Feel free to try both approaches, but often the one with the most historical data wins.\n\n\nTrend and Seasonality\nMost time series can be broken into three pieces. First is trend, is your data going up or down over time. Second is seasonality, are there peaks and valleys in your data that happen at the same time each year. Finally is the “error” or “residual” component, which is anything left over after accounting for trend and seasonality. Think of it as noise in your data. This approach of breaking down a time series into separate pieces is called time series decomposition.\nHaving recurring trends and seasonality in your historical data make things 100% easier to forecast in the future. If your data has trends that change month to month and seasonal patterns that evolve over time, your data is basically all noise. A noisy dataset is a bad dataset, one that can’t be modelled effectively by any ML model. Take a look at the below time series, each broken out by trend, seasonality, and residual. Now tell me which one would be easier to train a model on? Which would produce a high quality future forecast?\n\nYou can deal with noisy data like this in a few ways. The first is to just change the grain of the data. For example, if this was forecasting a specific product SKU, maybe instead sum it up to a higher level like product category. That way more stable trends and seasonality might appear. You can also try to add features (variables to your training data) to try to teach a model why the seasonality and trends in the data are messy. For example the main COVID years from 2020-2022 really throw a wrench in any trends or seasonal patterns in most data sets. So adding information to a data set that tells a model that there was a special one-off situation for specific periods can help a model learn the right kinds of relationships in the data and generalize well to new unseen data going forward.\n\n\nMissing Data\nMissing data is the silent killer in forecasting. If you don’t specifically look for it you might never know it’s the reason your forecasts perform poorly. Missing data is important because many models (either statistical models or ML models) often need all sequential date observations of the historical data to train a model. Even one period of missing data can throw off an entire model and lead to poor performance.\nOften times financial systems will not have tons of missing data. It’s important to know if the data that is missing should mean treated as actually missing or seen as a true zero value. For example, if we have product sales missing for a specific month, should we classify that value as truly missing or just hardcode that value to zero? Make sure you clarify that with whoever owns the data.\nIf the missing should be zero then that’s a quick fix, but if it’s truly missing then you now have another problem on your hands around what to do. Simply replacing the missing value with zero can throw off any trend or seasonality patters like we discussed earlier. Common ML advice is to replace missing feature data with the median or mean value of that feature, but this is terrible advice for time series forecasting. Usually the best approach is to use some sort of simple statistical model that can understand the trends and patterns of data around the missing value and impute what the value should be. This will keep existing trends and seasonality patterns in the data, meaning your future forecast will be more robust.\n\n\nOutliers\nAn outlier in time series forecasting is an atypical data point that significantly deviates from the overall pattern of the data. They can occur multiple times in a historical time series or just be a one off for a particular period. Either way, their presence can greatly impact how a model learns from the data.\nOutlier detection in time series forecasting often involves statistical methods, anomaly detection algorithms, or visual inspection to identify data points that significantly deviate from the typical patterns of the series. Techniques include setting thresholds based on standard deviations, using moving averages to smooth the series and highlight anomalies, applying machine learning models like isolation forests, or utilizing robust decomposition methods (like STL) to separate the series into components and identify outliers in the residuals.\nTake a look at the chart below. See how just one large value towards the end of the time series completely changes the trend and seasonality. A model might take this data and produce a huge forecast going forward, since the trend changed drastically based on the outlier. It might also have a huge spike for that specific period next year, since it learned that seasonality recently changed.\n\nThere are a few ways we can handle the presence of outliers. First we can leave it alone, and let it’s presence impact our future forecast. Maybe after talking with the business domain expert they say that there is a foundational change in the business (new product launch, tax change) that means we expect to see similar values in the future. Second we can add some more information to our data to explain what happened in that period and if we expect it to happen again in the future. If the outlier was caused by a new product launch, we can label that as a feature in the data and also tell the model if we expect any product launches in the future. A model will then learn of these one off patterns and adjust the forecast as needed. The final method is to remove the outlier altogether. Once removed, we can treat it like a missing value and replace it with a value more in line with recent trends and seasonality. If it’s truly a one off thing that will never happen again then removing it is sometimes the best approach. The choice you make always depends on the context of what caused the outlier and how we expect similar things to happen going forward.\n\n\nReversal\nSometimes having more historical data is a bad thing, and can hurt model performance. For example having 30 years of historical data could produce a good forecast, but do the trends and patterns of the business 20 years ago still apply to the business today? Often in fast changing industries this is not the case, so sometimes deliberately shortening your data is the right idea. Five years from now we may want to exclude data from pre-2023 to remove all impacts of COVID. How your customers purchased your services in 2019 is most likely very different than how they will buy them in 2024. Gee, thanks COVID.\n\n\nAutomatic Data Cleaning with finnts\nThankfully there is a solution to most of these problems. My package, finnts, helps solve a lot of the data sanitizing needed to produce a high quality forecast. It can handle outliers and missing values automatically for you. It abstracts away all of these hard topics and makes it easy to get up and running with a forecast in one line of code. Check it out.\n\n\nFinal Thoughts\nMessy data will always lead to a messy forecast. ML models can’t save you from bad data. There’s no magic wand to cure common data problems. What you can do though is make sure your data has solid historical trends/seasonality, no missing data, and good approach to handling outliers. With these taken care of, you’re own your way to building a high quality forecast."
  },
  {
    "objectID": "posts/2024-04-12-weekend-reads/index.html#articles",
    "href": "posts/2024-04-12-weekend-reads/index.html#articles",
    "title": "Weekend Reads (4/12/24)",
    "section": "Articles",
    "text": "Articles\n\n17 Important Questions\nTime Series First Principles: Garbage In, Garbage Out\nTime Series First Principles: The Future Is Similar To The Past"
  },
  {
    "objectID": "posts/2024-04-12-weekend-reads/index.html#videos",
    "href": "posts/2024-04-12-weekend-reads/index.html#videos",
    "title": "Weekend Reads (4/12/24)",
    "section": "Videos",
    "text": "Videos\n\nTelling Good Stories\nPower of Being a Contrarian\nMaking Friends as an Adult"
  },
  {
    "objectID": "posts/2024-04-12-weekend-reads/index.html#podcasts",
    "href": "posts/2024-04-12-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (4/12/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nProtocols to Improve Your Sleep, Huberman Lab"
  },
  {
    "objectID": "posts/2024-04-12-weekend-reads/index.html#tweets",
    "href": "posts/2024-04-12-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (4/12/24)",
    "section": "Tweets",
    "text": "Tweets\n\nPower of Motivation\nOur Planet Rocks\nShipping on Fridays\nCool Idea Around Building the Pyramids"
  },
  {
    "objectID": "posts/2024-04-12-weekend-reads/index.html#books",
    "href": "posts/2024-04-12-weekend-reads/index.html#books",
    "title": "Weekend Reads (4/12/24)",
    "section": "Books",
    "text": "Books\n\nSapiens by Yuval Noah Harari"
  },
  {
    "objectID": "posts/2024-04-19-weekend-reads/index.html#articles",
    "href": "posts/2024-04-19-weekend-reads/index.html#articles",
    "title": "Weekend Reads (4/19/24)",
    "section": "Articles",
    "text": "Articles\n\nUse Strategic Thinking to Create the Life You Want\nHow Time Flies\nTime Series First Principles: Higher Grain, Higher Accuracy"
  },
  {
    "objectID": "posts/2024-04-19-weekend-reads/index.html#videos",
    "href": "posts/2024-04-19-weekend-reads/index.html#videos",
    "title": "Weekend Reads (4/19/24)",
    "section": "Videos",
    "text": "Videos\n\nDave Chappelle - Unforgiven\nBalaji on AI Gods\nBuilding Laser Focus\nOverrated and Underrated Habits\nLessons Learned on Writing"
  },
  {
    "objectID": "posts/2024-04-19-weekend-reads/index.html#sites",
    "href": "posts/2024-04-19-weekend-reads/index.html#sites",
    "title": "Weekend Reads (4/19/24)",
    "section": "Sites",
    "text": "Sites\n\nMake Music with AI"
  },
  {
    "objectID": "posts/2024-04-19-weekend-reads/index.html#tweets",
    "href": "posts/2024-04-19-weekend-reads/index.html#tweets",
    "title": "Weekend Reads (4/19/24)",
    "section": "Tweets",
    "text": "Tweets\n\nBuilding an Engineering Dream Team\nExperiences &gt; Things"
  },
  {
    "objectID": "posts/2024-05-01-time-series-features/index.html",
    "href": "posts/2024-05-01-time-series-features/index.html",
    "title": "Time Series First Principles: The Magic Is In The Feature Engineering",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the sixth principle of a good time series forecast, the magic is in the feature engineering. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nTurning Data Into Insight\nA machine learning (ML) model is only as good as the data it’s fed. The process of transforming data, to make it easier for a model to learn from that data, is called feature engineering. It’s a technical term that is actually very simple in nature, really just data transformations. In the world of time series forecasting, feature engineering can make or break a good forecast.\nCreating high quality features is a combination of strong domain expertise and data transformation skills. We have already covered how domain expertise impacts a forecast in a previous post, so this post will cover how simple data transformations can drastically improve the accuracy of a machine learning forecast. Check out each category of time series feature engineering below to learn more.\n\n\nDate Features\nThe most common type of feature engineering for time series is around dates. Date features allow us to capture seasonality patterns in our data. Think of seasonality as repeating peaks and valleys in our data. For example, our business might make most of its revenue in Q4 every year, with a subsequent dip in sales in Q1.\nLet’s use the example time series below to illustrate each type of feature engineering.\n\nFake Time Series Data\n\n\nDate\nSales ($)\nConsumer Sentiment\n\n\n\n\nJanuary 2023\n100,000\n68\n\n\nFebruary 2023\n110,000\n67\n\n\nMarch 2023\n120,000\n65\n\n\nApril 2023\n115,000\n70\n\n\nMay 2023\n130,000\n72\n\n\nJune 2023\n125,000\n73\n\n\nJuly 2023\n135,000\n74\n\n\nAugust 2023\n140,000\n75\n\n\nSeptember 2023\n130,000\n70\n\n\nOctober 2023\n145,000\n72\n\n\nNovember 2023\n150,000\n71\n\n\nDecember 2023\n160,000\n75\n\n\n\nIn this time series we would like to forecast monthly sales. We also have information about consumer sentiment that we can use to help forecast sales. A multivariate machine learning model cannot easily use the date column as is, so we have to do some data transformations (aka feature engineering) to make it easier for a model to understand how date information can help predict sales. Let’s go through a few examples of new features we can create from the date column. It’s important to note that after we create these new features it’s a good idea to remove the original date column before training a ML model.\nSince the data is monthly there are a lot of simple features we can use. We can pull out the specific month, quarter, and even year into their own columns to use as features. If our data was at a daily level, we can even go deeper and get features related to day of the week, day of year, week of month, etc.\n\n\n\nDate\nMonth\nQuarter\nYear\n\n\n\n\nJanuary 2023\nJanuary\nQ1\n2023\n\n\nFebruary 2023\nFebruary\nQ1\n2023\n\n\nMarch 2023\nMarch\nQ1\n2023\n\n\nApril 2023\nApril\nQ2\n2023\n\n\nMay 2023\nMay\nQ2\n2023\n\n\nJune 2023\nJune\nQ2\n2023\n\n\nJuly 2023\nJuly\nQ3\n2023\n\n\nAugust 2023\nAugust\nQ3\n2023\n\n\nSeptember 2023\nSeptember\nQ3\n2023\n\n\nOctober 2023\nOctober\nQ4\n2023\n\n\nNovember 2023\nNovember\nQ4\n2023\n\n\nDecember 2023\nDecember\nQ4\n2023\n\n\n\nThat seems pretty straight forward right? Let’s keep squeezing our date fruit for more juice and see what other kinds of features we can create. Since this is a time series, adding some order of time can be helpful. This can be something as simple as an index starting at 1 (or even convert your date to a seconds format). This helps establish the proper order of our data and makes is easier for a model to pick up growing or declining trends over time. There is also slight differences in how many days there are from month to month, so we can add that too. If you don’t think that’s important then you have never been stung by the harsh mistress that is leap year. There have been multiple times where finance exec’s have dismissed forecasts for the quarter that includes February, where in the end we didn’t account for the fact that it was a leap year or we are one year removed from one. You can even take this one step further and add the number of business days for each month.\n\nAdding a time index and other day related features\n\n\nDate\nIndex\nDays in Month\nBusiness Days\n\n\n\n\nJanuary 2023\n1\n31\n22\n\n\nFebruary 2023\n2\n28\n20\n\n\nMarch 2023\n3\n31\n23\n\n\nApril 2023\n4\n30\n20\n\n\nMay 2023\n5\n31\n23\n\n\nJune 2023\n6\n30\n22\n\n\nJuly 2023\n7\n31\n21\n\n\nAugust 2023\n8\n31\n23\n\n\nSeptember 2023\n9\n30\n21\n\n\nOctober 2023\n10\n31\n22\n\n\nNovember 2023\n11\n30\n22\n\n\nDecember 2023\n12\n31\n21\n\n\n\nTo get the final drop of juice out of the date column, we can also add Fourier series features. A Fourier series feature in time series forecasting is a component that captures seasonal patterns using sine and cosine functions to model periodic cycles in the data. In a nutshell they are just recurring peaks and valleys that can occur at various date grains like monthly or daily. These features can help capture more complex seasonality in your data. The chart below shows some standard Fourier series at the monthly and quarterly grain.\n\n\n\nLag Features\nTime series forecasting is all about learning from the past to forecast the future. In order to learn about the past we have to create lags on our data. Often what we’re trying to forecast today is correlated to what happened in the past. This is a concept known as autocorrelation. For our monthly forecast example, a 3 month lag may be highly correlated to sales with a 0 month lag (or sales today). Consumer sentiment can also be correlated with sales, but this time a lag of 6 might have higher correlation, since there is most likely a long delay between customer purchase patters and how it affects our company’s product. Lags can be created for any amount, depending on your domain knowledge of the business and results from more exploratory data analysis (deep dive for a different day).\n\nAdding lag features\n\n\n\n\n\n\n\n\n\nDate\nSales ($)\nConsumer Sentiment\nSales 3-Month Lag\nSentiment 6-Month Lag\n\n\n\n\nJanuary 2023\n100,000\n68\n\n\n\n\nFebruary 2023\n110,000\n67\n\n\n\n\nMarch 2023\n120,000\n65\n\n\n\n\nApril 2023\n115,000\n70\n100,000\n\n\n\nMay 2023\n130,000\n72\n110,000\n\n\n\nJune 2023\n125,000\n73\n120,000\n\n\n\nJuly 2023\n135,000\n74\n115,000\n68\n\n\nAugust 2023\n140,000\n75\n130,000\n67\n\n\nSeptember 2023\n130,000\n70\n125,000\n65\n\n\nOctober 2023\n145,000\n72\n135,000\n70\n\n\nNovember 2023\n150,000\n71\n140,000\n72\n\n\nDecember 2023\n160,000\n75\n130,000\n73\n\n\n\nLast thing I’ll say here is that you can also create leading features, especially for features that you know with 100% certainty ahead of time. For example, customers knowing of a new product launch in the future will definitely change how they purchase similar products you sell for the periods leading up to the launch. Someone may hold off on buying a new iPhone until the latest one gets released in a few months. Same goes for cars and many other products.\n\n\nRolling Window Features\nOften using pure historical lags is not enough. The historical data of our target variable (what we want to forecast) can be very noisy, making it hard for a model to learn the proper trends and seasonality. One way to handle this is through rolling window transformations.\nRolling window features in time series forecasting help smooth out data, reduce noise, and capture essential trends and cycles by averaging or computing other statistics over a specified period. For a monthly forecast we can create rolling window features of averages, min/max, and other statistical calculations.\n\n\n\nRolling Window Averages aka Moving Average\n\n\nIt’s best to calculate rolling window features based on your existing lag features. That way there is no data leakage during initial model training. See below for example of creating a 3 month rolling window average of the 3 month sales lag.\n\nRolling 3 month average applied to the 3 month sales lag\n\n\n\n\n\n\n\n\nDate\nSales ($)\nSales 3-Month Lag\n3-Month Rolling Avg\n\n\n\n\nJanuary 2023\n100,000\n\n\n\n\nFebruary 2023\n110,000\n\n\n\n\nMarch 2023\n120,000\n\n\n\n\nApril 2023\n115,000\n100,000\n\n\n\nMay 2023\n130,000\n110,000\n\n\n\nJune 2023\n125,000\n120,000\n110,000\n\n\nJuly 2023\n135,000\n115,000\n115,000\n\n\nAugust 2023\n140,000\n130,000\n121,667\n\n\nSeptember 2023\n130,000\n125,000\n123,333\n\n\nOctober 2023\n145,000\n135,000\n130,000\n\n\nNovember 2023\n150,000\n140,000\n133,333\n\n\nDecember 2023\n160,000\n130,000\n135,000\n\n\n\n\n\nPolynomial Features\nThe final type of feature engineering I’d like to discuss are polynomial transformations. Sometimes there is a non-linear relationship between your initial feature and the target variable. Some models, like ones that use decision trees, can handle this kind of relationship while others like linear regression cannot. To fix this we can transform the data via polynomials like squaring, cubing, and even taking the log of the initial feature.\nLet’s take our example monthly sales data and add some spice to it. This time creating an exponential relationship between consumer sentiment and sales.\n\nUpdated sales data with an exponential relationship with consumer sentiment\n\n\nDate\nSales ($)\nConsumer Sentiment\n\n\n\n\nJanuary 2023\n1,309,000\n68\n\n\nFebruary 2023\n1,204,000\n67\n\n\nMarch 2023\n1,000,000\n65\n\n\nApril 2023\n1,525,000\n70\n\n\nMay 2023\n1,849,000\n72\n\n\nJune 2023\n1,964,000\n73\n\n\nJuly 2023\n2,121,000\n74\n\n\nAugust 2023\n2,500,000\n75\n\n\nSeptember 2023\n1,525,000\n70\n\n\nOctober 2023\n1,849,000\n72\n\n\nNovember 2023\n1,764,000\n71\n\n\nDecember 2023\n2,500,000\n75\n\n\nJanuary 2024\n2,890,000\n76\n\n\nFebruary 2024\n3,361,000\n78\n\n\nMarch 2024\n3,844,000\n79\n\n\nApril 2024\n4,641,000\n81\n\n\n\nWhen graphing the data, see how the increase in consumer sentiment has an exponential effect on sales?\n\nTo account for this, we can square the values of consumer sentiment and create a new feature to use. This new feature will make it easier for models like linear regression to capture these kinds of non-linear relationships.\n\nNew polynomial feature added\n\n\n\n\n\n\n\n\nDate\nSales ($)\nConsumer Sentiment\nConsumer Sentiment Squared\n\n\n\n\nJanuary 2023\n1,309,000\n68\n4,624\n\n\nFebruary 2023\n1,204,000\n67\n4,489\n\n\nMarch 2023\n1,000,000\n65\n4,225\n\n\nApril 2023\n1,525,000\n70\n4,900\n\n\nMay 2023\n1,849,000\n72\n5,184\n\n\nJune 2023\n1,964,000\n73\n5,329\n\n\nJuly 2023\n2,121,000\n74\n5,476\n\n\nAugust 2023\n2,500,000\n75\n5,625\n\n\nSeptember 2023\n1,525,000\n70\n4,900\n\n\nOctober 2023\n1,849,000\n72\n5,184\n\n\nNovember 2023\n1,764,000\n71\n5,041\n\n\nDecember 2023\n2,500,000\n75\n5,625\n\n\nJanuary 2024\n2,890,000\n76\n5,776\n\n\nFebruary 2024\n3,361,000\n78\n6,084\n\n\nMarch 2024\n3,844,000\n79\n6,241\n\n\nApril 2024\n4,641,000\n81\n6,561\n\n\n\n\n\nReversal\nSometimes too much of a good thing can be a bad thing. Adding a lot of new features can increase the chance that a model overfits. Overfitting in machine learning occurs when a model learns to capture noise or random fluctuations in the training data, leading to poor generalization and high performance on training data but low performance on unseen data. The best way to prevent this kind of overfitting is to limit the number of features used to train a model. This will be discussed in greater detail in another post in this series.\nDid you notice that when creating lags and rolling window features we had a lot of missing data at the start of the time series for those new features? This can be a problem. Some ML models do not like missing data, so we need to deal with those missing values. An easy way is to just drop the initial rows in the time series that have blank values for the new lags and rolling window features. This can work well if you have a lot of historical data. Dropping data can hurt model performance though, and if you don’t have a lot of data to start with it becomes a less favorable option. You could also replace the missing values, either by using a simple model to impute the value or just use the closest available value in the time series to “fill in” the missing values. Both of these missing value replacement approaches have their own pros and cons but could be a better strategy then just simply dropping rows with missing values.\n\nFilling in missing values with their closest available value\n\n\n\n\n\n\n\n\nDate\nSales ($)\nSales 3-Month Lag\n3-Month Rolling Avg\n\n\n\n\nJanuary 2023\n100,000\n100,000\n110,000\n\n\nFebruary 2023\n110,000\n100,000\n110,000\n\n\nMarch 2023\n120,000\n100,000\n110,000\n\n\nApril 2023\n115,000\n100,000\n110,000\n\n\nMay 2023\n130,000\n110,000\n110,000\n\n\nJune 2023\n125,000\n120,000\n110,000\n\n\nJuly 2023\n135,000\n115,000\n115,000\n\n\nAugust 2023\n140,000\n130,000\n121,667\n\n\nSeptember 2023\n130,000\n125,000\n123,333\n\n\nOctober 2023\n145,000\n135,000\n130,000\n\n\nNovember 2023\n150,000\n140,000\n133,333\n\n\nDecember 2023\n160,000\n130,000\n135,000\n\n\n\n\n\nOther Pre-Processing\nOne thing I wanted to add that technically isn’t considered feature engineering are other data pre-processing methods. These are things you apply before you start your feature engineering process. They are specific to time series forecasting and can greatly improve forecast accuracy. Here are two pre-processing methods you should know about.\nFirst is making your data stationary. This is a time series technical term that pretty much means removing the trend component of your data, where the time series has a constant mean and standard deviation. We can make a time series stationary by the process of differencing. This involves taking the difference between each date observation and using that as the new time series to train models with. Check out the example below. See how the upward trend gets removed when we simply use the difference between months instead of the original monthly values? Some machine learning models, like ones that rely on decision trees, cannot extrapolate trends. So differencing the data removes any trend pattern, making it a lot easier for these models to produce high quality forecasts.\n\nAnother pre-processing technique is a box-cox transformation. This helps remove any exponentially increasing trends by applying various types of power transformations. For example, taking the log of your time series. Removing non-linear trends can make it a lot easier for a model to create accurate forecasts. See the example below of a time series with a non-linear trend. We can then apply a box-cox transformation and then difference the data. See how nice the final time series looks? It will be way easier for a ML model to learn the patterns in the final transformed time series.\n\n\n\nfinnts\nThere’s a lot to unpack on feature engineering for time series forecasting. Thankfully my package, finnts, can automatically handle all of the feature engineering for you. It does everything I called out in this post plus more. Check it out and see just how easy ML forecasting can be.\n\n\nFinal Thoughts\nFeature engineering is the backbone of successful time series forecasting, allowing models to uncover hidden patterns and relationships within the data, ultimately leading to more accurate predictions. By transforming raw data into meaningful features like date-related attributes, lag features, rolling window statistics, and polynomial transformations, we equip machine learning models with the necessary insights to make informed forecasts. However, it’s crucial to strike a balance between adding informative features and avoiding overfitting, as too many features can lead to poor generalization on unseen data. With careful consideration and the right techniques, feature engineering becomes a powerful tool in the arsenal of any data scientist or analyst aiming to unlock the predictive potential of time series data."
  },
  {
    "objectID": "posts/2024-05-07-time-series-capture-uncertainty/index.html",
    "href": "posts/2024-05-07-time-series-capture-uncertainty/index.html",
    "title": "Time Series First Principles: Capture Uncertainty",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the eighth principle of a good time series forecast, capture uncertainty. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nBuilding Trust\nWould you give your retirement savings to a hedge fund manager because they asked nicely? Probably not. Instead, you would like to do your research about them. Ask them how well they performed in the market historically, and also see how they expect the future markets to unravel in the near term. If their answer to those questions are, “I don’t have a historical track record” and “I have no clue what the future holds” then you are probably not going to give them one penny of your hard earned money. The same holds true for using a time series forecast created by machine learning (ML) models. In order to build trust with the end user of the forecast, you need to show them how a similar forecast would have performed historically and also quantify some aspect about the future. Let’s dive into each one.\n\n\nPast Uncertainty\nBefore a ML model can be used to forecast the future, we need to see how it has handled the past. This is called back testing, where we see how a model performed historically. This can give us a good proxy around how it could perform in the future.\nBack testing at its core is all about training a model on a portion of your historical data set (training data), then using the trained model on another portion of the historical data (testing data). This can be as simple as using the first 80% of your historical data to train a model, and use the last 20% for testing. Check out a previous post to learn more about why the order of that train/test split is important.\nThere are also more advanced methods of doing this, like time series cross-validation. This involves many rounds of training a model and then creating a prediction on the testing data. Time series cross-validation can be used to tune model hyperparameters (inputs a model cannot learn from but must be given by a human) but is especially useful for model back testing. Check out the chart below that shows how we can effectively back test using a time series cross-validation approach. Each pass has its own train and test split, and the testing splits can overlap from one pass to another.\n\n\n\nSource: Uber Engineering\n\n\nIn order to capture how accurate the back testing is, we need to calculate a metric that summarizes the model’s performance on the testing data splits. There are countless metrics we can use, each with their own pros and cons. That kind of discussion is out of scope for this post but let’s highlight a few common ones you could use in determining how accurate a model is during back testing.\n\nMean Absolute Error (MAE)\n\nDescription: MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It’s calculated as the average of the absolute differences between forecasts and actual observations.\nStrengths: MAE is straightforward and easy to interpret as it directly represents average error magnitude.\nWeaknesses: MAE treats all errors with the same weight, thus large errors have the same influence as small ones, which might not be optimal for all applications.\n\nRoot Mean Squared Error (RMSE)\n\nDescription: RMSE is the square root of the mean of the squared errors. It measures the average magnitude of the error, with the squaring giving higher weight to larger errors.\nStrengths: RMSE is sensitive to outliers and provides a measure of how large errors are when they occur, which can be crucial for many practical applications.\nWeaknesses: Like MSE, RMSE can be heavily influenced by outliers and large errors, possibly leading to overestimations of the typical error if the error distribution is skewed.\n\nMean Absolute Percentage Error (MAPE)\n\nDescription: MAPE expresses accuracy as a percentage, and it measures the size of the error in percentage terms. It is calculated as the average of the absolute errors divided by the actual values, expressed as a percentage.\nStrengths: MAPE is scale-independent and provides a clear interpretation in terms of percentage errors, making it easy to communicate.\nWeaknesses: MAPE can be highly skewed when dealing with values close to zero, and it disproportionately penalizes underestimations compared to overestimations.\n\n\n\n\nFuture Uncertainty\nNow that we’ve quantified how well our model works historically, we can just give the future forecast to our end user right? Not so fast. Our model might say that next month our company’s product will make $100, but if that’s all the info we provide to the end user of that forecast that’s not a good way to build trust. Instead we need to show how confident we are in that $100 forecast. How likely are we to hit that number? That’s where prediction intervals come in.\nPrediction intervals help quantify the future uncertainty in our model’s forecast. They are statistical ranges, typically based on the forecast error, used to indicate the likelihood that the future value of a time series will fall within a specified range at a certain confidence level. Common ranges for a prediction interval are 80% and 95%. For example, the future forecast may be $100 but have a 95% prediction interval of $75 and $125. This means that there is a 95% likelihood that the future value will fall between $75 and $125. The tighter the range, the less uncertainty there is in the forecast. Below is an example forecast with 80% and 95% prediction intervals.\n\n\n\nReversal\nThe back testing process can only ever be a proxy of what kind of results to expect on the future forecast. It follows the assumption that the future will be similar to the past. Sometimes this is not the case, and future results may be worse than historical back testing performance.\nWhile prediction intervals help quantify uncertainty, they also do not do a perfect job. There may be times where the future forecast will fall outside of the ranges. It’s not the end of the world when it does, but instead shows that the future is often different than what happened in the past. This is where strong domain knowledge comes in to understand what’s truly an outlier and what’s a new fundamental factor in your business going forward. For example, a new product launch in the future is hard to quantify with a prediction interval, but once it happens we can learn from that information and try to capture it the next time we train our model.\n\n\nfinnts\nBack testing and prediction intervals is tough work. Thankfully my forecasting package, finnts, takes care of both of these for you. You can even customize the back testing process to fit your needs. Check out the package and see just how easy forecasting can be.\n\n\nFinal Thoughts\nCapturing uncertainty in time series forecasting is essential for creating robust forecasts that stakeholders can rely on. Utilizing back testing and prediction intervals not only strengthens the credibility of forecasts but also provides users with a clearer perspective on potential risks and variations. In the end these approaches help build trust with the forecast end user. The more trust we can build, the more likely the ML forecast will be used."
  },
  {
    "objectID": "posts/2024-05-28-time-series-model-avg/index.html",
    "href": "posts/2024-05-28-time-series-model-avg/index.html",
    "title": "Time Series First Principles: Model Combinations Are King",
    "section": "",
    "text": "Time Series First Principles Series\nThis post dives into the ninth principle of a good time series forecast, model combinations are king. Check out the initial post in this series to get a high level view of each principle.\n\nDomain Expertise\nGarbage In Garbage Out\nThe Future Is Similar To The Past\nHigher Grain Higher Accuracy\nOrder Is Important\nThe Magic Is In The Feature Engineering\nSimple Models Are Better Models\nCapture Uncertainty\nModel Combinations Are King\nDeep Learning Last\n\n\n\nWisdom of the Crowds\nIn 1906, famed statistician Francis Galton went to a county fair for some fun. While there he came upon a competition to guess the weight of an ox. Eight hundred people entered the competition but the guesses were all over the place, some too high, some too low. Francis was a big numbers guy, so he took all of the guesses home with him and crunched the data. He found out that the average of all the guesses was only one pound away from the actual weight of the ox, which weighed 1,198 pounds. That’s an error of less than 0.08%. What he stumbled upon that day is now know as the wisdom of the crowds.\nThe concept of wisdom of the crowds states that the collective wisdom of a group of individuals is usually more accurate than that of a single expert. When guessing the weight of the ox, the overestimates and underestimates of regular people cancelled each other out. Creating an average prediction that was more accurate and any single person’s estimate.\nThis principle is important in machine learning forecasting. Usually it’s not one single model that performs the best, but instead a combination of multiple models. Let’s take a look at how we can combine models into more accurate forecasts.\n\n\nTypes of Model Combinations\nThere are many different ways individual model forecasts can be combined to create more accurate forecasts. For today we’ll cover the most common approaches. If you’d like to dive deeper I recommend this amazing paper by our forecasting Godfather Rob Hyndman.\n\nSimple Average: As simple as it sounds. Just take the forecasts from individual models and average them together.\nEnsemble Models: Feed the individual model forecasts as features into a machine learning model, and have the model come up with the correct weighted combination. This is also known as “model stacking”.\nHierarchical Reconciliation: This involves forecasting at different aggregations of the data set based on its inherent hierarchies, then reconciling the down to the lowest level (bottoms up) using a statistical process. For example forecasting by city, country, continent, and global level then reconciling each forecast down to the city level. This reconciliation can be thought as combining different forecasts together to create something more accurate. This approach has more nuances, and will be covered in another post.\n\n\n\nModel Combination Example\nLet’s walk through a simple example around how combining the predictions of more than one model can outperform any single model. Below is an example monthly time series. We will try to back test the last 12 months of the historical data.\n\nTo keep things simple we can just run a few models to get the back testing results for the last year of the data. We’ll use various univariate time series models. Ignore the types of models used. Instead, let’s just see how each model did on it’s own. Learn more about accuracy metrics in a previous post.\n\n\nAccuracy by Single Model\n\n\nModel\nMAPE\nMAE\nRMSE\n\n\n\n\narima\n1.97\n3.76\n4.68\n\n\ncroston\n10.18\n19.54\n20.01\n\n\nnnetar\n9.77\n18.37\n26.00\n\n\nstlm-ets\n1.92\n3.68\n4.59\n\n\ntbats\n1.86\n3.51\n4.05\n\n\ntheta\n2.46\n4.71\n5.52\n\n\n\nIt looks like the tbats model performs the best across the board with stlm-ets and arima not far behind. What if we averaged the three of them together? Let’s see how the results change.\n\n\nAccuracy for Average Model\n\n\nModel\nMAPE\nMAE\nRMSE\n\n\n\n\narima_stlm-ets_tbats\n1.84\n3.51\n3.99\n\n\n\nEven better results! See how creating simple model averages can improve the results? Averaging the results can help smooth out any under or over forecasts, creating more accurate models.\nSimple model averages are often the quickest way to improved forecast accuracy. Another way is to create an ensemble model that can create the weights on its own. Let’s feed the predictions from each model into a linear regression model and have it determine the optimal weights.\n\n\nAccuracy for Ensemble Model\n\n\nModel\nMAPE\nMAE\nRMSE\n\n\n\n\nEnsemble\n1.81\n3.40\n3.65\n\n\n\nAlight more accurate results! By feeding each individual model forecast into a final ensemble model, we were able to get a more accurate forecast.\n\n\nReversal\nWhen trying to combine models, there is always a risk of overfitting. Meaning the combination approach (like simple average or ensemble) could have great accuracy on the back test data but not generalize well to new unseen data in our future forecast. To prevent that we can make sure to back test on enough historical data to prove our combination approach works well for more than just a period or two. We can also have separate validation and test splits in the back testing to see how combinations made on one data set can generalize well when tested on the other.\nPrediction intervals are harder to create. Simply combining the 80% and 95% prediction intervals of multiple models together is not going to fully capture the uncertainty of forecasts created by the new model combination. So we would need to re-create the intervals based on the results of the new combined model.\nSimilar to prediction intervals, combining models can also make it harder to interpret them. Instead of just understanding one model and its predictions, we now have to understand how multiple models work and are combined to get the final forecast.\n\n\nfinnts\nModel combinations can be hard to do effectively. Thankfully my forecasting package, finnts, is here to help! It automatically handles every kind of model combination method listed in this post. Check out the package and see just how easy forecasting can be.\n\n\nFinal Thoughts\nJust like the county fair crowd nailed the ox’s weight, combining multiple models in time series forecasting yields more accurate predictions by balancing out individual errors. When you’re forecasting, remember to embrace the collective wisdom of models for better results!"
  },
  {
    "objectID": "posts/2024-06-04-how-to-master-storytelling/index.html",
    "href": "posts/2024-06-04-how-to-master-storytelling/index.html",
    "title": "How To Master Storytelling",
    "section": "",
    "text": "Our dumb caveman brains can’t remember everything. There is too much information passing through our heads each day. The only thing that sticks are stories. Either ones we tell ourselves or ones we hear from others. Don’t believe me? Let’s play a game. Tell me what you had for lunch last week on Tuesday. Now tell me the plot to the first Star War’s movie. Ha gotcha. Stories stick. More than anything else in this world. Whoever can tell the best stories has the power to do great things in our world.\nShaan Puri is an entrepreneur and content creator. Known for his work in the tech industry and his popular podcast “My First Million.” A few months ago he was on the popular “How I Write” podcast. While on the show he blew my mind with tons of great ideas around storytelling. I learned a lot and wanted to share the best ideas here.\n\nAaron Sorkin’s 30 Second Masterclass\n\n“I worship at the alter of intention and obstacle.”\n— Aaron Sorkin\n\nAaron Sorkin is a famous screenwriter. Creating hits like the TV show “The West Wing” or movies like “The Social Network”. His says that every story needs to have a clear intention and obstacle.\nIn any story, the main character has a have a clear intention. What do they want? Why do they want it? After that you need an obstacle. Who or what is trying to stop them from getting their intention? It’s fundamental to every story, but doesn’t have to be life and death. Every movie you have ever seen has this. If it didn’t have a strong intention or obstacle in the first 10 minutes, you probably hated the movie. For example, all Harry Potter wants is to live a normal life with a loving family and close friends. Since the day he was a born a dark wizard is trying to kill him. Intention (living), and obstacle (trying to kill him).\n\n\nHooks vs Frames\nEveryone has seen Twitter/X threads that start with “The unbelievable story of XYZ person doing XYZ thing”. It’s clickbait and makes me cringe every time I see it. These are tactics that try to hook a reader into continuing to engage in the content. Instead of creating hooks, Shaan recommends creating the right frame for a story. Hooks are about the words you’re going to write. Frames are about the idea. And how you’re going to connect many ideas together to make it relevant to an audience.\nCheck out the following tweets about the popular audio app Clubhouse.\n\nThis first tweet is by someone who founded a similar audio app before Clubhouse. He should be very knowledgeable on the subject. The tweet gained little traction because he told no story. It was dry and full of technical industry facts and jargon.\n\nNow take a similar tweet by Shaan. In his tweet he told a story. He put in the frame of “every one thinks X, but I think Y, and here’s how I think it’s going to go down”. It had millions of views. The thread is a mini screenplay and masterclass on how the right frame can make an idea turn into a powerful story.\nAnother powerful example is Dave Chappelle trying to get the rights back to his famous sketch show. He didn’t complain to Viacom (owners of Comedy Central which initially hosted the show). Instead he told a story with a powerful framing. It wasn’t a funny story. But one that made his fans start boycotting the show on streaming platforms. Hoping to get paid for his original hard work. This was all due to a powerful story with the right framing. Check out this video to hear the full story.\n\n\nTell 100 Stories\nIf you don’t know who Mr. Beast is than you live under a rock with no internet connection. People always ask how they can be like him. His advice is simple. Make 100 videos. Each time do one thing better than the last video. He says it’s the perfect advice. Because either no one actually goes through with making 100 videos. Or the people that do never reach out to him again because once they make 100 videos they figured out how to make them successful.\nTo tell great stories you need to tell hundreds of stories, maybe thousands. You need to get intelligent reps in. With each rep you get a little better. These gains can compound a powerful skill over time.\n\n\nMake A Story Go Viral\nThere are companies out in the world whose sole purpose is to make content go viral. Now that’s a wild job. They know the only way something goes viral is how many people share the content. People will normally share content if it creates an emotional reaction. Shaan calls these emoji reactions. Things like 🤣😍😎🤔😮🤬. When the company is creating a piece of content, they start with the emotion they want the content to create. Then they work back from that emotion to create the content. The north star is always the emotion the want the audience to feel. If their first draft doesn’t create that emotion, they will adjust until they get it right.\n\n\nStoryworthy Book\nMatthew Dicks wrote the book on storytelling. No he actually did. He wrote a book called “Storyworthy”. I’m currently reading it and love the book, future post coming soon. Shaan calls out a few kew ideas from the book in the podcast.\nFirst is every story needs stakes. You need to make clear what’s at stake if the main character doesn’t get what they want (their intention). These stakes don’t have to be life or death though. Stakes come from the emotion. Something closer to regular life has the biggest impact because other people can see themselves in the story. Telling a heartwarming story about learning life lessions from your kids at the dinner table can be more powerful than a shark attack story.\nNo one cares about your vacation or crazy party story. If you have something that’s generally interesting about your vacation, then only keep the parts that are relevant to the story. If you got pickpocketed at the Vatican, but were able to chase down the bad guy and get your wallet back, then just tell that part of the story. It may not matter that you were on the Amolfi coast the previous day or you were even at the Vatican. No one cares. Just keep your story as long as it is interesting. Not a second longer.\nAt its core, a story is a five second moment of change. Everything comes up to one moment where a thing or person is transformed. If the story doesn’t have change, then it’s just an anecdote. A sequence of events. Not a real story. Stories start with the world one way, and end with it another way. Shaan says a good example is every romantic comedy you’ve ever seen. Whatever the main character is, they are 100% going to be the opposite at the end of the movie. Is the woman a fast paced lawyer who never made time for love? By the end of the movie she will have a romantic partner and will take more time away from work. Is the guy a ladies man who will never settle down? By the end of the movie he will fall for someone who will make him rethink everything and start hearing wedding bells. It’s always about change.\n\n\nAdding Humor in Storytelling\nHumor is the sauce but not the meal in most stories. All humor is just surprise. If you see the punch line coming it’s not very funny. Try to add humor into your storytelling. Just don’t make it the whole story. Leave that up to the comedians.\n\n\nBinge Bank\nIf someone wanted to learn more about you, what would they do? Maybe they’re a recruiter trying to offer you the perfect job. Maybe they’re an entrepreneur trying to find their next co-founder. Or maybe they’re a cute girl you’re about to have dinner with. Chances are the only things they can find on you is your social media presence, and hopefully not any mugshots.\nShaan recommends creating a binge bank for yourself. Think of it as a bank of content that someone can go down the rabbit hole on you. Blogs, videos, newsclippings. Hopefully a collection of stories. Whatever content that gives someone all access pass to how your brain works and who you are as a person. By the end a person’s opinion of you should drastically change. This binge bank can become more powerful than a resume, because it shows your true self. While also showcasing how you communicate.\nAfter hearing this I started to curate my own binge bank. I added a Start Here section to my personal site. On this page I have links to posts that best describe me and my capabilities. Right now it’s small but one day I plan to grow it to the binge banks of people like Ryan Holiday, Tim Ferriss, or Mark Manson.\n\n\nGrowing An Audience\nShaan has a friend with a cool rule about being interesting. If you tell someone something interesting, they will say “wow that’s interesting”. If you tell someone two interesting things, the will say “those are interesting”. If you tell someone three interesting things, they will say “ok, now you are interesting”. To build a following on the internet or at your job, you need to demonstrate your insight constantly. Instead of doing this three times you may have to do this 100 times to get people to come back and pay attention.\nAnother powerful point that Shaan learned from a branding expert is people will follow you to the ends of the earth if you can give them a feeling more consistently than anyone else. Why do people listen to podcasts from comedians? It’s not because they have groundbreaking insight about current events or they perform their standup routine. It’s because when people listen they feel like “one of the guys” who like to hang out with their friends and crack jokes at one another. How people feel after consuming your story is the end goal.\n\n\nClosing Thoughts\nStorytelling is more art than science. It takes reps to get it right. If you can tell good stories, you can do almost anything in this world. Get out there and start telling better stories today. Just not ones about your vacation."
  },
  {
    "objectID": "posts/2024-06-26-msft-ml-fcst-journey-2/index.html",
    "href": "posts/2024-06-26-msft-ml-fcst-journey-2/index.html",
    "title": "Microsoft Finance ML Forecasting Journey: Part Two",
    "section": "",
    "text": "This is a multipart series:\n\nPart One\nPart Two\n\nPart Three\n\nThe success of Microsoft finance’s first machine learning (ML) forecast spread like wildfire throughout finance. The ML forecast was shared with all finance leaders. So naturally knowledge of ML’s potential trickled down to more people across the organization. Eventually the news came to a team in central finance (CFT). Think of this team as Microsoft’s core FP&A team across the entire company. After seeing the accuracy at a worldwide level, this team knew it could help in the biggest forecast process at Microsoft. Something called the commercial field forecast. This forecast is created by finance members who sit in the “field”. The field is just a cool way to say regional offices all around the world. These field finance teams support the sales teams who also sit in the field. How could they take a worldwide forecast by product and break it down into specific countries all across the world? Well buckle up gang, it’s time to find out! This is how a tool called “Commercial Predict” was born in 2017.\n\nHow Things Used to Work\nBefore we dive into all the ML goodness, we have to understand how the old way used to work. I know, it’s kind of like eating your vegetables. But we just have to do it real fast then we can get to the fun parts.\nIn the past, each finance team in the field was responsible for their own forecast each quarter. These forecasts would happen in “CFO forecast cycles”. With cycles happening in October, January, and April. Microsoft’s fiscal year runs from July - June so these forecast cycles happen at the start of Q2, Q3, and Q4. The forecast at the start of Q1 is budget (that’s a story for a different day). Each cycle, a forecast would be created for the remainder of the fiscal year.\nMicrosoft sells products in over 100 countries. Most of those countries have a sales team that tries to sell products to companies in that geographical region. If there’s a sales team, then there is a finance team who supports them. This means there are dozens of sales finance teams creating quarterly forecasts for the rest of the fiscal year each CFO forecast cycle. Each team had their own secret recipe of how the forecasting was done. Often a custom excel model that would create the forecast. This model needed to be handled with care. Since each quarter it would have to be rolled over and prepared for the next forecast cycle. Anyone who has ever created and owned a financial model in excel knows the anxiety faced with trying to build and maintain one. These models were complex, and you said a little prayer every time you opened the file. Hoping it wouldn’t crash your machine because it was so large.\nOnce each team in the field had their forecast for their geography, it would get sent up the food chain. Forecasts from each country would be combined to form higher level aggregations in Microsoft’s sales territories. Each aggregation added more countries and continents together. This continued until you got the total worldwide number for the entire commercial business. Each time the forecasts got combined together at a higher level, senior finance leaders had the opportunity to make adjustments to that forecast. Based on their domain knowledge of the business. Eventually the final forecast the CFO, Amy Hood, saw was something completely different than what was initially created by each sales finance team for their specific geography.\nLayers upon layers of bias were added to the forecast. Some was good bias that could improve forecast accuracy, but often it was too many cooks in the forecast kitchen. Too many people touching a forecast that didn’t need to be touched. Resulting in worse accuracy and more confusion once the books were closed at quarter end. This process would take upwards of a month every quarter. From the initial forecast created by field team all the way up the food chain to the CFO. In the spirit of every good infomercial, “there just has to be a better way!”.\nNow you know why finance had to do something different. Drastically different.\n\n\nExcel Prototype Built in a Redmond Garage\nAll good things start from humble beginnings. The team in CFT wanted to centralize the field forecast process for the commercial business. Create a single way that everyone in the field would follow to create a forecast. To make this a reality, they started with the swiss army knife of every finance professional. That’s right. You guessed it. They started with excel. Like any innovative project, it quickly became their baby. And all babies need a name. The named it Commercial Predict.\nThe team created an excel prototype of a single model that every field finance team could use. It was a combination of the old and the new. First was old but reliable PxQ forecasts. Where you take what’s in the sales pipeline for a quarter and multiply it by how many deals on average have closed in similar historical quarters. Second was classic CAGR and year over year percentage growths, which actually still work quite well. These traditional methods were combined with more statistical rigor. Something more along the lines of machine learning. They built by hand, formula by formula, exponential smoothing statistical models. Which is a common model in time series forecasting. It’s more stats than machine learning, but still performed really well. Today exponential smoothing is a simple function call in excel, but this team built it from scratch. I tip my cap to them, because that was hard to do.\nNow there were multiple forecasting methods in this mega excel model. The beauty of the idea is that someone could come into the model and choose what methods they wanted to use to forecast a specific geography, customer segments, and products. Users could even combine multiple methods together to get a more accurate forecast. This was powerful because someone could use the PxQ sales pipeline method for products that depended on big customer deals landing, and use the other methods for things that had more stable trends and seasonality.\nIt was genius prototype. The team was able to take this to their leadership team and show how one single approach to do forecasting in the field could save thousands of days of combined human effort across the field every year. One mega model to rule them all. It had the promise to cut forecasting down by 50%. Would it work though? To test it out, the team ran this excel model alongside the traditional bottoms up forecast process from each field team. They could then compare the results and even track accuracy across the old and new ways. The results were good. The new prototype was the same or even better than the existing process, but was 50% faster.\nThe new approach was fast, and it was accurate. The final roadblock before adopting the new approach was to get everyone in the field to agree on what level they should forecast at. This historically was a difficult subject to discuss, with everyone having differing opinions. Thankfully this was solved by getting the buy in from the top senior finance leaders in central finance and the field. Once that happened everyone was able to get on board.\nOk, so the team had a cool prototype that knew worked well. But if they just used that excel model, then they are still maintaining a model that is messy and requires constant upkeep. It would be hard to scale. They needed something more robust. A real tool that was built by engineers. Thankfully there was a team who could do just that.\n\n\nBuilding the Tool\nThe vendor team who was created to take over the initial ML forecasts was the up to the task. They had the data science knowledge, but the data engineering and software engineering needed to build a software tool to scale out the excel prototype was missing. So the team got other vendors to fill in those gaps. Now there was a team of engineers all capable of making the tool a reality.\nThe first version of the production level Commercial Predict tool had to be built fast, before an upcoming CFO forecast cycle. V1 was built into excel within six months as an add-in field users could download and connect to. It needed to combine new machine learning methods with traditional PxQ and CAGR/YoY run rate methods.\nHere’s how it worked.\n\nData engineers would pull historical revenue and sales pipeline data. All the forecasts methods were precomputed and saved in a database that was turned into a cube. This would be the starting point for all field members. Instead of calculating these forecasts by hand in their old excel models, it would be precomputed for them. At scale.\nThe forecasts in the cube were then served to users in a custom excel file. Each field team could come into the tool and select the geography, products, and segments they were responsible for forecasting. After making these selections, all the forecast methods would populate in the excel.\nUsers could then see each forecast method and see which ones pass their smell test of what they expect to happen in the business based on their domain knowledge. They could choose a specific forecast method to use, or combine multiple methods together to get a more robust forecast. Finally there might be things these forecast methods don’t know about. Like upcoming tax changes or product strategy changes. Field users could ultimately make manually adjustments to get a final forecast.\nOnce the final forecast was created, they could save it back to the cube. This allowed finance leaders to see the forecast creation in real time. Also it would prevent the classic “excel crash without saving” headache we’ve all been through in the past.\nOnce the forecast was complete for each field team, a static output file was created at the touch of the button. Teams could take this output and load it into the final planning system.\n\nBefore official launch, training sessions were held to make sure everyone knew how to use it. It was also a good opportunity to fix any bugs in the tool. This resulted in some late nights and even weekend shifts, but the job got done. The tool was launched on time and the rest is history.\nWe were able to go from a forecast process of 21 business days each quarter, down to just 10. It was a revolution. This saved Microsoft millions of dollars each year of human capital. Finance teams in the field could now forecast faster, with less headaches, and prevent the layering of bias that was a staple of the previous way.\n\n\nEvolutions\nAfter this officially launched in 2018, the Commercial Predict tool has gone through a lot of iterations. What started in excel then moved into a web based tool. Then back to excel. With each iteration, we got better at the machine learning methods. Better at adding more features to give users more control over the final forecast.\nEventually Commercial Predict evolved into a much broader solution called “Fusion” in 2022. Think of it as a tool that could still do the commercial field forecast process but now also take on other forecasts within Microsoft finance. A true one stop shop for all things planning. Fusion is an excel add-in with a built in UI on the side of excel. Kind of like how excel copilot opens on the side of your excel tab, Fusion does the same. A user could select what forecast they want to do, select the parts they’re responsible for forecasting, and Fusion would populate the blank excel file with all the information they need to finalize their forecast. Methods like PxQ and machine learning are still ran ahead of time. The UI was truly dynamic. You could take any excel file and open the Fusion app inside to get going on the forecast. Fusion allowed finance to scale the learnings of Commercial Predict to so many other forecast processes. Improving the impact ML and centralization can have on forecasting.\nPlanning tools like Fusion will most definitely change in the future. As the business evolves, so should our way of forecasting it. What doesn’t change is how ML has become a central part of the forecast process.\n\n\nLessons Learned\n\nIterate Iterate Iterate\nRome wasn’t built in a day. Instead of building this complex centralized forecasting tool from the start, we started small. Built a prototype. Got senior leadership buy in. And continued to make it better every 6-12 months. Even as I write this we are in the process of improving the ML accuracy of the commercial field forecast. If you’re coasting, you’re going downhill. You need to continue to iterate.\n\n\nCombine the Old with the New\nML didn’t outright replace every part of the commercial field forecast. Instead we combined ML techniques with older methods like PxQ sales pipeline methods. This allowed us to use the strengths of each approach based on what product was being forecasted. Some products are sensitive to large customer deals closing, so PxQ works best. Others have stable trends and seasonality, that’s where ML shines. Using both gives us the best of both worlds.\n\n\nSenior Leadership Buy In\nA forecast process is like a ship. The bigger the ship, the harder it is to change course. So the bigger the forecast process, the higher the buy in needed from a senior leader. Getting a GM or CVP level support allowed us to supercharge the change management. It’s easy to get bogged down in arguing with senior finance managers about how a forecast process should be done. Once a CVP (someone who reports to the CFO) comes in and says this is how we’re going to do it. Then everyone gets on board and starts turning the wheel of the ship together to change direction. The commercial field forecast had to get support from GM and CVP level leaders or else it would have taken years to change it instead of months.\n\n\n\nFinal Thoughts\nThe hardest part of any ML project comes down to people. Training models is easy, convincing people to use them is hard. It takes time. It takes senior leader buy in. It takes an open mind to rethink how your job can be done. It might be hard, but in the end it’s worth it.\nOften we get asked by finance teams outside the company if they can take our “Commercial Predict” or “Fusion” tool off the shelf and start using it at their own company for forecasting. Sadly you cannot. We build a lot of these custom tools because we don’t have a choice. Microsoft’s business is complex. Often we need custom solutions that are hard to standardize in external products. Thankfully the machine learning methods we use are available for free as an open-source R package. Check it out if you’d like to learn more."
  },
  {
    "objectID": "posts/2024-07-15-msft-ml-fcst-journey-3/index.html",
    "href": "posts/2024-07-15-msft-ml-fcst-journey-3/index.html",
    "title": "Microsoft Finance ML Forecasting Journey: Part Three",
    "section": "",
    "text": "This is a multipart series:\n\nPart One\nPart Two\n\nPart Three\n\nBy now you should know how we started our machine learning (ML) forecast journey and how we applied it to Microsoft’s largest forecast process. But the fun doesn’t stop there. We were able to transform the biggest forecast process, but not every forecast process. There are hundreds more forecast processes in finance that are still in the dark ages. Essentially people with paper and pen creating these forecasts (written down inside excel models). Not knowing the potential ML can have on their job. All of these forecasts are important, but cannot scale in centralized tools like we discussed in part two. Something else had to be done. Centralizing processes can help solve many problems but not all problems. Sometimes you have to build democratized tools that give the power back to the people. That’s exactly what we did with ML forecasting. Keep reading to find out how.\n\nTotal Addressable Market\nThere are around 5,000 full time employees who work for Amy Hood, the CFO of Microsoft. They do a lot of different jobs. Some are considered finance roles while others are not. About 40% of these employees do some sort of predicting the future. This is the “planning” in “financial planning and analysis” roles in corporate finance. These predictions are often around future financial metrics. Like how much revenue a product will make next month or how much headcount we will have on a specific engineering team. Let’s run the numbers to see how much time, and in essence money, is spent doing this one simple job of forecasting.\n\n5,000 finance employees\n40% create forecasts\nSpend ~4 days a quarter creating forecasts (very conservative number)\n\nDoing the math this equals around 24,000 days of human effort spent every year forecasting (24,000 = 5,000 x .40 x 12). If the average finance headcount costs Microsoft $250,000 year (salary, benefits, office space, etc) then the total cost of forecasting is around $24,000,000 ($1,000 per day x 24,000 days). Yep, that’s 24 million a year just crunching numbers in excel. This is also a pretty conservative estimate. I know some teams who spend weeks every quarter just forecasting, so it could easily be higher.\nThink of this 24 million as the total addressable market (TAM) for forecasting in Microsoft finance.\nThe largest forecast process we discussed in part two only saves 10% of this 24 million. So we have a long ways to go in making a dent in this TAM. Unfortunately we cannot create 9 more centralized forecast solutions to cover the TAM. There is a long tail effect here, where the last 50-60% of forecasting might be done by 100+ forecast processes. We simply cannot create ML forecasts for everyone. There are not enough data scientists and data engineers needed to do that.\nBut hang on a second, what if we flip the script on that idea? Instead of having a dozen data scientists and data engineers maintain 100 ML forecast solutions. What if we have 100 regular finance people maintain their own single ML forecast? This is how a tool called “Finn” was born.\n\n\nSelf Serve Ice Cream Prototype\nI graduated Microsoft’s Finance Rotation Program (FRP) in the fall of 2018. My full time post program role was on the same business intelligence team as my fourth rotation. Basically I stuck around until they gave me a job. In that job I supported the Bing finance team. I helped them build ML forecasts for things like search volume and revenue on the Bing platform. It was my first true ML job where I wrote code and had legit business partners. It was awesome. Writing code to train models each day was a dream come true.\nInitially I wrote code to create models to forecast search volume. Pretty soon that spread to trying to calculate search rates, or how much money we could make off “x” amount of search volume. This PxQ approach would be used to get to search revenue. I created monthly forecasts, then weekly, then daily. Each time I had to rewrite the code to purpose fit it for the specific task at hand.\nPretty soon I started to turn the custom code written for each forecast task into reusable components that could be used across all forecast projects. Just take the code and plug ’n play with new historical data. It was fun to build and I was learning a lot. This meant that I was always on the hook for producing new forecasts though. Each time the Bing finance team needed an updated forecast, I had to be there to run it, create a report, and send it to them. I was now the data scientist training the ML models, the ML engineer serving the final outputs, and also the PM creating the final report and working with the business partner. I quickly learned that this was impossible to scale.\nWhat really cemented the scale issue was when another team, called Device Market Intelligence (DMI), heard about the work in Bing and wanted it applied to their market analytics of the Windows PC ecosystem. I was able to take the reusable code and apply it to DMI PC shipment forecasts, but I was still the human in the loop. Always on call to update forecasts and make tweaks as I got business partner feedback. I needed to scale myself.\nIt was around this time that I was at my desk having just come back from lunch. I was thinking about ice cream, and how awesome those self serve ice cream machines are. Where you can make the worlds largest ice cream cone or a huge sundae in a bowl. As a random thought I wrote down “self serve machine learning” on a sticky note and stuck it on my computer monitor. I didn’t give much thought to it, but thought it was cool enough to write down to make sure I didn’t forget it.\nMonths rolled by. I continued to run ML forecasts for both the Bing and DMI finance teams. Each time manually pulling the data and kicking off ML runs. Then sending them the outputs via excel report. It was at this time that I spoke with a coworker about an automation project that blew my mind. They were able to create an API that could kick off a large forecast process on demand. Without having to manually run code. Basically you press a button and the rest of the process takes care of itself. This started to turn the wheels in my head. What if I could take my reusable ML forecasting code and put it behind this kind of automated process?\nI couldn’t stop thinking about it. How could I create a way to let someone kick off a ML forecast run without needing to come talk to me? Could they pull their own historical data, upload it inside of some tool, then an hour later get back a forecast they could use? I had no clue how to do this but wanted to figure how.\nOver a few months I was able to put together a god awful prototype. One that allowed a user to fill out an excel template that had tabs to add their historical data and tabs to control certain inputs like forecast horizon. They could then upload that file into a Microsoft Power Automate flow. This flow would take their file, copy it to a shared server, then kick off an API that would run the ML forecast process. After it finished running the user would get an email with a link to where they could download the ML output results. It was all built with chewing gum and duct tape, but it worked. It actually worked. I felt like a super hero.\nAll great tools need a name. Most things at Microsoft are acronyms, which everyone hates. There is literally an internal company acronym lookup tool because there are so many. So I wanted to create a name that wasn’t an acronym, but also kind of described what it did. In essence it was a tool for financial forecasting. I settled on a one syllable word that kind of had finance in the name. I called it “Finn”. When I told this to my manager, they said “are you sure”? And “ok we can always update the name later”. The name stuck and is still around today. Sadly people still think it’s an acronym.\nI rolled out an early beta of Finn at Microsoft’s company wide hackathon in July 2019. Finance employees bravely signed up to be my guienne pigs for three days. They would stop by, bring some historical data they want forecasted, and attempt to use the tool. They got confused at times. And even more times the tool broke on a corner case with their data. It was glorious. Fixing bugs on the fly and seeing people use something I’ve built, even be a little excited about it, was like nothing I’ve ever experienced. I knew this was it for me, I had to make this work.\nThe hackathon was a medium success. Less than 10 people showed up, but those who did gave amazing feedback that I used to make the tool better. I was now ready to officially launch it to the masses. Before the big launch, I demoed it to my BI team, and people laughed. Yep, laughed. People said it was too complex and no one in finance would follow the steps needed to submit a forecast run. It was hard to hear but good feedback. I continued to iterate.\nBy the fall of 2019 Finn was ready for launch. We held training sessions, presented at CVP level all hands meetings, and basically told everyone I knew that they could start using Finn. Machine learning was still so new to people, so there were always people willing to try it out. With each passing month I would gather feedback, making improvements, and slowly usage grew.\n\n\nScaling Up\nFinn was an interesting tool with a terrible UI. The UI was an excel template uploaded through a Power Automate flow. It wasn’t pretty but it worked. The tool needed a facelift, something to take it to the next level. Thankfully at this time my BI team was working on another tool called “Replay”. It was touted as an internal portal for custom built apps specific to Microsoft Finance. The first big feature was going to be centralized reporting. They were also interested in adding other apps into the mix. We pitched them Finn and thankfully they agreed to add a Finn app.\nThe newest Finn app launched alongside the new Replay site in March 2021. People could now go to a legit website, upload an excel or csv file, click through a UI to adjust various inputs, then click submit. The ML magic would happen behind the scenes. Then they could come back to the site and see when their ML run finished. Even download the output files. A user could submit a ML forecast run in 10 clicks of their mouse. It was magnificent.\nIn order to make this happen we had to completely rebuild the ML backend of Finn. Moving away from on-prem servers and into the cloud on Azure. This involved things like data lakes and Azure Machine Learning. We were able to migrate the Finn API to an Azure ML pipeline. Allowing for better scale and monitoring. The core modeling code also got an upgrade. We partnered with the Chief Economist team within Microsoft Research to make the ML brain of Finn more robust. The data scientists on the Chief Economist team were amazing mentors to me. I learned years worth of knowledge within months. It goes to show how a good mentor can make all the difference.\nAfter giving the core ML code a makeover, we realized that we could package it up and make it open-source. Allowing other finance teams at other companies to use the exact same forecast process we use at Microsoft. This was released as an R packaged called finnts. Anyone can now take the code off the shelf and use it, free of charge.\nWe also realized that not everyone wants to use the self-serve UI to get a forecast. Sometimes people still want to use code, so we built a more general purpose API that allows anyone to call Finn on demand via API and give them a forecast on their specific data lake. Various engineering teams in Treasury, FinOps, etc now call Finn via API to produce forecasts on demand and at scale. This is how the Fusion tool mentioned in Part Two uses Finn today.\n\n\nFuture State\nFinn continues to change almost on a monthly basis. There are hundreds of things we want to add or improve in the tool. Here are a few that come to mind.\n\nForecast Accuracy: Always a top priority. No one will use a ML forecast if it’s less accurate than the previous manual forecast. There are countless ways we can make Finn more accurate. Thankfully all of them will be made available in the open-source finnts package.\nModel Interpretability: Once finance users get the level of accuracy needed, their next question is always around knowing how these models created the forecast. This is our next top priority that we are now starting to work on.\nForecast Adjustments: ML forecasts may not capture everything about the future. There might be changes in business strategy, new product launches, or tax changes that could affect the forecast. This is where human domain knowledge comes in the form of manual adjustments. We want to create a way for ML to do the initial 80% of the work, and allow humans to come in for the last 20% and easily make and track manual forecast adjustments when needed.\n\n\n\nLessons Learned\n\nParadigm Shift\nGoing from a manual forecast done in excel to a ML forecast requires a large paradigm shift in the brain of a finance person. You’re going from a deterministic process, where you know all the inputs and formulas used to create the forecast. To a probabilistic one, where there is this black box that takes in data and spits out a forecast. There are ways to explain what’s going on in the black box, but they are not perfect. You will not be able to trace through a ML forecast step by step like you can with a good excel model. So finance people need to change their mindset and know the potential tradeoffs of using ML. It boils down to a change in control. From letting a human control the entire forecast to now letting a machine control most of the process. As AI technology gets better, we will all turn into a form of a manager. Instead of managing people, we will manage little AI employees (or agents) to do our work for us. The sooner you embrace this, the bigger the impact you will have going forward into this AI future.\n\n\nDuplication of Work\nMost of the time people do not stop their manual forecast process and immediately switch to a 100% ML powered forecast. Often there needs to be overlap between the old process and the new process. This can be a few months (or even years) where ML is ran alongside the manual forecast process. With ML serving as a triangulation point until the team is ready to make the switch. This creates a duplicate of effort. Finance people barely have enough time to do their current job to sometimes learn a new way of doing it. That’s when senior leadership needs to kick in to help their teams prioritize using ML. Not just as a nice to have but to eventually change how their jobs get done. To do that brings us to the next lesson, taking the leap of faith.\n\n\nLeap of Faith\nYou know that scene in the Dark Knight Rises where Batman escapes the third world prison? The reason he escaped was because he literally took a leap of faith, without the rope. Using ML as a triangulation point is like jumping with the rope attached to you. You will never truly have ML help you because you can always fall back on the manual forecast. I’ve seen some teams use Finn for years but only ever as a triangulation point. In order to truly get the benefits of ML, you need to take the leap of faith. Jump without a rope. Ditch the manual forecast. It seems scary at first, but soon you’ll be laughing at how much time you spent forecasting each month.\nThe best way to take a leap of faith is having senior leaders give you the kick in the butt to make the jump to full ML. If senior leaders aren’t expecting to see ML usage grow on their team, adoption will slow to only those employees brave enough to jump by themselves. That’s a small number. Get higher ups to buy in and lead the way.\n\n\nInvest in Youth\nIt’s a lot easier to teach someone fresh out of school to use ML compared to a senior employee who has built excel models for 20 years. Younger people are more open to technology. They have no bad habits to unlearn. They don’t know any other way of doing things. These are the people you want to equip with ML tools. If they use ML in their initial jobs, once they become a senior executive they will make sure ML use becomes standard on their teams. This is kind of like planting trees. It takes time to see results, but one day you will have an entire forest of ML experts leading the charge.\n\n\n\nFinal Thoughts\nThis wraps up our three part series on ML forecasting in Microsoft finance. I started at the beginning with our first ever solution in Part One, then discussed how we built tools to centralize the forecasting process in Part Two, and finally I wrapped up with telling you how we decentralized ML to everyone in Finance in Part Three. If you’d like to use the same forecasting techniques as we do at Microsoft, check out our free forecasting package called finnts.\nMay the MAPE ever be in your favor. Happy forecasting!"
  },
  {
    "objectID": "posts/2024-08-23-weekend-reads/index.html#articles",
    "href": "posts/2024-08-23-weekend-reads/index.html#articles",
    "title": "Weekend Reads (8/23/24)",
    "section": "Articles",
    "text": "Articles\n\nSimple Answers to Hard ML Forecasting Questions\nLLM Performance vs Price\nEx Google CEO on Work From Home\nFixing Offsides in Soccer for Good\nAI Tongue Scans Spot Disease Early"
  },
  {
    "objectID": "posts/2024-08-23-weekend-reads/index.html#videos",
    "href": "posts/2024-08-23-weekend-reads/index.html#videos",
    "title": "Weekend Reads (8/23/24)",
    "section": "Videos",
    "text": "Videos\n\nBreathing Expert on Diary of a CEO"
  },
  {
    "objectID": "posts/2024-08-23-weekend-reads/index.html#podcasts",
    "href": "posts/2024-08-23-weekend-reads/index.html#podcasts",
    "title": "Weekend Reads (8/23/24)",
    "section": "Podcasts",
    "text": "Podcasts\n\nPeter Thiel on Joe Rogan\nPieter Levels on Lex Fridman"
  },
  {
    "objectID": "posts/2024-08-23-weekend-reads/index.html#movies",
    "href": "posts/2024-08-23-weekend-reads/index.html#movies",
    "title": "Weekend Reads (8/23/24)",
    "section": "Movies",
    "text": "Movies\n\nKumare\n10 mph\nBill Cunningham New York"
  },
  {
    "objectID": "posts/2024-09-04-sde-career-path/index.html",
    "href": "posts/2024-09-04-sde-career-path/index.html",
    "title": "My Path from Finance Intern to Senior Software Engineer at Microsoft",
    "section": "",
    "text": "I write code every day as a senior software engineer at Microsoft. But I didn’t use to. When I came to Microsoft as an intern in summer 2015, I was just your ordinary finance intern. The only code I knew was excel formulas. During the summer I realized the power of data+code and made it my mission to figure it out.\nI have previously written about my journey in Microsoft’s Finance Rotation Program (FRP). That details the first half of my career and how I got my first engineering job, but not what I do today as a senior software engineer. Most of my time is spent on building and operating machine learning systems used by others across the finance organization. It’s been a hard journey, but also a fun one. If you’re like me and are interested in making the switch, here are some pieces of advice that might help you. Like all advice, take it with a grain of salt. Advice is like getting last years winning lottery numbers. They might have worked for that person in the past, but it may not work for you. Anyways I hope you find it helpful.\n\nFocus on Slope over Intercept\nWhen I came back as a full time employee after my internship, I kept seeing these new signs everywhere talking about something called a “growth mindset”. During my internship these words were never mentioned. Now they are taped to every blank wall on campus. What’s going on? Microsoft’s CEO, Satya Nadella, was in the midst of working on a culture shift at the company. Trying to go from employees that were “know it alls” to a more improved “learn it all”. This new attitude was perfect for me because I knew jack squat about coding, and I wanted to learn.\nIf you want to pivot your career into software engineering, there are two ways to go about it. The first and most common is to go to school and get a degree. The second is to figure it out on your own, aka the learn it yourself path. Both paths have their pros and cons. Going to school is the more direct path, but self-learning may have a different kind of benefit. If you’ve only ever learned how to code on your own, then you are setting yourself up for lifelong learning. As opposed to someone who went to school, who is used to classroom learning. The learning never stops once you start coding for money, so having the skill to learn proactively and not on a predetermined schedule is a super power.\nHaving technical knowledge from school puts you ahead of someone else who is just starting to learn on their own. But if that self-taught person is learning at a pace higher than the school smart person, eventually their trajectories will cross and the self-taught coder will surpass them. Think of it like a graph. Where school knowledge gives you a high Y intercept, a good starting point, but the rate of your learning every day after is the slope. The most important thing is to focus on the slope over the intercept.\n\nThe learning never stops. In fact it only get’s harder. Not the learning itself (maybe it does), but taking the time to learn. Build the habit now, stay curious. The biggest trap is getting into the mindset of “I’ll start learning new things again once current xyz thing cools down”. That thing might be a new job, getting married, having a kid. As your responsibilities grow in life, your free time shrinks in proportion. Make sure learning stays a priority.\nA good way to learn on your own while having an existing job is to make the learning part of your job. If you want to learn more about AI technology, go tell your boss that you want to work on a project with AI that could help the team. I’m 100% positive you’re boss will say yes. Getting free engineering help is usually not turned down by anyone. We all need better software in our lives. Once your boss expects you to deliver on something, with a deadline, then that’s the healthy pressure needed to go out and learn the damn thing. In addition to signing up for project work you can also do the same for traditional learning through books, courses, and certifications. For me back in the day I told my boss I was going to finish a Microsoft sponsored course in data science by xyz date. Each month I would provide her updates on my progress. Finishing that course become one of my deliverables in my job. That allowed me to spend some time each day at work on learning, without feeling guilty because it was actually part of my job to learn. Make it part of your job and things become a lot easier. Compared to squeezing in a few minutes before or after work without telling anyone. If your upcoming performance review is tied to reading a few books and building a new thing for your team, you will have all the motivation you need to get it done.\nIf you can’t find any good projects on your team, look elsewhere. You could go help another team at your company that might need some coding help, or a local charity or organization you care about. Towards the end of my time in the FRP, I stumbled across a team at Microsoft called the Airband Initiative. This team is focused on growing internet access to communities all around the world. They needed some help crunching data from policy documents and with zero experience I was able to come join the team as a temporary resource to help them with this analytics project. I was free labor so they didn’t have anything to lose, but I had everything to gain. I got good experience with natural language processing and working with unstructured data. Eventually the project grew. I ended up presenting the whole thing to Microsoft’s chief data scientist and even got to showcase the work at the 2019 Internet Governance Forum in Berlin. Where all my expenses were paid. All because I spoke up and offered to help. The Airband team got help, I learned a ton, and become an international data traveler. Talk about a win-win-win.\nMaybe you’re still in school, but want to pick up some coding skills. Go see if any student organizations need help analyzing data or updating their website. Maybe the Football team could use another hand at analyzing what plays work best on third down, or if the basketball team needs to play a smaller lineup to score more points. The coffee shop in your business school might need some help on seeing what kind of sales promotions bring in the most revenue. The opportunities are endless. And because you’re offering your services for free they will not turn you down.\nLast thing I’ll say about learning is that I’ve seen too many people get a masters in data analytics, AI, computer science, etc. only to get a job post-school that does none of that. It’s a shame. Don’t settle. Don’t give up. Keep learning.\n\n\nReal Coders Ship\nReal coders don’t just take classes and read books. They ship, meaning they build stuff and put it out into the universe. My learning only started when I put the books down and started pulling real data and training real machine learning models. Projects that actually helped my team in my current job, even if that job wasn’t software engineering. Shipping 10 projects is more impressive than acing 10 classes.\nSome people create a “portfolio” of projects and have it live on their GitHub. I don’t think that’s a good idea. Real work is messy, and is the best indicator of future performance to an employer. Projects in a portfolio allow you to control all the variables, which is not how the real world works.\nYou can have public GitHub repos that show as samples of your work. But they should not be sample projects. They should either be pieces of work that you applied in your real job made open source and accessible to others. Or side projects where you wanted to build something cool to help you in your personal life or another organization. For example if you built your own website for your wedding, and had the code in a GitHub repo for hiring managers to see your coding chops. Or you built a bot that constantly analyzes your fantasy football league’s waiver wire and automatically picks up the best players for you based on AI powered analysis. There are endless ideas. Get building. Go out and ship.\n\n\nAI is Your Friend\nIn this post ChatGPT era, all technical work becomes 100x easier to learn. You don’t need school, or even books. You can, for free, start using a large language model (LLM) to learn coding concepts and even have it ask you technical questions to test your learning.\nChatGPT and other tools like GitHub Copilot now become your coding buddy, always there to help you when you get stuck or don’t know how to do something. Any new kid on the coding block can now get up to speed on languages like Python quickly because they have a personal AI tutor available 24/7. Go pay for a few of these AI services, and use them every day.\n\n\nUse Your Competitive Advantage\nSome people feel like coming into software engineering from a nontechnical discipline is a major handicap. I actually think it’s a superpower. If you come from a business background you actually have something other engineers do not. Business domain knowledge combined with technical knowledge is a deadly combination. You now understand both sides of the coin. You know how to make decisions about the business (through the lens of marketing, finance, sales) and can now go and build things that improve how the business serves customers.\nFor me having a finance background first, then learning coding second was a good combination. It created a lot of skills that are now invaluable. For example the skill of communicating things in simple terms. When I was building machine learning solutions for finance users, no one cared about the sophistication about my model or how much feature engineering I applied to the data. They just wanted to know how the solution ran at a high level and if it actually worked by showing improvements in a certain metric. For example, creating a more accurate revenue forecast for a Microsoft product. I’ve seen traditional data scientists/engineers try to explain their work to regular people and it can sometimes be like nails on a chalkboard. It’s painful to watch because they cannot explain themselves in plain english. And if the end user can’t understand how something works, then they won’t trust it, which means they won’t use it.\nUsually being 80% competent at two skills can create more impact than being 99% competent in a single skill. Focus on combining skills. If you can build and you can sell, you will be unstoppable.\n\n\nBaby Steps, Not One Giant Leap\nDon’t go and build the next social networking app on your first coding project. Crawl, walk, then run with new technology. Here is a good workflow for learning progression.\n\nCrawl: Get some code working on your computer.\nWalk: Move that code up into the cloud and have it run on a different machine that’s not yours.\nRun: Build a production ready system with proper CI/CD, unit testing, version control, etc.\n\nThose steps can take years, and that’s ok. You might even get a software engineering job with just knowledge of #1. But as you progress you will eventually figure out #2 and #3. The same can be said for the actual job progression.\n\nBusiness Partner: You are the business person who works with the engineers.\nProgram Manager: Now you are the person in the middle between the business partner and the software engineer. This is an engineering role, and in some companies like Microsoft your compensation will be the same as a software engineer.\nSoftware Engineer: You now build things, in collaboration with the program manager who tells you what the business partner needs.\n\nGoing from business partner, to program manager, to software engineer is a lot easier than skipping directly to software engineer. When I graduated the FRP my first role title was “IT Solution Manager”. I still don’t know what that means. Overall I was basically a program manager that coded on the side when no one was looking. Eventually that side coding turned into real solutions I pitched to my manager and got into production. Which eventually led me to working solely on building things. Then I finally got the title of software engineer. It didn’t happen overnight, but it was possible.\n\n\nGetting Promoted\nOk so maybe you now have a sweet engineering gig. Now what? How do you grow your career? Let’s talk about getting promoted.\nYou get promoted when what you build has impact. Not just for building cool or complex things. Most impact with technology can boil down to a few things.\n\nTime savings\nMoney savings\nIncreasing revenue\n\nYou work needs to boil down to numbers. Not what you shipped but what happened after you shipped it. Most impact is money and time. Either saving it or getting more of it. What gets measured gets managed. What gets measured gets promoted. Training a deep learning model is cool but shipping a simple linear regression in half the time might get you promoted twice as fast. Everyone is in sales. Once you build it, you have to sell it. If you can build and sell, you will be unstoppable. Ask your boss what you need to do this year to get promoted, then do those things. Ruthlessly prioritize and execute. Ignore everything else.\n\n\nFinal Thoughts\nLearning to code is important. Life is all about leverage. The best types of leverage in the world are permissionless leverage. They have no marginal costs of replication. These are things like code and content (podcasts, blog posts, etc.). Why do you think I wrote this? Because this kind of content has leverage, meaning it can scale. It’s easier to write this once then to give the same advice to 100 people who ask for it. Save your keystrokes folks. That’s why this personal blog exists. Hopefully it’s helpful to you my dear reader 😊.\nPretty soon everyone will be a manager, just not managing people. You will manage AI people, or bots/agents, who will do your work for you. Knowing how technology works will help you grow your leverage, whether you’re a software engineer or not. Most things 50 years from now will boil down to AI doing the thinking plus executing digital tasks, and robots executing physical tasks. What do you think those run on? You guessed it, code. Software will eat the entire world, it’s just a matter of time. I’d rather be giving our robot overlords instructions then taking instructions from them. Everyone will have a choice. Take the red pill. Go all the way down the rabbit hole. Learn some code.\nLast thing I’ll say is that it takes guts to move to a new field. People who only do what their boss tells them to do will never have the guts to do it. It takes a certain amount of high agency to do it. If you’re considering or are already in the process of doing it, I salute you. The world needs more people like you. Good luck!"
  },
  {
    "objectID": "posts/2024-10-02-ts-fundamentals-whats-a-time-series/index.html",
    "href": "posts/2024-10-02-ts-fundamentals-whats-a-time-series/index.html",
    "title": "What’s A Time Series?",
    "section": "",
    "text": "This post is part of a larger learning series around time series forecasting fundamentals. Check out the learning path to see other posts in the series.\n\nAI’s Are Like Onions, We Both Have Layers\nThere’s a lot of terminology in the world of artificial intelligence. With a lot of layers to it. The practice of time series forecasting is many layers down. How deep? Let’s start peeling back the layers.\n\n\nArtificial Intelligence\nArtificial Intelligence, or AI, can be described as the ability for machines to learn and think like humans. It’s as simple as that. While that’s an easy thing to say, it’s a hard thing to do. AI has been around since the mid 20th century, with a lot of ups and downs. There are multiple subdisciplines of AI. Things like robotics and expert systems. But the biggest and most important is machine learning.\n\n\nMachine Learning\nMachine Learning, or ML, is all about creating algorithms learn on their own from data. ML turns the idea of traditional programming on its head. Traditional programming is all about writing out explicit instructions or rules, in the form of code, that tell a computer what to do. ML on the other hand is code in the form of an algorithm where the rules are learned from training data. It’s a unique paradigm shift that has unlocked countless advancements in computing. Self-driving cars, human genome sequencing, and yes even forecasting has all been driven by these algorithms that can learn. Similar to AI, the world of ML has multiple disciplines. The one we’re interested in is supervised learning.\n\n\n\nimage source\n\n\nSupervised Learning\nSupervised learning is a type of machine learning where you teach a computer to make predictions or decisions using examples. It’s like teaching a child to identify animals by showing them pictures of different animals and telling them what each one is. In supervised learning, you give the computer a bunch of “input” data (like pictures of animals) and the correct “output” (the names of the animals). The computer learns from these examples, and over time it gets better at predicting the outputs based on new inputs it hasn’t seen before. There are a few different flavors of supervised learning, but in this learning series we care about regression.\n\n\nRegression\nRegression is a type of supervised learning method that helps predict a numerical value based on past data. Imagine you want to predict the price of a house based on its size, location, and number of rooms. Regression analyzes the historical data of house prices and their characteristics to find a relationship. Once this relationship is understood, you can input the characteristics of any house and the regression model will predict its price.\nTraditional regression modeling usually doesn’t have a time component, but when you need to predict a number over time, that’s when we enter the crazy world of time series.\n\n\nTime Series\nA time series is a series of numerical data points recorded at regular intervals. For example the highest temperature each day over the last 10 years can be thought of as one time series. If we have that temperature data for 50 cities, then we have 50 time series in our dataset. A time series can be at any kind of time interval. Yearly, quarterly, monthly, daily, hourly, etc. You get the idea.\nThe art of forecasting is just taking historical time series data, and feeding it into a model that can learn from historical trends and seasonality patterns. Ultimately creating a future prediction for the next few periods of time.\nMost regression models in machine learning can be turned into a time series forecast model by the process of incorporating time dependant data as features through a process called feature engineering. A feature is just a variable in your dataset that can be used to help predict your target variable. The process of creating features before you train a model is called feature engineering. The target variable is what you want to predict going forward. These machine learning regression models can be thought of as multivariate models, since they can incorporate many different variables or features into a model.\nLet’s apply this to a quick example. For a monthly revenue time series forecast, our target variable will be the revenue variable in our dataset. And we can create features like month of the year, how much money we made in the same month last year, etc. to help predict revenue.\nThere are also models that only exist in the world of time series forecasting. These are more statistical models and less machine learning models. Meaning they operate like equations that can applied to your data compared to a machine learning model that tries to learn the patterns underlying the data. These models are often univariate, which means that you only need the historical target variable values accompanied by a timestamp (list of dates) to train these models. While they might be simple in nature, they are still very useful even today. Often beating more complex models like deep learning.\n\n\n\nTime Series Example\nNow that we’ve peeled back all of the layers on time series forecasting, let’s take a look at some data. In this learning series I will be using the most common time series that applies to every company in the world, historical revenue. The data we’ll use is completely made up, but created in a way to illustrate a lot of the learning concepts throughout the series. The sales data is recoded at a monthly level, broken out by countries our hypothetical company operates in and what products it sells. Let’s take a quick look at the data.\nYou can find the raw data here.\n\nIn the data there are sales for two countries across two products. This means we have four unique time series we can forecast. One for each unique combination of country and product. Throughout this learning series we will use this data as an example as we work through different time series concepts.\n\n\nFinal Thoughts\nTime series is a very niche corner in the AI universe. It may seem small but in reality every company in the world deals with time series. They make up most financial data like sales and are top of mind for every leader in the C-suite. You should now know what a time series is and understand more of the common terms you will hear around machine learning. Now that we have a good foundation of terminology, lets explore the data even more in the next post."
  },
  {
    "objectID": "posts/2024-10-15-ts-fundamentals-eda-data-shape/index.html",
    "href": "posts/2024-10-15-ts-fundamentals-eda-data-shape/index.html",
    "title": "Exploratory Data Analysis: Shape of the Data",
    "section": "",
    "text": "This post is part of the exploratory data analysis chapter within a larger learning series around time series forecasting fundamentals. Check out the main learning path to see other posts in the series.\nThe example data used in this series can be found here.\n\nInitial Data Smell Test\nThe first step of exploratory data analysis for time series is getting a feel for the data at a high level. This doesn’t involve complex statistical analysis. Just simple charts to see how things look. When analyzing these charts we’ll use the most underrated concept in machine learning. The human smell test. This involves using your eye balls to look at the data and understand what could be going on based on your domain experience. Let’s chart the time series in a few ways and give it a smell test.\n\n\n\nTotal revenue by month\n\n\nLooking at the data all up shows a pretty stable upward trend with a few bumps in the data. Right now it’s hard to tell if those bumps in 2019 and 2021 are outliers or not. We would have to dig deeper into the data. The trend seems to be growing exponentially. Meaning it’s not a straight line but instead one that seems to grow faster each year. This kind of hockey stick growth is what all businesses aspire to. Lastly it’s hard to tell if there is any strong seasonality.\n\n\n\nTotal country revenue by month\n\n\nBreaking out the data by each country gives us a better picture of what’s going on in our monthly sales. Canada is the country with the exponential growth, combined with a few spikes in some months. These spikes could be one off events that are unpredictable, or something happened in those months that can be quantified in data.\nRevenue in the United States has more of a linear upward trend with strong seasonality. There is something strange going on in 2020 though. Any guesses on what that could be?\n\n\n\nTotal product revenue by month\n\n\nLooking at revenue by product tells a similar story as revenue by country. Cookie sales seem to be a lot more bumpier than ice cream, which is more stable and growing fast.\n\n\n\nRevenue for each individual time series by month\n\n\nFinally we can view each time series individually. Our initial hunches at higher aggregations are now confirmed. We can see Canada cookie sales have those crazy spikes. After talking to our sales and marketing teams, we learn that the spikes are caused by new cookie product launches. Ice cream sales in Canada is the one series with the exponential growth. Both don’t have much seasonality.\nRevenue in the United States is all about seasonality. Cookie sales show an interesting dip in 2020 but seem to rebound in 2021. This could be caused by the impact of COVID. Ice cream sales has both strong trend and seasonality. This will make it easy to forecast going forward. The more stable the trends and seasonality in your time series, the easier it is to create an accurate future forecast. The future should always resemble the past.\n\n\nFinal Thoughts\nWe have now completed our initial smell test of the historical data. Based on our domain knowledge of the business we can start to understand what happened in the past, and use that information to help us create a better future forecast. The smell test was easy on this dataset since there are only four time series. You might be working with thousands of time series at your company, so higher level smell checks might be the only thing you can do. There are more automated ways at analyzing the data, and will be explored more in future posts."
  },
  {
    "objectID": "posts/2024-11-06-ts-fundamentals-decomposition/index.html",
    "href": "posts/2024-11-06-ts-fundamentals-decomposition/index.html",
    "title": "Exploratory Data Analysis: Time Series Decomposition",
    "section": "",
    "text": "This post is part of the exploratory data analysis chapter within a larger learning series around time series forecasting fundamentals. Check out the main learning path to see other posts in the series.\nThe example monthly data used in this series can be found here. You can also find the python code used in this post here.\n\nYour Data is so Trendy\nEvery single time series you have ever seen has a trend and seasonality. If they don’t, then they are random white noise. A trend is just values steadily rising or falling over time. You could have a rising trend in some years, then a falling trend in others. Below is a chart with a strong trend but weak seasonality.\n\nSeasonality just means that your data has a recurring seasonal pattern. For example your data could spike during the holiday season from Oct-Dec, then fall drastically from Jan-Jun. Your seasonality could also change over time. Below is a chart with strong seasonality but weak trend.\n\nIn order to better understand the trend and seasonal patterns in your data, you need to isolate them into separate components. This is done through a process called time series decomposition. This process breaks out a time series into three separate parts: trend, seasonality, and remainder. The remainder component is just parts of your time series that can’t be explained by the trend and seasonal component. Let’s try decomposing a time series from our example data set.\n\n\nBreaking Out Trend and Seasonality with Additive Decomposition\nWe can isolate the trend component of a time series using simple methods like moving averages (MA). These are just simple averages over a span of time. Let’s take a time series from our example revenue data and create a moving average. In the first chart below we used a 2x12-MA. This moving average now isolates the trend of the time series. We can see a dip in trend around 2020 then a strong rebound going forward. The moving average doesn’t start and end at the same time as our time series, since we need a burn in period to start creating the averages. If we wanted to detrend the time series we can subtract the new trend series from our original revenue, shown in the second chart below.\n\n\nNext we can get the seasonal component of our data. This is very straightforward. All we have to do is take our detrended series and calculate the average revenue for each month of the year. After getting the average value per month, we can divide by the total average across all months. This ensures that the seasonal values of each month add to zero. Then we repeat the final values per month across all years. The final result is shown below. Now we can see there is very strong seasonal patterns at certain times of the year. Some quarters have high sales, while others do not. This kind of repeatable pattern will be very important for us when it comes to training models.\n\nFinally we can get the final piece of our decomposition, the remainder (aka residual) series. This is everything that couldn’t be explained by the trend and seasonal components. Simply take the original revenue time series, subtract the trend and seasonal components, and bam you have the remainder series.\n\n\n\nSTL Decomposition\nThe additive decomposition approach can work very well if your data has a linear trend and stable seasonality. But if the series is growing exponentially, or has changing seasonality, then more advanced methods are needed. This is where STL decomposition comes in. STL is an acronym for “Seasonal and Trend decomposition using Loess”, while loess is a smoothing method for estimating nonlinear relationships. This smoothing process allows for changing seasonality, a more complex understanding of the trend component, and more resistance to outliers. Here is the STL process broken down step by step.\n\nExtract Initial Seasonal Component\n\nFirst, STL identifies the seasonal pattern in the data. It does this by looking at the regular repeating cycles (e.g., monthly or weekly patterns).\nUsing Loess smoothing, it calculates a rough seasonal pattern that captures these cycles.\nThis step gives an initial estimate of what the seasonal pattern looks like.\n\nRemove Seasonality to Estimate the Trend\n\nAfter finding the initial seasonal component, STL subtracts it from the data to get a rough idea of the trend (the overall up or down direction).\nLoess smoothing is applied again to the deseasonalized data (data without seasonal patterns) to create a smooth trend line.\n\nRefine the Seasonal and Trend Components\n\nSTL repeats the process in cycles, fine-tuning both the trend and seasonal estimates.\nEach cycle alternates between smoothing the seasonal component and the trend component, adjusting them with each pass to better fit the data.\nBy repeating this process, STL allows the seasonal component to change over time if necessary and captures any evolving trends.\n\nCalculate the Remainder\n\nOnce the trend and seasonal components are finalized, STL subtracts both from the original data. What’s left is the remainder component, representing the random, unpredictable part of the data.\nThis remainder is what can’t be explained by the regular trend or seasonal patterns, capturing any noise or outliers.\n\n\nHere is the output of a STL decomposition. Each component can be added together to get the original time series, just like additive decomposition.\n\n\n\nReading the Tea Leaves\nUnderstanding the outputs of time series decomposition can be more of an art than science. The more you do it, the more your data intuition grows about what’s happening. Looking at the results of STL decomposition tells us a few things for our time series.\n\nThere has been mostly a stable trend over the years, except for a strange dip in 2020.\nSeasonality is strong with repeating peaks and valleys across the years. But it does seem to be growing with each passing year.\nMost of the residual points hover around zero, which is ideal. Having the residual component have an average of zero and a constant standard deviation over time means the data is just white noise, or things that can’t be explained. We do see a large drop in 2020 though…\n\nThis analysis shows us that there is most likely an outlier in 2020 that seriously impacted our data. If you don’t have a guess as to what this might be than you might have been living under a rock the last decade. The impact is from COVID-19. Thankfully this impact can be taken care of later in our modeling process. But for now it’s good that we identify it and get the domain knowledge to know what could have caused it.\n\n\nFinal Thoughts\nDecomposing a time series is often one of the first things an accomplished data scientist does before training any kind of machine learning model. Breaking the time series into its components allows us to understand the potential forces that is impacting the trend of the data and if there are any outliers in the data. Knowing this allows us to change our approach when training models. Ultimately leading to more accurate forecasts down the road."
  },
  {
    "objectID": "posts/2024-11-12-ts-fundamentals-autocorrelation/index.html",
    "href": "posts/2024-11-12-ts-fundamentals-autocorrelation/index.html",
    "title": "Exploratory Data Analysis: Autocorrelation",
    "section": "",
    "text": "This post is part of the exploratory data analysis chapter within a larger learning series around time series forecasting fundamentals. Check out the main learning path to see other posts in the series.\nThe example monthly data used in this series can be found here. You can also find the python code used in this post here.\n\nTime Series Data Has Memory\nThe biggest factor that differentiates a time series from another form of data is order. Each observation in a time series is ordered by well, you guessed it, time. Because of this order, there are unique relationships that develop in the data. Let’s take temperature for example. If today’s temperature is 70°F, and the temperature in the previous seven days was around 65-80, what do you think the temperature tomorrow will be? Probably not below freezing. It’ll most likely be similar to the temperature today. This memory about the past is one of the most powerful concepts in time series forecasting. It’s the concept of autocorrelation.\nAutocorrelation is when a value in a time series is related to previous values in that series. In simple terms, it measures how much the current value depends on past values. For example, if high temperatures today make it likely to have high temperatures tomorrow, the temperature series has a positive autocorrelation.\n\n\nCalculating Autocorrelation\nGetting the autocorrelation is as simple as computing the correlation of the existing time series with a lagged time series. For example let’s take a time series from our example data set, shown below.\n\nWe can create lags of one month, two months, and onwards. Then calculate the correlation between the original time series (with a lag of 0) and these new lagged time series. The results of that process are shown below, with a lag up to 24 months.\n\nCorrelation is shown on the y-axis and the lag amount is shown on the x-axis. The chart shows high autocorrelation for a lag of 0, 1, 12, and 24 months. A lag of 0 has a perfect correlation because it’s the exact same as the original time series. These lag correlations are shown as positive values, meaning that there is a positive correlation with the original time series. There are also above the shaded blue region of the chart. This shaded region is a confidence interval. This confidence interval helps us filter out lags that are not statistically significant, meaning they don’t have a correlation due to random chance. Any lag outside of the shaded region is significant and should have a real relationship with our original time series.\n\n\nPartial Autocorrelation\nAutocorrelation has one fatal flaw though. It can provide misleading correlations since it doesn’t account for correlations between lags. For example, there might be a very strong correlation with the original time series and lag 1 of the time series. Because of this correlation, there is a good chance that a lag 2 also shows a strong correlation. The lag 2 correlation might only be due to lag 2 having similar information to lag 1. So the end results could be misleading.\nTo account for this, partial autocorrelation was created. This process takes into account the correlations between lags. Making sure that the end correlations are only connected to the original time series. It is the preferred method of analyzing autocorrelation. So make sure it’s part of your time series tool kit. Here is the partial autocorrelation of our example time series.\n\n\n\nReading the Tea Leaves\nLet’s take a deeper look at the partial autocorrelation and see what insights it might provide. When looking at the partial autocorrelation chart, the biggest correlation is at lag 0. This makes sense since a lag of 0 is just the original time series, so there will be a perfect correlation of 1. Lag 1 shows a strong correlation, which makes sense, since it’s contains values closest to the original series. There are also strong correlations at lag 11, 12, and 13. This confirms our data has strong seasonality. The highest correlation is at lag 12, which means values today are highly correlated to values at the same time in the previous year (12 months ago). This information will be invaluable when we start to build models in the future.\n\n\nFinal Thoughts\nHaving strong partial autocorrelation in your time series will always lead to more accurate future forecasts. Especially if you expect the future to be similar to the past. If your data has little or no correlation with historical lags, then it will be more difficult to train accurate models to forecast the future. Calculating partial autocorrelation should always be one of the first things you do when working with time series. It’s quickest way to start building intuition about your data."
  },
  {
    "objectID": "posts/2024-11-26-ts-fundamentals-missing-outliers/index.html",
    "href": "posts/2024-11-26-ts-fundamentals-missing-outliers/index.html",
    "title": "Exploratory Data Analysis: Missing Data and Outliers",
    "section": "",
    "text": "This post is part of the exploratory data analysis chapter within a larger learning series around time series forecasting fundamentals. Check out the main learning path to see other posts in the series.\nThe example monthly data used in this series can be found here. You can also find the python code used in this post here.\n\nGarbage In, Garbage Out\nHaving quality data helps produce quality forecasts. There’s an old saying in the data universe, garbage in, garbage out. When your data is bad, you will get bad outcomes. The presence of missing data and outliers can reduce forecast accuracy. Which is a shame because they are so easy to fix.\n\n\nMissing Data\nTime series are unique from other types of data because they have a built in order, based on time. For this to work it’s important that each period of time is represented in the data. For example with a monthly data set spanning five years, we want there to be no missing value for any of the month’s during those five years. Having missing values creates challenges when trying to analyze and model the data, since most statistical and machine learning techniques assume all available time periods are present in the data. This leads to reduced forecast accuracy.\nThere are a few ways data can be missing in a time series.\n\nMissing Completely at Random (MCAR)\n\nMeaning: Data is missing for no specific reason, it’s just random. The missing data doesn’t depend on any pattern or the actual value that’s missing.\nExample: A sensor that records the temperature randomly fails to record a value now and then.\n\nMissing at Random (MAR)\n\nMeaning: Data is missing for a reason that depends on other information you already have, but not on the missing values themselves. You can explain the missing data using the rest of the data.\nExample: A temperature sensor is more likely to fail during hot weather, but the failure doesn’t depend on the exact temperature value it would have recorded.\n\nNot Missing at Random (NMAR)\n\nMeaning: Data is missing because of the value itself, it’s missing for a reason related to what’s not recorded. The missing data is biased, and you need to understand why it’s missing to deal with it.\nExample: A temperature sensor doesn’t record very high temperatures because it shuts down when it overheats.\n\nMixed Mechanisms\n\nMeaning: Missing data can be a mix of random, explainable, and biased causes.\nExample: Some values are randomly missed due to glitches (MCAR), some during maintenance periods (MAR), and others during extreme weather (NMAR).\n\n\nMost data I deal with is some combination of missing completely at random or not missing at random. For example there might be a months worth of revenue data missing in a system due to issues with the company’s enterprise resource planning (ERP) system. So that data is missing completely at random. A system error created the missing data. Another example is having monthly revenue for three products, where two of the products have five full years of revenue data and the other only has the last three years of data. This is due to that third product being newer and only being sold in the last three years. So that data is not missing at random but is due to a specific reason.\nFor the first two types of missing data (#1 and #2 in the list), it’s a good idea to use some sort of simple statistical model to interpolate what the missing value should be based on actual values on either side of the data. Below is a monthly time series from our example data. I removed a few months at random. We can use a statistical model that breaks down the time series by trend, seasonality, and residual. Then predict what the missing value should be based on the existing trend and seasonal components.\n\n\nFor the third type of missing data, it’s usually a good idea to go talk to the owner of that data. Maybe missing data in a system is simply due to that observation having a value of 0, so to save memory they just don’t record that value in the system. The easy fix there is to go in and replace those values with zero. You might have some time series in your dataset that are shorter than others. For example, most of your time series could have five years of monthly data, but one may have only the recent three years of monthly data. To deal with this, you could also replace those missing values with 0. That way each time series is the same length. This becomes important later on when we want to start back testing different models to analyze forecast accuracy.\nAfter you replace the missing data, it’s also a good idea to flag where missing values were originally. This can be done with a simple binary variable added to your dataset. Showing a value of 1 when the original data point was missing, and 0 when it wasn’t. This allows us to give this information to a model, and have it adapt to the changes we made in the data.\n\n\n\nDate\nValue\nMissing\n\n\n\n\n2023-01-01\n10\n0\n\n\n2023-02-01\n15\n0\n\n\n2023-03-01\n20\n0\n\n\n2023-04-01\n25\n0\n\n\n2023-05-01\n30\n1\n\n\n2023-06-01\n35\n0\n\n\n2023-07-01\n40\n0\n\n\n2023-08-01\n45\n1\n\n\n2023-09-01\n50\n0\n\n\n2023-10-01\n55\n0\n\n\n2023-11-01\n60\n0\n\n\n2023-12-01\n65\n1\n\n\n\n\n\nOutliers\nOutliers are values in a time series that depart from the norm. They can be extremely high or low values. Having outliers in your time series can worsen forecast accuracy. A model may try to learn the new changes in trend/seasonality caused by an outlier and carry them forward into the future forecast. We don’t want that. Instead we should identify when outliers occur and do our best at dealing with them.\nThere are a few ways we can spot outliers.\n\nSmell Test\n\nUse our domain knowledge to point out values that don’t make sense after inspecting the time series on a chart. For example, a temperature of 150°F in Seattle may not be physically possible. It doesn’t pass our smell test, and so we can flag it manually as an outlier.\n\nStatistical Methods\n\nCalculate statistical measures like Z-scores and interquartile ranges (IQR) to spot data points that are outside of standard ranges.\nThe Z-score method identifies outliers by measuring how far a data point is from the mean, in terms of standard deviations, with values typically beyond 3 considered outliers.\nThe Interquartile Range (IQR) is a measure of statistical dispersion and represents the middle 50% of the data. It is calculated as the difference between the third quartile (Q3), which is the value below which 75% of the data lies, and the first quartile (Q1), which is the value below which 25% of the data lies. The IQR method identifies outliers as data points that fall outside Q1−1.5×IQR (too low) or Q3+1.5×IQR (too high). This method works well for skewed or non-normal data because it uses the quartiles, which are less affected by extreme values than the mean or standard deviation.\n\nResidual Methods\n\nResidual methods, such as those used in STL decomposition (Seasonal-Trend decomposition using LOESS), identify outliers by isolating the residuals after removing trend and seasonality from a time series. STL splits a time series into three components:\n\nTrend: The long-term movement or pattern in the data.\nSeasonal: Repeating patterns or cycles at fixed frequencies.\nResidual: The remaining random noise or deviations after subtracting the trend and seasonal components.\n\nOutliers are detected in the residuals, which represent the part of the data that cannot be explained by the trend or seasonal patterns. Typically, residuals that fall outside a certain threshold, such as ±3 standard deviations or another robust range, are flagged as outliers. This method is particularly effective for time series data as it accounts for both trend and seasonality, ensuring that outliers are not mistaken for normal patterns.\n\nMachine Learning Methods\n\nMachine learning methods identify outliers by training models to learn the normal patterns in the data and then flagging deviations from these patterns. These methods include:\n\nIsolation Forest: This tree-based algorithm isolates data points by creating random partitions. Outliers are points that require fewer splits to isolate, as they differ significantly from the rest of the data.\nAutoencoders: Neural networks are trained to reconstruct the time series. Data points with high reconstruction errors (i.e., the model struggles to recreate them) are flagged as outliers.\nClustering: Algorithms like K-Means or DBSCAN group similar data points into clusters. Outliers are points that either belong to sparse or distant clusters or are classified as noise by the algorithm.\nProbabilistic Models: Techniques like Gaussian Mixture Models (GMM) assign probabilities to data points based on how well they fit the learned distribution, flagging low-probability points as outliers.\n\nMachine learning methods are powerful because they can handle non-linear, multi-dimensional, and non-Gaussian patterns, making them highly effective for complex time series. However, they require careful tuning and may need more data than simpler statistical approaches.\n\n\nLet’s use the residual method and see if we can spot a few outliers in our time series. First we will decompose our time series, then see what residuals are 3 or more standard deviations away from the average.\n\n\nThere seems to be on major outlier in mid 2020, shown as outside of the dotted upper and lower range. Which is not surprising since our smell test could have told us the same thing. COVID-19 in 2020 was most likely the cause for the sudden change in trend and seasonality for those periods around 2020. We can now confirm it with the residual method for spotting outliers. How cool is that!\nOnce we know the outliers in our data, there is a few things we can do to handle them.\n\nDo Nothing: Leave it alone, and let its presence impact our future forecast. This is a good approach if there is a structural change in the time series that you expect to carry onward in the future. For example COVID might have caused a permanent work from home revolution, which could impact your business that relies on growth in office space. So leaving it alone would give the model insight into a change that is expected to continue in the future.\nFlag It: Similar to flagging missing values, we can add a binary variable to our dataset. A value of 1 to indicate an outlier occurred, and a 0 when it did not. This will allow our model to understand true one time events that are not expected to happen again in the future. Making it less likely for a model to learn the change in pattern. COVID-19 was truly a one off thing, so flagging the outliers could be helpful for our time series.\nRemove It: Remove the outlier, and treat it like a missing value. You can then use the methods discussed earlier to fill in the missing value with something that is more in line with future trends and seasonality. Combining this method with adding an outlier flag from #2 is a solid approach.\n\n\n\nFinal Thoughts\nHandling the presence of missing values and outliers can sometimes be more art than science, so always make sure to be adaptable in using the right approach for your use case. When done correctly, cleaning your data of missing values and outliers can help you understand your data while improving forecast accuracy."
  },
  {
    "objectID": "posts/2024-12-01-november-learnings/index.html",
    "href": "posts/2024-12-01-november-learnings/index.html",
    "title": "November Learnings",
    "section": "",
    "text": "I love reading books, watching YouTube videos, listening to podcasts, you name it. Anything learning related is my jam. But I recently realized that if I don’t take notes on what I’m learning, I will probably forget everything. Now when I hear something interesting, I write it down in an Apple note for that month. Below are some of the learnings I jotted down in November, summarized by ChatGPT. I hope you find them as interesting as I did.\n\nPersonal Growth and Mindset\n\nHappiness and Presence: True happiness comes from being present and avoiding overthinking. Expectations play a big role in happiness: reality minus expectations equals happiness.\nAction and Results: Be impatient with actions but patient with results. Start quickly, but allow time for outcomes. Enthusiasm and belief are competitive advantages. Avoid the vicious cycle of inaction fueled by doubt.\nOverthinking: Rumination is the #1 symptom of depression. Action is a better way to change your thoughts than overthinking.\nQuotes on Mindset:\n\n“Knowledge is knowing what to do; wisdom is doing it.”\n“Cynics get to be right, optimists get to be rich.”\n“Ask a better question, get a better answer.”\n“At the end of the day you’re going to feel some type of way. So why not feel unbeatable. Why not feel untouchable. Why not feel like the best to ever do it.”\n\n\n\n\nProductivity and Decision-Making\n\nAction Frameworks:\n\nTime between deciding and acting defines success.\nBe clear on priorities: what you choose to work on is more important than how hard you work.\nUse tools like the Dickens Method to evaluate how today’s actions affect your future.\n\nWorry Management: Schedule a time to worry instead of letting it linger. Outside of that window, maintain a zero-worry mindset.\nTeams and Companies: Companies should be viewed as sports teams, not families—performance matters. Be clear about whether a role requires a commando (startup mindset), soldier (scaling), or policeman (stability).\n\n\n\nBusiness and Sales\n\nProduct-Market Fit:\n\nBefore product-market fit: founders must engage directly with customers in unscalable ways.\nAfter product-market fit: scale with sales staff and established processes.\n\nSales Approach:\n\nUse the problem-solution-specifics framework. If the customer disagrees about the problem, drop them.\n“Reality vs. narrative” moments are key opportunities for emerging tech.\n\nWealth and Risk:\n\nWealth is a byproduct of knowledge.\nRisk is what remains after considering every possible variable.\n“Survive” is the golden rule for emerging technologies.\n\n\n\n\nSociety and Systems\n\nSocial Observations:\n\nOlder industries prioritize regulation capture and splitting spoils over innovation.\nTraditions can empower tyranny when unquestioned.\n\nHomelessness and Addiction:\n\nAddiction and trauma are major causes of death for homeless individuals. Programs like Haven for Hope in San Antonio show promise in recovery-based justice systems.\nFentanyl is a growing public health crisis, being 50x more addictive than heroin.\n\nEconomic Realities:\n\n78% of Americans live paycheck to paycheck.\nThe first 15 years of mortgage payments largely benefit banks, not homeowners.\n\n\n\n\nStorytelling and Communication\n\nPower of Stories:\n\nStories connect human brains like APIs. They involve intention, obstacle, and stakes.\nThe best podcasters are “vibe architects,” creating environments that feel like casual, engaging conversations.\nGreat tweets follow principles of truth, novelty, and conciseness while being opinionated.\n\nMusic and Stories: Both resonate emotionally and inspire action.\n\n\n\nPractical Knowledge and Insights\n\nGeneral Insights:\n\nHanlon’s Razor: Never attribute to malice what can be explained by incompetence.\nRecruiting agencies charge ~25% of the first-year salary for hires.\nWarren Buffet owns more treasury bills than the U.S. government.\nSuccess is a window (external factors), and failure is a mirror (personal responsibility).\n\nTeaching and Learning: Computer literacy has untapped potential. Teaching AI-based math instead of manual long division could redefine education.\n\n\n\nMiscellaneous Wisdom\n\n“Tradition is the crown of the tyrant.”\n“The larger the group, the worse the conversation.”\n“Trying to think your way out of overthinking is like sniffing your way out of a cocaine addiction.”\n“Life without love is the worst prison of all.”\n“Make your life your hobbies, but don’t make your hobbies your job.”"
  },
  {
    "objectID": "posts/2024-12-19-who-ya-gonna-call/index.html",
    "href": "posts/2024-12-19-who-ya-gonna-call/index.html",
    "title": "Who Ya Gonna Call? ChatGPT!",
    "section": "",
    "text": "OpenAI has gotten into the holiday spirit by launching 12 Days of OpenAI. Each day at 10am PST they release a new product or feature to the world. We are currently on day 11 of their 12 day ship’mas extravaganza. Previous days have seen massive product launches of their new Sora video model and ChatGPT Search. The most exciting one of them all though has been the product they launched yesterday on day 10, something called 1-800-chatgpt.\nYou can now, toll free, call ChatGPT on any phone in the United States. For people outside of the US you can talk to it via WhatsApp. How cool is that! No account or credit card required. Just a friend you can phone 24/7 for whatever you need. This may seem like a small gimmicky feature but it’s actually one of their most profound launches to date, because now everyone has easy access to AI.\nNothing is truly mainstream until our parents, and to an extent our grandparents, start using the technology. My grandpa lives on a small farm in Kansas. He has no WiFi at his house, and most days he keeps his cell phone turned off and in a drawer. He does use his landline phone every day though. With this latest launch he can now talk to the most powerful language model in the world, using only his voice. No set up necessary, just call and chat. AI has jumped the walled garden of the internet and is getting embedded into more aspects of our lives. Now it can live as a contact in our phones. The barrier to entry on AI is getting closer to zero.\nIn addition to having the technically challenged use AI, there are countless other use cases of the new toll free AI number.\n\nAnywhere there is poor cell service. Stranded in the middle of Montana with a flat tire? The ChatGPT app may not work but you might be able to make a phone call. Now the AI can walk you through step by step how to change that flat tire, all for free.\nFind yourself in jail for a crime you were wrongly convicted for? The AI can be your one phone call, and might give you better advice than a paid lawyer.\nAre you one question away from untold riches on “Who wants to be a millionaire?” Phone your AI friend and have them provide an immediate answer to win the cash prize.\n\nThe power of AI is now available to everyone. No one knows what will happen when every human has equal access to limitless intelligence. Now we are one step closer. If you’ve never tried out ChatGPT before, get out your phone and make the call!"
  },
  {
    "objectID": "posts/2025-01-02-80-20-saas/index.html",
    "href": "posts/2025-01-02-80-20-saas/index.html",
    "title": "80/20 SaaS: Why AI-Driven Customization Is the Future",
    "section": "",
    "text": "Artificial intelligence (AI) has advanced to a point where one or two developers, working with AI-generated code, can build functional software in a matter of weeks. That prospect has led some to conclude that it spells the end of Software-as-a-Service (SaaS). After all, why rent a platform if you can build a decent alternative in-house—cheaply and on-demand?\nHowever, I believe there’s a more practical way forward. Instead of entirely replacing SaaS with AI-built tools, SaaS providers should deliver 80% of the core features and allow customers to customize the remaining 20% via AI. This approach maintains the convenience of off-the-shelf software while offering personalized functionality."
  },
  {
    "objectID": "posts/2025-01-02-80-20-saas/index.html#introduction-will-ai-kill-saas",
    "href": "posts/2025-01-02-80-20-saas/index.html#introduction-will-ai-kill-saas",
    "title": "80/20 SaaS: Why AI-Driven Customization Is the Future",
    "section": "",
    "text": "Artificial intelligence (AI) has advanced to a point where one or two developers, working with AI-generated code, can build functional software in a matter of weeks. That prospect has led some to conclude that it spells the end of Software-as-a-Service (SaaS). After all, why rent a platform if you can build a decent alternative in-house—cheaply and on-demand?\nHowever, I believe there’s a more practical way forward. Instead of entirely replacing SaaS with AI-built tools, SaaS providers should deliver 80% of the core features and allow customers to customize the remaining 20% via AI. This approach maintains the convenience of off-the-shelf software while offering personalized functionality."
  },
  {
    "objectID": "posts/2025-01-02-80-20-saas/index.html#why-some-believe-ai-will-kill-saas",
    "href": "posts/2025-01-02-80-20-saas/index.html#why-some-believe-ai-will-kill-saas",
    "title": "80/20 SaaS: Why AI-Driven Customization Is the Future",
    "section": "Why Some Believe AI Will Kill SaaS",
    "text": "Why Some Believe AI Will Kill SaaS\nThe primary argument for “AI replacing SaaS” goes like this:\n\nLower Development Costs: With AI writing boilerplate code, you need fewer developers.\n\nFaster Time to Market: AI can generate basic functionality quickly.\n\nTailored Features: Instead of dealing with generic solutions, an in-house AI tool can theoretically be customized to meet specific business needs.\n\nOn paper, this sounds like a dream come true for businesses. But the reality is more complicated. Building robust, production-grade software, from scratch, requires significant resources, including project management, maintenance, and ongoing support. Most companies aren’t equipped (or willing) to handle the long-term overhead that comes with building and hosting complex systems themselves."
  },
  {
    "objectID": "posts/2025-01-02-80-20-saas/index.html#embracing-the-8020-model",
    "href": "posts/2025-01-02-80-20-saas/index.html#embracing-the-8020-model",
    "title": "80/20 SaaS: Why AI-Driven Customization Is the Future",
    "section": "Embracing the 80/20 Model",
    "text": "Embracing the 80/20 Model\nRather than viewing AI and SaaS as competing forces, I propose a hybrid model. The 80/20 approach involves:\n\nCore Product (80%)\n\nSaaS providers develop and maintain a set of core features that almost every customer needs. Think user authentication, data storage, basic dashboards, etc. This allows them to keep their development teams lean and focus on rock-solid functionality.\n\nCustomer Customization (20%)\n\nCustomers then use AI (offered as part of the SaaS platform) to build out the features that are specific to their individual business needs. This might involve specialized workflows, custom dashboards, or niche integrations.\n\n\nBy dividing responsibilities this way, the SaaS provider avoids bloating its product with endless feature requests. Customers, in turn, get exactly what they want without having to pay for an army of developers."
  },
  {
    "objectID": "posts/2025-01-02-80-20-saas/index.html#a-crm-example",
    "href": "posts/2025-01-02-80-20-saas/index.html#a-crm-example",
    "title": "80/20 SaaS: Why AI-Driven Customization Is the Future",
    "section": "A CRM Example",
    "text": "A CRM Example\nConsider Customer Relationship Management (CRM) software. Giants like Salesforce and Microsoft Dynamics offer thousands of features, but not every company needs each one. Despite the vast customization options, many organizations still find themselves adapting to the CRM, not the other way around.\nWhat if a startup CRM provider offers a bare-bones product that covers 80% of what most sales teams require. Things like contact management, basic analytics, and lead tracking, and then invites customers to build the rest with AI-driven tools? For instance:\n\nA sales manager wants automated follow-up emails that adapt to different time zones.\n\nShe types her request into the built-in AI feature: “Generate customized follow-up emails that go out at optimal local times for each lead.”\n\nThe AI creates this functionality on the fly, integrating it seamlessly into the CRM interface.\n\nNo waiting for lengthy releases, no large development staff. Just a straightforward 80% solution with a customizable 20%."
  },
  {
    "objectID": "posts/2025-01-02-80-20-saas/index.html#benefits-for-both-sides",
    "href": "posts/2025-01-02-80-20-saas/index.html#benefits-for-both-sides",
    "title": "80/20 SaaS: Why AI-Driven Customization Is the Future",
    "section": "Benefits for Both Sides",
    "text": "Benefits for Both Sides\n\nFor SaaS Providers\n\nLean Development Team: Focus on a stable core, lowering overhead.\n\nReduced Feature Creep: No more chasing every edge case. Let AI and customers handle that final 20%.\n\nNew Revenue Streams: Providers can charge a flat monthly fee for the core product plus incremental fees for AI-built custom features.\n\n\n\nFor Customers\n\nFaster Customization: No waiting for your feature request to pass through the SaaS provider’s roadmap.\n\nLower Costs: Pay for essential features plus only the custom pieces you really need.\n\nReduced Risk: Save time and money by not building entire applications from scratch."
  },
  {
    "objectID": "posts/2025-01-02-80-20-saas/index.html#potential-challenges",
    "href": "posts/2025-01-02-80-20-saas/index.html#potential-challenges",
    "title": "80/20 SaaS: Why AI-Driven Customization Is the Future",
    "section": "Potential Challenges",
    "text": "Potential Challenges\nOf course, the 80/20 model comes with its own complexities:\n\nMaintenance of Custom Features\n\nAI-generated features could break when the core platform updates. SaaS providers will need robust versioning and testing to ensure stability.\n\nSecurity & Compliance\n\nAs customers add custom modules, the platform must ensure data remains secure and compliant with regulations like GDPR.\n\nQuality Control\n\nWho is responsible when an AI-built feature malfunctions? Customers will demand that providers offer at least minimal support for AI-generated functionality.\n\nTraining Data & Privacy\n\nAI systems rely on data to learn. Providers must have clear policies on how user data is used to train and improve models."
  },
  {
    "objectID": "posts/2025-01-02-80-20-saas/index.html#startups-vs.-giants",
    "href": "posts/2025-01-02-80-20-saas/index.html#startups-vs.-giants",
    "title": "80/20 SaaS: Why AI-Driven Customization Is the Future",
    "section": "Startups vs. Giants",
    "text": "Startups vs. Giants\nThis business model has the potential to disrupt established SaaS giants. Companies like Salesforce and Microsoft already offer highly customizable platforms, but they are weighed down by thousands of existing features. Re-engineering their entire codebase to an AI-first platform would be like trying to rebuild an airplane in mid-flight.\nEnter the new startups:\n- Clean Slate: They can design products from the ground up with an 80/20 approach.\n- Modern Architectures: They can embed AI customization layers from day one, rather than retrofitting existing systems.\n- Agile Mindset: Less legacy overhead means they can pivot faster to market demands.\nThe incumbents won’t disappear overnight, but they may lose out on smaller, more agile customers who prefer a streamlined, customizable experience."
  },
  {
    "objectID": "posts/2025-01-02-80-20-saas/index.html#conclusion-thriving-in-an-ai-future",
    "href": "posts/2025-01-02-80-20-saas/index.html#conclusion-thriving-in-an-ai-future",
    "title": "80/20 SaaS: Why AI-Driven Customization Is the Future",
    "section": "Conclusion: Thriving in an AI Future",
    "text": "Conclusion: Thriving in an AI Future\nAI may not spell the end of SaaS, but it will transform it. By offloading the final 20% of custom features to AI, SaaS providers can remain lean, stay focused on a reliable core, and still give customers the freedom to adapt software to their unique processes.\n\nFinal Thought\nAre you ready to adapt? If you’re a SaaS provider, consider slimming your product down to the essential core and building out an AI-driven customization layer. If you’re a customer, think about the time and money you could save by adopting an 80/20 approach, rather than building or buying bloated software.\nThe future of SaaS will hinge on who best embraces this new reality. Those who see AI not as a rival to existing software, but as an ally in creating truly personal, on-demand solutions.\nWhich side of the 80/20 equation is your business on?"
  },
  {
    "objectID": "posts/2025-01-31-knowledge/index.html",
    "href": "posts/2025-01-31-knowledge/index.html",
    "title": "What Item Would You Bring to a Deserted Island?",
    "section": "",
    "text": "For the first time in history, the answer to the following questions of “what item would you bring to a deserted island” and “what item would you keep to start civilization over from scratch” is now the same thing. You would bring ChatGPT, or put more generally a large language model (llm) or generative AI model. At its core, these AI models condense the knowledge of the data it was trained on. It can acquire knowledge and know how to apply it, which is the definition of intelligence. This proves that the most important thing in our world is access to knowledge and how to use it, aka intelligence, and AI models are the best container for knowledge. Let’s dive deeper into why ChatGPT is the best item to bring to a deserted island. Before everyone gets their panties in a bunch about needing internet access for ChatGPT, let’s assume it’s an offline model running locally inside an indestructible cube that is powered by a hand crank and solar panels. Ok? Good. Let’s continue.\nWhen you crash on the island, the AI would tell you what you need to do first to survive. Most likely how to build a shelter with the elements you have around you. Teach you how to get fresh water. Show you which mushrooms are poisonous and which make for a tasty treat. It could entertain you with stories. Even act as a friend for companionship in a lonely place. Having a physical tool like a hatchet would be nice, but it’s not going to help administer first aid to you when you accidentally cut yourself while chopping wood.\nImagine trying to bring other forms of knowledge to the island. Maybe a waterproof copy of a survival manual. That would definitely help you survive the first few nights and eventually thrive on the island. But only for your most basic needs. What if you found other people on the island? Maybe a potential partner you want to settle down with and start a family? The survival book is not going to help you deliver a child.\nInstead of a survival book you could bring an entire encyclopedia. Either one of those old school physical books, or a print out of everything on Wikipedia. That could help you with a lot of things. But now your problem is knowing where to find the right knowledge to help you with new problems. Does that rash on your arm match with the description of a rare disease in the encyclopedia? Hard to say. You then find another tribe on the island that speaks a different language as you and they seem pretty pissed off. Scrolling through Wikipedia for 30 minutes can’t help you there. So it’s not enough to have the knowledge itself at hand, but easy ways to access exactly what you need, when you need it.\nEasy access to knowledge used to be driven by search engines like Google and Bing. They could search through various information for you automatically. In a sense they condensed the entire internet into a text box to ask questions. But what came out on the other side are things that might be able to help, but not the exact knowledge you might need immediately or how to apply it to solve your problem. New AI services like ChatGPT can do just that. It knows most of what a search engine might know, but it can find the right answer immediately. It’s also able to create new information never thought possible, because it wasn’t in the original training data. Google search may not know what to do when a completely new event like a meteorite hits your island. But ChatGPT can reason and give you the knowledge and implementation to deal with the incident. It not only knows what, it also knows how. Knowledge is power. But being able to apply knowledge correctly, aka intelligence, is even more powerful.\nThe cost of intelligence with AI is quickly going to zero. What happens then? It feels like the two most important jobs going forward in an AI future is either building smarter AI models. Or finding the right data to feed them. By building smarter models you are improving the how (intelligence), and by getting better data you are improving the what (knowledge). The scary part is that AI models can now create their own synthetic training data, and techniques like reinforcement learning allow them to essential train themselves. So manual human intervention in these models might go away too. What’s left to do if the AI can train itself and get better every day?\nThe answer is to use the AI to make humans better. To increase their knowledge of the what and the how. Imagine a 30 year old accountant who has never camped outside in their life lands on the island with ChatGPT. They would most likely be able to get by with the help of the AI, but they would still struggle initially, maybe even perish from plain bad luck. Now replace that person with Bear Grylles, the famous British survival expert. I mean, the dudes name is literally “Bear”, how could he not survive in the wild? Combine him with ChatGPT and now he turns into a superhuman. He already knows how to survive, and paired with ChatGPT he can now build an island empire.\nHaving a smart AI is good, but having a smart AI combined with a smart human is even better. As these AI models get smarter, they should also enable humans to get smarter as well. If you want to start learning how to code today, you shouldn’t read a textbook on python. Instead have ChatGPT start teaching you. It’s a world class tutor available 24/7, tailored to your learning style, and is infinitely patient. It can write code for you, explain how it works, and then quiz you about it. Using AI to increase the collective knowledge and intelligence of every person in the world should be a top priority.\nSo many problems in our world today can partly be due to a lack of education. When using a college degree as a proxy for education, the problems in our society stick out. People who don’t have a college degree have more problems.\n\nPoverty: In 2019 people with a college degree earned $25,000 more than someone with only a high school diploma.\nCrime: Less than 4% of formerly incarcerated people have a college degree.\nSuicide: Men without a college degree are twice as likely to die from suicide compared to a college graduate.\n\nHaving a college degree is not a perfect measure of who has knowledge and who does not. But the above stats paint a bad picture for those without a degree. Soon everyone with an internet connection will have access to the best tutor in the world, essentially for free. AI tutors can teach billions of people at once. The only limiting factors will become tailoring the AI tutor to each persons wants and needs, and a person’s thirst for knowledge. There’s an old saying that goes: “The cure for boredom is curiosity. There is no cure for curiosity.” We can tailor the AI tutors to the learner, but creating the want inside of people to learn is the hard part. In a world of video games and TikTok, getting someone excited to learn about chemistry or to simply read a book will be a challenge.\nThis is where the multimodal nature of AI models can save the day. The AI can print out text, talk to you via voice, even create photos and videos. This unlocks limitless ways for an AI to teach you something. Someone may prefer a book like experience with an AI using chat, whereas another may learn best through a superhero movie that teaches you about greek mythology. Creating an AI tutor that a person can’t turn away from is one of the most important problems to work on. Imagine your 10 year old staying up all night because they don’t want to stop learning about the migratory patterns of birds, or how nuclear fusion works, instead of watching TikTok dance trends. This is a possible future. AI is up to the task.\nAI models are not only positioned to make the world better, they are also well positioned to make humans better. Let’s go build one."
  },
  {
    "objectID": "posts/2025-02-03-ts-fundamentals-data-cleaning/index.html",
    "href": "posts/2025-02-03-ts-fundamentals-data-cleaning/index.html",
    "title": "Data Cleaning For Time Series",
    "section": "",
    "text": "This post is part of a larger learning series around time series forecasting fundamentals. Check out the learning path to see other posts in the series.\n\nData Overview\nThe result of exploratory data analysis can reveal a lot about your data. One factor it can bubble up is how clean the data is, and if there needs to be any interventions done before we start training models. Cleaning your data is an important next step. If your data is garbage, you will create a garbage forecast. Gargbage in, garbage out.\n\n\nData Cleaning for Time Series\nThe way you clean your time series data is very different than of machine learning domains. Here are the building blocks of good data cleaning. Some are the same posts from previous chapters, but they still apply here. Click on each to explore further.\n\nMissing Values, Outliers\nBox-Cox Transformation\nStationary"
  },
  {
    "objectID": "posts/2025-02-24-life-principles/index.html",
    "href": "posts/2025-02-24-life-principles/index.html",
    "title": "Life Principles",
    "section": "",
    "text": "A Man Must Have a Code\nEvery person lives by a set of principles that govern their actions throughout life. They are either principles inherited by the society they were raised in (unconsciously), or principles that were chosen and updated by a person on their own (consciously). The best principles in life are ones you have thought through yourself. Below are the top principles I like to live my life by.\n\n\nBias for Action\nAction is the best answer to most problems. Sitting around waiting for something to happen to you rarely works out. No one ever get’s lucky sitting on their couch watching Netflix. Action creates luck, creates feedback, and also creates momentum.\n\n\nSense of Urgency\nLife is short, and timelines are negotiable. Ever heard of the parkinson principle? It states that work expands to fill the time available for its completion. Got an important work project due in a month? It will probably take you a month to do. Does the same project need to be done by Friday? Then you will also probably finish it Friday morning.\nReality is negotiable. Peter Thiel has a saying: “If you have a 10 year plan of how to get somewhere, you should ask: why can’t I do this in 6 months?”. The quality of your questions determines the quality of your life.\nWhen you combine a sense of urgency with a bias for action, you become an unstoppable force. This brings us to my next principle.\n\n\nHigh Agency\nWho would you call to break you out of a third world prison? That person has a high sense of agency, which means they have have the ability to control what happens to them in life. Think of it as the opposite of a victim mentality. Agency is the combination of a bias for action with a sense of urgency.\nGeorge Mack does a good job discussing agency. Here is a good example.\n\n\nZero Expectations\nWhen Charlie Munger was asked about what it takes to live a happy life his response was simple, have low expectations. In a similar vein, Naval Ravikant states that happiness is a simple equation. Happiness = Reality - Expectations.\nIf you came to Seattle in January and expected it to be warm and sunny, you’d be bummed out if it was 50 degrees with a slight drizzle. Instead if you came expecting it to be cold and constantly raining, then you’d be pleasantly surprised to find out that it didn’t rain much at all. The same external events happened in both examples, but the change in expectations made for a better outcome.\nIt’s ok to have high expectations for yourself, but having high expectations over things in life you can’t control is a recipe for disaster.\n\n\nTime Over Money\nControl over how you spend your time is the ultimate freedom and source of power. It’s the life razor I use to make most decisions in my life.\n\n\nLove Over Power\nMost things we chase in life are a form of power. Money, status, fame, beauty, these are all forms of power. They will not make you fulfilled in life. You know what will? Love.\n\n\nGiving Without Getting\nGiving away my time, attention, and skills with no expectations of something in return is very important to me. Nothing grinds my gears like someone doing a good deed in the world then immediately posting about it on social media. They are giving with the intention of receiving status in return. Giving yourself to others is an amazing act in and of itself, full stop.\n\n\nStay Present\nIf you think about the future, you are anxious. If you think about the past, you are depressed. Staying in the future moment is the best way to live a happy and purpose filled life.\n\n\nStagnation is Death\nIf you are coasting, you are going downhill. Always be learning, always be evolving. If you are not learning, you are dying.\n\n\nKeep it Simple\nThe best solutions to problems in life are the simplest. Keep it simple stupid.\n\n\nIncentives Matter\nCharlie Munger once said “show me the incentive and I’ll show you the behavior”. Incentives rule most things in life. Use the right incentive and you will change your life, and maybe the world.\n\n\nRun Towards Fear\nUsually the things you are the most afraid of are the most important things that you need to do to improve your life. Scared of public speaking? Then getting good at it will probably change your life more than anything else.\n\n\nAllergic to Status\nIncreasing your status will not create a fulfilling life. So I try not to do things that inflate my status. Comparison is the thief of joy. Never compare yourself to others (aka your status compared to others), only compare yourself to who you were yesterday.\n\n\nDo Things Worth Writing, Write Things Worth Reading\nThis is a quote from Ben Franklin. I think it’s a great life philosophy. I try to do both interesting things, and write interesting things. Heck, you are hopefully reading something interesting right now!\n\n\nWhat Would You Do If You Couldn’t Fail?\nThis is a fantastic question to ask yourself. It can help you determine your career path and most of your major decisions in life. If you are stuck in your career, asking this question can point you in the right direction.\n\n\nWhat Makes for the Best Story?\nAmjad Masad, CEO of Replit, uses this question as the final decision framework for big life moves. When deciding between two paths, taking the one that makes for the better story will almost always create a better life for you. Choose wisely.\n\n\nDid I Always Do My Best?\nJohn Wooden says that “success is the peace of mind that is the direct result of self-satisfaction knowing you did your best to become the best that you are capable of becoming”. He didn’t care about the score of the game, he only cared that his players rose to their full potential. If they did that, everything else will take care of itself. Even if they lost, they still were at peace because they knew they did their best. This is a powerful idea and one that people today seem to overlook.\n\n\nDon’t Be the Best, Be the Only\nThis comes from the law of category from a book called The 22 Immutable Laws of Marketing. It’s better to be the first person in a new industry than to be the 10th person. Finding things that you are the only person in the world doing is a good place to be.\n\n\nStop Trying to Sound Smart\nI’ve had to learn this one the hard way. Stop trying to be right all of the time. Stop trying to one up people with more interesting facts or stories. Stop correcting people. Once you stop, the quality of your relationships improve, which improves your entire life.\n\n\nHard Choices, Easy Life. Easy Choices, Hard Life.\nThis comes from Jerzy Gregorek. If you make the easy choice of eating junk food today, your life will become much harder in the future once you develop diabetes and cardiovascular disease. But if you make the hard choices of diet and exercise today, future you will be happy and healthy. This approach applies to most things in life. Hard now, easy later.\n\n\nGear Over Stuff\nThis was outlined in Michael Easter’s book called Scarcity Brain. Having more stuff in my life weighs me down, increasing stress. Instead gear serves a purpose in your life. You can have tons of stuff, but only so much gear.\n\n\nAvoid Scarcity Loops\nThis is the main concept outlined in Michael Easter’s book called Scarcity Brain. It has three parts.\n\nOpportunity\nUnpredictable Rewards\nQuick Repeatability\n\nScarcity loops are built to make us crave certain behaviors that are most likely not good for us. Once you learn this you see it everywhere. Let’s take checking your email. It’s easy to open the app on your phone (opportunity), you may or may not have any new unread emails (unpredictable rewards), and you can easily recheck for new emails five minutes later (quick repeatability). Checking your email constantly is obviously not good for you, but scarcity loops change your behavior for the worst. There are also scarcity loops on social media, junk food, drinking, drugs, gambling, online shopping, and literally anything else that is not healthy for you.\nAvoid them at all cost.\n\n\nThe Most Important Question\nJosh Waitzkin has a framework called the most important question that helps train you on discovering what matters most. Knowing where to focus your energy is often more important than how hard you work at something. Remember, the quality of your questions determines the quality of your life. So being able to think about the most important question to solve for in your job, marriage, etc. is paramount.\n\n\nFinal Thoughts\nTo live a quality life you must have quality principles. If you don’t, you will be easily influenced to live an unauthentic life based on the expectations of society. Feel free to take some of mine to use as your own principles, or don’t! I could care less, because I have zero expectations about people reading this post.\nPhoto Credit"
  },
  {
    "objectID": "posts/2025-03-15-genetic-methylation/index.html",
    "href": "posts/2025-03-15-genetic-methylation/index.html",
    "title": "Your Genes Are Making You Sick",
    "section": "",
    "text": "What Gets Measured, Gets Managed\nOur bodies are mostly a mystery to a regular person. Everyone takes vitamins, but do they actually help your body operate better? If you don’t know what kind of vitamins your body needs, there’s a good chance supplements you take every day are actively hurting your body or are simply being pissed down the drain (literally). Nobody wants to take something that is a waste of time. So how do you know what nutrients your body needs? This is where understanding methylation in your body comes in.\n\n\nWhat’s Methylation?\nMethylation is the process of converting nutrients you eat into things your body can actually use. We eat crude oil in the form of food, and our body uses methylation to process it into gasoline in to use for various processes that help us thrive. The issue is that our body rarely does this all perfectly. Many of us have gene mutations that inhibit this kind of conversation process. Let’s walk through some of the most common gene mutations around methylation and see how we can make sure our bodies are operating at their maximal potential.\n\n\nMTHFR\nCommonly known as the “mother fucker” gene, this one can be a real mother fucker. Up to 44% of people in the world carry this gene mutation. MTHFR deals with how our body handles the nutrient called folate. It translates the folate we get from our food into a form our bodies can use called methylfolate or 5-MTHF, which is the methylated version of folate.\n5-MTHF helps break down a compound in our body called homocysteine. Which is one of the most inflammatory compound in the human body. Inflammation is very bad for our body folks. When this compound is high all crazy types of things happen. Here are just a couple of things that are impacted.\n\nCreating neurotransmitters\nDNA repair\nDetoxing\nRegulating blood pressure\nMiscarriages during pregnancy\nAnxiety\n\nFolate can be found in leafy greens and legumes. But the most common way we consume folate is through a compound called folic acid. Folic acid is the most common additive to our food. It’s literally everywhere. White rice and can flour contain folic acid. When ingredients say “fortified” or “enriched”, that means folic acid was added. This is bad news for people with the MTHFR mutation because they cannot process folic acid into the form our body needs. I will repeat, folic acid is the most common additive in food but cannot be properly processed by almost half of the worlds population. It’s even found in some prenatal vitamins, which has been linked to miscarriages and postpartum depression. Not good!\nI know this is kind of scary to read, but thankfully there is some good news. There are easy ways to work around this gene mutation and ensure you are thriving in life. Here’s what to do.\n\nAvoid all forms of folic acid, which limits your processed food intake, which is a major health win overall\nTake the methylated version of folate, called 5-MTHF, at night before going to bed\n\n\n\nMTR\nSimilar to MTHFR, the MTR gene also plays in a role in regulating homocysteine by converting it into another compound called methionine. For this reaction to work, your body needs vitamin B12, specifically the methylated versions of B12 like methylcobalamin or hydroxocobalamin.\nIf you have this gene mutation, you may experience certain gut issues like these.\n\nGas\nBloating\nDiarrhea\nConstipation\nCramping\n\nSupplementing with B12 is crucial for this gene mutation, but similar to MTHFR not all supplements are created equal. Stay away from a form of B12 called Cyanocobalamin. This form contains hydrogen cyanide, which is a known neurotoxin. It’s often added into cheap energy drinks and even milk! Companies do this because it’s cheap and they don’t care, so care for yourself and avoid it at all costs.\nHere’s what to do if you have this gene mutation.\n\nLimit alcohol, which depletes methyl donors and can interfere with B12 absorption\nAvoid heavy metals (like mercury and lead) that bind to B12, making it unavailable for MTR to use\nTake the methylated version of B12, either methylcobalamin or hydroxocobalamin found in methylated multivitamins\n\nFYI not everyone can process methylcobalamin, but 100% of humans can process hydroxocobalamin in their body\nIf you also have the COMT gene mutation, you absolutely should be taking hydroxocobalamin since your body can process that better than other forms like methylcobalamin\n\n\n\n\nMTRR\nThis gene works closely with the MTR gene, hence similar names. It helps reactivate B12 that was used by the MTR gene when converting homocysteine into methionine. Over time B12 can become oxidized when used by the MTR gene, so MTRR comes in to convert it back into its methylated form so it can be properly used again. MTRR is also important in the thyroid, helping convert one important thyroid hormone T4 into T3, which happens in your gut.\nIf you have this gene mutation, you might suffer from the following.\n\nHeart burn\nAcid reflux\nThyroid issues\nShort temper\nHigh blood pressure\nExacerbations of ADD, ADHD, OCD\n\nFollow the same guidelines as described in the MTR section above. Avoid the wrong types of B12 and make sure to take versions that your body can actually use.\n\n\nAHCY\nThis gene helps break down S-Adenosylhomocysteine (SAH) into homocysteine and adenosine. This process is critical because SAH is a strong inhibitor of methylation enzymes. If it builds up, it slows down methylation reactions throughout the body. SAH is one of the most inflammatory compounds in your body, even worse than homocysteine.\nThis is a rare gene mutation to have, but people with it can have addictive tendencies. For example, addiction to video games as kids, then phones as teenagers, then drugs, alcohol, sugar, working out, eating, sex, shopping, working. This is caused by wide ranges in dopamine in the body, which is tied to our behaviors. So people with this mutation tend to seek out dopamine rewarding behavior.\nHere’s what to do if you have this gene mutation.\n\nConsume collagen to ensure your ingesting enough glycine to help your body balance methionine. If AHCY isn’t efficiently breaking down SAH, excess methionine intake might lead to toxic byproducts instead of proper methylation.\n\n\n\nCOMT\nThis gene breaks down major neurotransmitters in the body. Things like dopamine and adrenaline. It’s also responsible for metabolizing estrogens, and having too much of this hormone for both men and women can be bad. When your body can’t break these these things, all sorts of things get out of whack.\nIf you have this gene mutation, you might suffer from the following.\n\nStress and anxiety\nTrouble winding down and sleeping\nQuick to anger, slow to calm down\nHigher estrogen levels, which can lead to water retention under the belly button and around the waist\nPain sensitivity\nADD or ADHD\n\nThankfully it’s not all bad with this gene mutation. Because things like dopamine stay around longer in your brain, it can lead to higher creativity and deep thinking. But this comes at a cost of overanalyzing and worrying.\nHere’s what to do if you have this gene mutation.\n\nAvoid green tea and quercitin (found in green tea and other supplements) at all costs. These can make your COMT symptoms much worse.\nIf you have excess water retention around your belly, look into taking a diindolylmethane (DIM) supplement, which can help reduce that water retention by removing excess estrogen in your body.\nConsider taking a trimethylglycine (TMG) supplement, which can help regulate the processes that maintain your neurotransmitters\n\n\n\nHow They All Connect\nProcesses in our body can be thought of like a group of construction workers passing sandbags to each other in a single line. One gene can do a process the body needs, then pass the output along to another gene to do another process, and so on. But if one of those genes can only pass along 4 sandbags instead of the 10 needed, then the most any of the downstream genes will get is 40% of what’s required.\nThis is why understanding genetic methylation in our bodies is so important. Each of these 5 genes play a crucial role in making our bodies work.\nLet’s do a recap of each gene and what they do in our methylation process.\n\nMTHFR starts the process by making active folate (5-MTHF)\nMTR uses 5-MTHF & B12 to convert homocysteine into methionine\nMTRR keeps MTR working by recharging B12\nAHCY clears out byproducts and provides fresh homocysteine\nCOMT uses methylation to break down stress chemicals and estrogen\n\nIf any of these genes don’t work well, the whole system slows down! The fix is to support each gene with the right nutrients and to avoid things your body may not be able to handle.\n\n\nDNA Testing\nHow the heck do I know if I have any gene mutations? This is where testing comes in. There are a lot of services out there that will do a genetic methylation test for you. They cost a few hundred dollars, but it’s a test you only need to run once in your life.\nIt’s a simple cheek swab you do at home then mail in. I used a service called 10X Health, but there are many providers out there who can help.\n\n\nFinal Thoughts\nI got my results back about a month ago, and things did not look good. I had mutations in all 5 genes except AHCY, with the MTHFR and COMT genes having the strongest mutation (meaning they don’t work). These results are kind of like getting a personality test result. You start to understand who you are as a person more. Why you act and feel a certain way. You start to understand your parents and siblings so much more, because they most likely carry similar genes as you. It’s life changing to receive information that shows you how to live a better life with 100% certainty.\nMy gene results showed that I had to really crack down on folic acid (MTHFR gene) and green tea/quercetin (COMT). I realized some of my most beloved foods and supplements, that are supposed to be healthy, actually contain things that are the worst to put into my body. Since cleaning those up I have had an easier time falling asleep, and my brain is working better than ever. It’s also been easier to avoid processed food, because I know almost all of it contains things that my body cannot handle well. So the temptation to binge on them has been removed. Thank goodness!\nIf you’d like to live up to your highest potential in life. Consider taking control of your genetics and get tested! It might just change your life. To learn more about genetic mythlation, check out Gary Breka’s channel on YouTube, he’s got tons of great info.\nPhoto Credit"
  },
  {
    "objectID": "posts/2025-03-24-ts-fundamentals-univariate-models/index.html",
    "href": "posts/2025-03-24-ts-fundamentals-univariate-models/index.html",
    "title": "Univariate Models For Time Series",
    "section": "",
    "text": "This post is part of a larger learning series around time series forecasting fundamentals. Check out the learning path to see other posts in the series.\n\nWhat’s a Univariate Model?\nThe simplest, yet often most powerful, models in time series forecasting are the oldest. These models are univariate approaches that are more statistics than new age machine learning. Univariate means that they only rely on previous values of the target variable to predict future values of that target variable. Just a timestamp and a target variable is all you need to get off the ground with univariate models.\nThis might sound too simple, but often it’s the simplest models that can create the best forecasts.\n\n\nTypes of Univariate Models\nThere are countless univariate models we could cover, but instead we will keep things simple and discuss some of the most common models that provide the biggest bang for buck.\n\nARIMA (in progress)\nExponential Smoothing\nSimple Benchmark Models"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software I’ve Written",
    "section": "",
    "text": "finnts - low code package for time series forecasting"
  },
  {
    "objectID": "software.html#r",
    "href": "software.html#r",
    "title": "Software I’ve Written",
    "section": "",
    "text": "finnts - low code package for time series forecasting"
  },
  {
    "objectID": "software.html#web-apps-built-on-replit",
    "href": "software.html#web-apps-built-on-replit",
    "title": "Software I’ve Written",
    "section": "Web Apps Built on Replit",
    "text": "Web Apps Built on Replit\n\nAI Brain Clone - scrapes my blog daily and allows users to ask questions about my blog\nWorkout Builder - use AI to create custom workouts tailored to your needs\nDecision Tracker - track your major decisions and analyze them in the future"
  }
]