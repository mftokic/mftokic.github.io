---
title: "Time Series First Principles: Garbage In, Garbage Out"
description: "Training a model on bad data leads to bad forecasts"
author: "Mike Tokic"
date: "2024-04-08"
categories: [time-series, machine-learning, finance]
image: "image.png"
---

![](./image.png)

### Time Series First Principles Series

This post dives into the second principle of a good time series forecast, garbage in garbage out. Check out the [initial post](https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/) to get a high level view of each principle. 

1. [Domain Expertise](https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/)
2. **Garbage In Garbage Out**
3. The Future Is Similar To The Past
4. Higher Grain Higher Accuracy
5. Order Is Important
6. The Magic Is In The Feature Engineering
7. Simple Models Are Better Models
8. Capture Uncertainty
9. Model Averages Are King
10. Deep Learning Last

### You're Not a Wizard, Harry

The biggest mistake I see people in finance make when trying to use machine learning (ML) is around approaching it like a magic wand. Thinking as long as they bring in some data and throw it over the fence to a ML process, a perfect forecast will come back to them. All shiny and new. ML should be able to find all of the patterns in the data and do things we humans can't fathom right? No, wrong. ML is not a cure all thing. Having a good ML forecast starts with having quality historical data for a model to learn from. Without good data, you won't get a good forecast. It's as simple as that. Let's dive into ways data can be stinky and how we can sanitize it before training models. To help illustrate each point we'll use an example of a monthly sales forecast.   

### Amount of Historical Data

Ideally you want to get as much historical data as possible. If we want to forecast the next quarter of sales, it's a bad idea to only use the last 12 months of historical data to train a model. Usually I try to get 5+ years of historical data before training any model. This allows for enough year over year observations for a model to learn from. For monthly forecasts, I won't even start a forecast project if there is less than 3 years of historical data. 

Having sufficient historical data creates the opportunity to have sufficient model back testing. Where we can see how models performed over the last few months of the data. We can then use that back testing accuracy as a proxy for what to expect for the future forecast. The less historical data you have, the less you can back test. 

The more data you have, the longer you can forecast into the future. If I only have 3 years of historical data, it's a bad idea to try and forecast the next 2 years. A good heuristic is to cap your forecast horizon (periods you want to forecast out) to less than 50% of the historical data. So if you want to forecast out the next 3 years of monthly sales, you need at least 6 years of historical data. Preferably more. 

What if I have tons of historical monthly sales data, but a cool new feature I want to use around sales pipeline is only available for the last 2 years? Most of the time, stick with using more historical data even if it's at the expense of using highly correlated features but less historical data. Feel free to try both approaches, but often the one with the most historical data wins. 

### Trend and Seasonality

### Missing Data

### Outliers

### Reversal

### finnts

### Final Thoughts
