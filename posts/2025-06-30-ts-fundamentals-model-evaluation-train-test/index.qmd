---
title: "Model Evaluation: Train Test Splits"
description: "Analyzing how your forecast performs on unseen data"
author: "Mike Tokic"
date: "2025-05-30"
categories: [machine-learning, time-series]
image: "image.png"
draft: True
---

*This post is part of the [model evaluation chapter](https://mftokic.github.io/posts/2025-05-13-ts-fundamentals-model-evaluation/) within a larger learning series around time series forecasting fundamentals. [Check out the main learning path](https://mftokic.github.io/posts/2024-09-25-ts-fundamentals/) to see other posts in the series.* 

*The example monthly data used in this series [can be found here.](https://github.com/mftokic/mftokic.github.io/blob/main/posts/2024-10-02-ts-fundamentals-whats-a-time-series/example_ts_data.csv) You can also find the [python code used in this post here.](https://github.com/mftokic/mftokic.github.io/blob/main/notebooks/2025-05-30-ts-fundamentals-model-evaluation-train-test-split.ipynb)*

### Building Trust In The Forecast

Let's say your companies CFO is asking you, the hotshot data & AI person, to produce a revenue forecast for the next 12 months. The CFO needs this forecast to communicate expectations to wall street, help optimize product inventory, and make future capital allocation decisions based on where the business is headed the next year. You take a model like ARIMA off the shelf and produce that 12 month forecast, shown below using a time series from our example dataset. Let's take a look. 

![](./chart1.png)

The future 12 month forecast seems to capture the seasonality from month to month, and even the upward trend. You show the results to the CFO, even calling out the 95% prediction interval, which shows the upper of lower bounds with 95% certainty that the future revenue values are likely to fall in between. The CFO looks at it for one second, then says "so what", I can't use this forecast. How do I know it's accurate, this is a black box. You're hopes and dreams, including that potential promotion, are now crushed. Congrats, you learned one of your first hard lessons in the forecast game. **Building trust in the forecast is harder than creating one in the first place.** It might take seconds to train an ARIMA model, but convincing people to use it might take years. 

Hang on a second, you just remembered in our chapter covering [univariate models](https://mftokic.github.io/posts/2025-03-24-ts-fundamentals-univariate-models/) that there is this thing called a residual, which allows you to see historical forecasts compared to actual values on the training data.

### Residuals

> Residual = Actual Value - Forecast Value

Let's calculate the residuals for the ARIMA model we trained and plot them on some nice charts using different residual analysis techniques. 

#### Forecast vs Actual Plot

![](./chart2.png)

Overall it looks like the residuals look ok. The historical forecast closely tracks the actual value in most months. There are some months where we can see a large residual, shown by a big gap between the two lines.

#### Residuals Over Time Plot

![](./chart3.png)

Now we can see the actual residual across each historical period. Some are positive, where we underforecast, and some are negative, where we overforecast. **Ideally residuals hang around zero on average. This means the residuals are truly white noise aka random. If they are not white noise and clustering around zero, this means there is potential for more predictive insights to be learned in the data.**

#### Histogram of Residuals

![](./chart4.png)

#### Q-Q Residual Plot

![](./chart5.png)

Another interesting way to look at residuals is by plotting them in a different graph with the forecast on one axis and the historical actual values on the other. Let's take a look. 

#### Summarizing the Residuals

#### ACF of Residuals

### Train vs Test Data

Importance of hold out data. Having a model generalize well to future unseen data. In sample vs out of sample. How to split the data by time.  

### Final Thoughts

### Learn More