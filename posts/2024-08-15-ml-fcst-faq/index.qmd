---
title: "FAQ on Machine Learning Forecasting"
description: "Straightforward answers to all of your ML forecast questions"
author: "Mike Tokic"
date: "2024-08-15"
categories: [finance, machine-learning, forecasting]
image: "image.png"
draft: true
---

Over the last few years I've presented to hundreds of people outside of Microsoft around how we approach machine learning (ML) forecasting within Microsoft finance. A lot of great questions were asked during those conversations. With many overlaps around common themes. In this post I want to highlight some of the most commonly asked questions and my take on answering them. Hopefully this can be a quick reference for anyone ML curious or want to deepen the ML work being done on their teams. Use the table of contents to skip around to the sections you're most interested in. If there are any topics missing please reach out to me via [LinkedIn](https://www.linkedin.com/in/michaeltokic/) and I will continue to update this post. 

## FAQ Table of Contents {#toc}

### [Data](#data)

- [Getting high quality historical data](#historical-data)
- [Using third party data](#third-party-data)
- [New info not found in the training data, one time events](#one-time-events)
- [Handling outliers](#outliers)

### [Technical](#technical) 

- [Interpreting the black box](#black-box)
- [What models to use, should we use deep learning](#models)
- [What programming language or framework to use](#language)
- [Using large language models like ChatGPT to forecast](#llm)
- [What level of accuracy is good](#accuracy)

### [Humans](#humans)

- [How to make forecast owners accountable for the ML number](#accountability)
- [Building Trust in the ML forecast](#trust)
- [Who owns the ML creation process](#ownership) 
- [How to get started with ML](#starting)
- [Going from ML as a triangulation point to replacing manual human forecasts](#migrating)
- [Building data science talent in finance](#talent)

## Data {#data}

### Getting high quality historical data {#historical-data}

[Garbage in, garbage out](https://mftokic.github.io/posts/2024-04-08-time-series-garbage/). That's probably the most common saying in the world of ML. If you cannot get high quality historical data, then there is no easy way to produce an accurate ML forecast. You can't work your way around noisy or incomplete data. If your data is messy, hard to find, and comes from 10 different systems, then ML is not something you should be worried about. Fix your data first, then focus on using that data in combination with other things like ML. A nice bow on top of a pile of crap is still a pile of crap. Fix your data first, then focus on ML after.  

[Back to Table of Contents](#toc)

### Using third party data {#third-party-data}

Your company's business is most likely impacted by greater market forces outside of your control. For example the health of the economy or how much money customers have to spend. Adding data from outside of your company (third party data) as features in your ML models is a good way to improve forecast accuracy, while also being able to describe what outside forces impact your business the most. Some data is freely available, while others have to be paid for. What data you use is up to your domain knowledge of your business.

Free Data

- [FRED](https://fred.stlouisfed.org/)
- [World Bank](https://data.worldbank.org/)
- [International Monetary Fund](https://data.imf.org/?sk=388dfa60-1d26-4ade-b505-a05a558d9a42)
- [United Nations](https://data.un.org/)
- [Google Trends](https://trends.google.com/trends/)

Paid Data

- [IDC](https://www.idc.com/data-analytics)
- [Trading Economics](https://tradingeconomics.com/indicators)

[Back to Table of Contents](#toc)

### New info not found in training data and one time events {#one-time-events}

If you are changing the price of your product in three months, this is most likely going to impact your future revenue forecast. But if you have never changed the price of your product before, then a ML model cannot learn from that information. For a model to learn from one time events, it needs to be present in the historical data a model is trained on. [The future must always learn from the past](https://mftokic.github.io/posts/2024-04-11-time-series-past-future/).

[Back to Table of Contents](#toc)

### Handling Outliers {#outliers}

[Outliers](https://mftokic.github.io/posts/2024-04-11-time-series-past-future/) in your data can have a bad impact on your future forecast. It can hurt accuracy or give false signals of the future. There are many ways to deal with outliers. The easiest way is to use statistical methods to identify and remove them. Treating them as a missing value you can then replace. Sometimes outliers are more subtle, and take a trained eye to spot them. This is where the [domain expertise](https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/) of a person comes into play. 

[Back to Table of Contents](#toc)

## Technical {#technical}

### Interpreting the black box {#black-box}

This one is a toughie. When a person creates a forecast manually, most often using excel, someone else can come into that financial model and trace cell by cell exactly what's going on. Going from input data, to assumptions, to the formulas that create the final output. This is the kind of exactitude that allows accountants to sleep peacefully at night. Everything is in order and everything is perfectly understood. But we are not accountants. This is finance. We have to make calls about the future that are uncertain. We can never have 100% certainty that something is going to happen. If that's the case then your company is doing something illegal. Get out now!

The biggest paradigm shift someone has to make with machine learning is giving up this total control of the forecast. And in essence take a [leap of faith](https://mftokic.github.io/posts/2024-07-15-msft-ml-fcst-journey-3/#leap-of-faith). Machine learning models are enigmas. Sometimes akin to magic. The cannot be perfectly understood because the capture non-linear relationships in data that a human never could. That's why we have them, because they can work better and faster than our human brains in some tasks. 

There are ways to understand these models, but they cannot be perfectly audited like a manual forecast done in excel. Instead they have to be interrogated. Not like a criminal wanted for war crimes but more like a therapist talking to their patient. There is no way to know exactly what's going on inside of their patients mind. But they can start to ask questions that can give clues into what's going on and see why the person has past decisions in their life. 

The best resource I know on explaining ML models is [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) by Christoph Molnar. Here's a quick overview of the method's described in the book. 

- **Model Specific:** These are models like linear regression or a single decision tree where we can see exactly what's going on under the hood. The model structure is more like an excel formula we can trace step by step. But because they are easy to explain, they are simple in nature and may not produce the most accurate forecast. That is the tradeoff between having a forecast that can be easily explained versus having a forecast that is the most accurate. The more accurate the forecast, the more likely you cannot explain it perfectly. 
- **Model Agnostic:** For more advanced models like gradient boosted trees and deep learning, we can use methods that approximate what's going on under the hood of a complex model. This is when we have to act like a therapist and start asking questions to our model and see what answers it gives back. There are two ways of doing this. 
    - **Global Interpretability:** This uses methods that can see overall what's impacting the model the most. For example you can see what input variable (or feature) is the most important in the model overall.  
    - **Local Interpretability:** This uses methods to see what's going on for each individual forecast data point. For example you can see for a specific future forecast what's impact that number the most. 

The last thought I'd leave you with is this. Have you ever not used ChatGPT because you couldn't get an explanation of its answer? For example maybe you asked it to help you write some excel formulas to format dates. Do you trust the output it gave you because the excel formula was correct or because it could tell you exactly how it came to that conclusion? What if the explanation it gave was made up or a hallucination? If the excel formula is correct you would still use it right? Even the CEO of OpenAI, Sam Altman, cannot explain how models like GPT-4 think under the hood. But hey, ChatGPT was still the fastest growing product of all time. Sometimes imperfect interpretability is ok. But maybe your CEO is demanding an explanation of the forecast numbers, so this is [still a hard problem to solve in finance](https://mftokic.github.io/posts/2023-02-11-three-levels-of-ml-adoption/).  

[Back to Table of Contents](#toc)

### What models to use, should we use deep learning {#models}

People are always attracted to the hot new thing, and I can't blame them. New is exciting. When it comes to ML forecasting, new isn't always better. The newest trend in ML is all about deep learning. Or models that can mimic the human brain. While they work really well for things like analyzing photos and text, using them on tabular data (aka excel data) hasn't always worked out well. That's why I [recommend using deep learning last](https://mftokic.github.io/posts/2024-05-31-time-series-deep-learning/).

Here are the models to use first, then you can always resort to deep learning if need be. 

- **Univariate Models**: These are the simplest forecasting models. Since they only need one variable, hence the name univariate. If you want to forecast revenue, then you only need historical revenue and you're off and running. They run extremely fast and can scale to millions of data combinations without spending too much on cloud compute. Here are a few popular ones. 
    - **ARIMA**: An ARIMA (AutoRegressive Integrated Moving Average) model predicts future values in a time series by combining differencing (modeling the difference between periods), autoregression (using past values), and moving averages (using past forecast errors). It's the most common univariate model in the forecasting game. 
    - **Exponential Smoothing**: Forecasts future values in a time series by applying decreasingly weighted averages of past observations, giving more importance to recent data points to capture trends and seasonal patterns.
    - **Seasonal Naive**: Predicts future values by simply repeating the observations from the same season of the previous period, assuming that future patterns will mimic past seasonal cycles. Don't sleep on this one! You'd be surprised how often it comes in handy as a good benchmarking model to compare with more complicated models. 
- **Traditional ML Models**: After trying univariate models, itâ€™s time to try more traditional machine learning models. These are models built specifically for tabular data, or data that can live in a SQL table or excel spreadsheet. These models are multivariate, which allow them to incorporate outside variables as features to improve their forecast accuracy. They require more handling than a model like ARIMA, since they need [feature engineering](https://mftokic.github.io/posts/2024-05-01-time-series-features/) and proper [time series cross-validation](https://mftokic.github.io/posts/2024-05-01-time-series-features/). Multivariate models can also learn across multiple time series at the same time, instead of being trained on just a single time series like a univariate model. Here are a few common multivariate models.
    - **Linear Regression**: Predicts future values by fitting a line to the historical data, where the line represents the relationship between the dependent variable and one or more independent variables.
    - **XGBoost**: Predicts future values using an ensemble of decision trees, boosting their performance by iteratively correcting errors from previous trees, resulting in a highly accurate and robust prediction model.
    - **Cubist**: Predicts future values by combining decision trees with linear regression models, creating rule-based predictions that incorporate linear relationships within each segment of the data for greater accuracy. 

[Back to Table of Contents](#toc)

### What programming language or framework to use {#language}

Should I use python? But what if I learned R in my statistics class? What about stata or good ole javascript? Ask 10 data scientists what programming language to use and you'll probably get 10 different answers. There is no right answer. It's kind of like arguing what hammer to use when building a house. Shouldn't we just be worried about getting the house built? And make sure we don't screw it up? 

Here is my take on the ML language wars. You should use both, python and R. Different languages offer different things, and depending on the task you might want to use one over the other. Often it'll boil down to specific open source software that might only be available in one language but not the other. So over the course of a long data career it's probably a good idea to be competent at both. 

With that said, if you could only learn one programming language, learn python. That'll get you the farthest the fastest in terms of useful knowledge to get building. I think in a few years these debates will go away, because large language models (LLM) will come and save the day. Imagine writing code in your favorite language, using the packages you like, and then have a LLM take your code and translate it into blazing fast machine code that runs like a race car. So it won't matter if you only know one language or the other. That's where I hope we're headed. 

[Back to Table of Contents](#toc)

### Using large language models like ChatGPT to forecast {#llm}

With the explosion of large language models (LLM) that can do everything from tell dad jokes to write production grade code, can't we just offload all forecasting work to them? In essence you could, but I think it's kind of overkill and leads a lot to be desired. LLMs take in text, and spit out text. They are good with words, but not that good with numbers. They can't perform on par with a calculator, because that's not how they were designed. So you can't just copy a table from excel, give it to ChatGPT, and hope to get next quarters revenue forecast. It might give it to you, but it won't be a good forecast. It might even make something up. 

The more sensible route to take is to have the LLM write code that can create forecast models and have it execute it for you. For example use the code interpreter feature in ChatGPT to have it take your uploaded CSV file and execute a bunch of python code against it to get a final forecast. This kind of workflow might be good for initial exploration, but it shouldn't be used in a production setting where you need an updated forecast each month. It's kind of like needing a place to sleep and each night you build a new house from scratch, only sleep there one night, then the next night build another house from scratch. Having LLMs produce code on the fly each time can lead to inconsistent results that may not be reproducible when you ask ChatGPT to do it again. You could take the code from the first forecast iteration, save it, and either run it yourself each time or give it to ChatGPT as a prompt. But even then you are still missing out. Some automated forecasting packages, like the one I own called [finnts](https://microsoft.github.io/finnts/index.html) have over 10,000 lines of code. So a LLM will most likely not be writing that much code to answer one prompt, and if they did having you trying to save and manage that code is out of the question. 

I think LLMs can still have some part in the forecasting process. I think they are useful before and after the actual model training is done. For example you can have code interpreter analyze your data for things like outliers or help in running correlation analysis to see what variables could help improve your forecast accuracy. Then you will take those learnings and run the forecast outside of the LLM environment, or you could have the LLM call an outside function to kick off a forecast. This is called "function calling", where a LLM can use an outside tool (most often calling an API via code) that can accomplish the task a LLM cannot (like getting the current weather). LLMs can then be used after the forecast process is ran to then analyze the final forecast outputs. Tools like code interpreter can make charts and analyze historical back testing accuracy. 

Maybe in the future the LLM can help explain how the final forecast was created, or better yet answer questions about forecast variance once actuals land in the future. But using it to actually create predictions or train models itself is still a tall order. With that said there are [new time series specific LLMs](https://docs.nixtla.io/) that are being released, so this is an exciting area to watch closely. 

[Back to Table of Contents](#toc)

### What level of accuracy is good {#accuracy}

There is no right answer. It all depends on the specific data you are using and how it compares to non ML approaches you have done previously. 

A common metric in time series forecasting is the MAPE error metric. MAPE stands for mean absolute percentage error. This is very similar to variance to forecast percent metrics you might already use. Think of it as the average percent error (as an absolute value) across every forecasted data point. There are plenty of other metrics out there, but MAPE is a good one since it's not dependant on scale and percents are easy for anyone to wrap their head around. 

A MAPE of 5% means that on average, your forecast is off plus or minus 5%. So the closer to zero the better. If you ask any finance person what kind of MAPE they want for their forecast, almost all of them will say less than 1%. Or more than 99% accuracy. Think of accuracy as the inverse of MAPE. Maybe the ML forecast has a MAPE of 10%, but your previous manual excel model ended up having a 20% MAPE. The ML forecast has a 50% reduction in forecast error, even though it's still in the double digits. So evaluating ML is always relative to what kind of performance you got with other forecast methods probably done in excel. 

With that said, here are some rules of thumb I use when running my own ML forecasts. 

- **Daily and Weekly Data**: Less than a 10% MAPE is terrific. But even MAPEs as high as 30% are ok, because often we might sum up a daily forecast to a monthly or quarterly level. And when evaluated at that aggregate level you may start to have drastically improved MAPEs. 
- **Monthly, Quarterly, Yearly Data**: There are a few levels of accuracy that I think are good at this level. Again, it's all relative to what kind of performance you had before using ML. 
    - Less than a 5% MAPE is good
    - Less than a 3% MAPE is great
    - Less than a 1% MAPE is amazing

Even going from a 3% MAPE to a 2% MAPE is a big achievement. Because it's a lot harder to do that than to go from a 15% MAPE to a 10% MAPE. It's still the same level of improvement but once you get closer to zero the MAPE improvements seem to happen on a log scale. Where each percentage point you reduce continues to get harder as you get closer to zero. 

[Back to Table of Contents](#toc)

## Humans {#humans}

### How to make forecast owners accountable for the ML number {#accountability}



### Building Trust in the ML forecast {#trust}

### Who owns the ML creation process {#ownership}

### How to get started with ML {#starting}

### Going from ML as a triangulation point to replacing manual human forecasts {#migrating}

### Building data science talent in finance {#talent}