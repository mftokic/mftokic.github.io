---
title: "Univariate Models: Simple Benchmark Models"
description: "Creating simple models to benchmark against complex ones"
author: "Mike Tokic"
date: "2025-05-12"
categories: [machine-learning, time-series]
image: "image.png"
draft: True
---

*This post is part of the [univariate models chapter](https://mftokic.github.io/posts/2025-03-24-ts-fundamentals-univariate-models/) within a larger learning series around time series forecasting fundamentals. [Check out the main learning path](https://mftokic.github.io/posts/2024-09-25-ts-fundamentals/) to see other posts in the series.* 

*The example monthly data used in this series [can be found here.](https://github.com/mftokic/mftokic.github.io/blob/main/posts/2024-10-02-ts-fundamentals-whats-a-time-series/example_ts_data.csv) You can also find the [python code used in this post here.](https://github.com/mftokic/mftokic.github.io/blob/main/notebooks/2025-05-12-ts-fundamentals-univariate-benchmarks.ipynb)*

### What Are Benchmark Models?  

A benchmark model is a simple model that can be used as a forecast or to compare its forecast outputs against another model that is more complex. These models do not incorporate statistical or machine learning methods, instead they use standard formulas to produce forecasts. 

### Type of Benchmark Models

There are three main benchmarking models we'll discuss today. They are many other models/methods that can be used as benchmarks but these are the three most common. 

- **Mean Forecast**
    - Simple average of the last few periods of the time series. You could take the average of the last 12 months and use that as the forecast, the time period you use to create the average is up to you. 
- **Naive Forecast**
    - Take the last value in the time series and use that as the forecast. This is the simplest forecast method possible, that's why it's called "naive".
- **Seasonal Naive Forecast**
    - Similar to the naive method, but you take the value from the previous season and use that as the forecast. For example with monthly data, next months forecast could be the value for that month in the previous year. 

### Let's Create a Forecast

Let's take one of our example time series and create these benchmark forecasts. 

![](./chart1.png)

Looks like our future forecast for each model does not look good. No trends were captured, same goes for seasonality outside of the seasonal naive model. These models produced future forecasts, just none that we want to use for our final forecast.

### How to Use Them

Benchmark models can be ran first when starting a new forecast project to establish a simple baseline of accuracy for more advanced models to beat. You might build a fancy neural network model from scratch, but it may have the same accuracy as a dumb seasonal naive model. So starting with benchmark models first is always a good idea. [Simple models are better models](https://mftokic.github.io/posts/2024-05-03-time-series-simple-models/). 

### Other Benchmark Methods

Today we just covered the three most common benchmark models, but there are countless others you can use too. Here are a few more to pique your interest. 

- **Other Mean Forecast Methods**
    - Instead of averaging over the past year, you can average over the past quarter, the entire time series, or any other significant period. 
- **Random Walk with Drift**
    - A variation of the naive method allows the forecasts to change over time. The amount of change, called drift, is the average change seen in the historical data.
- **Seasonal Mean Forecast**
    - Similar to previously discussed mean forecast methods, but this time you are averaging the values within the same season. For example, for monthly data you might take the average value of the previous years for the month of December to use that to create a future forecast for December. 
- **Growth Rate Forecast Methods**
    - If the time series lacks seasonality, you can apply a compound annual growth rate (CAGR) or similar average growth over "xyz" time periods and apply that to create a future forecast. 

### Final Thoughts

Benchmark models might seem overly simplistic, but their real value lies in their role as the first step of your forecasting process. Establishing a simple, transparent baseline makes it easier to measure and justify the incremental gains from more sophisticated methods. Remember, complexity does not always equal accuracy; sometimes the simplest models offer the clearest insights. When in doubt, start simple and build complexity as needed.

### Learn More 

- [Benchmark Models from Nixtla's statsforecast python package](https://nixtlaverse.nixtla.io/statsforecast/src/core/models.html#baseline-models)