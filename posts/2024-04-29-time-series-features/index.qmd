---
title: "Time Series First Principles: The Magic Is In The Feature Engineering"
description: "How you transform your data before model training can transform a mediocre forecast into a world class forecast"
author: "Mike Tokic"
date: "2024-04-29"
categories: [time-series, machine-learning, finance]
image: "image.png"
draft: true
---

![](./image.png)

### Time Series First Principles Series

This post dives into the sixth principle of a good time series forecast, the magic is in the feature engineering. Check out the [initial post](https://mftokic.github.io/posts/2024-03-26-time-series-first-principles-initial/) in this series to get a high level view of each principle. 

1. [Domain Expertise](https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/)
2. [Garbage In Garbage Out](https://mftokic.github.io/posts/2024-04-08-time-series-garbage/)
3. [The Future Is Similar To The Past](https://mftokic.github.io/posts/2024-04-11-time-series-past-future/)
4. [Higher Grain Higher Accuracy](https://mftokic.github.io/posts/2024-04-18-time-series-grain/)
5. [Order Is Important](https://mftokic.github.io/posts/2024-04-23-time-series-order/)
6. **The Magic Is In The Feature Engineering**
7. Simple Models Are Better Models
8. Capture Uncertainty
9. Model Averages Are King
10. Deep Learning Last

### Turning Data Into Insight

A machine learning (ML) model is only as good as the data it's fed. The process of transforming data, to make it easier for a model to learn from that data, is called feature engineering. It's a technical term that is actually very simple in nature, really just data transformations. In the world of time series forecasting, feature engineering can make or break a good forecast. 

Creating high quality features is a combination of strong domain expertise and data transformation skills. We have already covered how domain expertise impacts a forecast in a [previous post](https://mftokic.github.io/posts/2024-04-02-time-series-domain-expertise/), so this post will cover how simple data transformations can drastically improve the accuracy of a machine learning forecast. Check out each category of time series feature engineering below to learn more.   

### Date Features

The most common type of feature engineering for time series is around dates. Date features allow us to capture seasonality patterns in our data. Think of seasonality as repeating peaks and valleys in our data. For example, our business might make most of it's revenue in Q4 every year, with a subsequent dip in sales in Q1. 

Let's use the example time series below to illustrate each type of feature engineering. 

| Date        | Sales ($) | Consumer Sentiment |
|-------------|-----------|--------------------|
| January 2023| 100,000   | 68                 |
| February 2023| 110,000  | 67                 |
| March 2023  | 120,000   | 65                 |
| April 2023  | 115,000   | 70                 |
| May 2023    | 130,000   | 72                 |
| June 2023   | 125,000   | 73                 |
| July 2023   | 135,000   | 74                 |
| August 2023 | 140,000   | 75                 |
| September 2023 | 130,000| 70                 |
| October 2023 | 145,000  | 72                 |
| November 2023 | 150,000 | 71                 |
| December 2023 | 160,000 | 75                 |

: Fake Time Series Data

In this time series we would like to forecast monthly sales. We also have information about consumer sentiment that we can use to help forecast sales. A multivariate machine learning model cannot easily use the date column as is, so we have to do some data transformations (aka feature engineering) to make it easier for a model to understand how date information can help predict sales. Let's go through a few examples of new features we can create from the date column. It's important to note that after we create these new features it's a good idea to remove the original date column before training a ML model. 

Since the data is monthly there is a lot of easy to create features we can use. We can pull out the specific month, quarter, and even year into their own columns to use as features. If our data was at a daily level, we can even go deeper and get features related to day of the week, day of year, week of month, etc. 

| Date        | Month     | Quarter | Year |
|-------------|-----------|---------|------|
| January 2023| January   | Q1      | 2023 |
| February 2023| February | Q1      | 2023 |
| March 2023  | March     | Q1      | 2023 |
| April 2023  | April     | Q2      | 2023 |
| May 2023    | May       | Q2      | 2023 |
| June 2023   | June      | Q2      | 2023 |
| July 2023   | July      | Q3      | 2023 |
| August 2023 | August    | Q3      | 2023 |
| September 2023| September| Q3     | 2023 |
| October 2023 | October  | Q4      | 2023 |
| November 2023| November | Q4      | 2023 |
| December 2023| December | Q4      | 2023 |

That seems pretty straight forward right? Let's keep squeezing our date fruit for more juice and see what other kinds of features we can create. Since this is a time series, adding some order of time can be helpful. This can be something as simple as an index starting at 1 (or even convert your date time a seconds format). This helps establish the proper order of our data and makes is easier for a model to pick up growing or declining trends over time. There is also slight differences in how many days there are from month to month, so we can add that too. If you don't think that's important then you have never been stung by the harsh mistress that is leap year. There have been multiple times where finance exec's have dismissed forecasts for the quarter that includes February, where in the end we didn't account for the fact that it was a leap year or we are one year removed from one. You can even take this one step further and add the number of business days for each month. 

| Date         | Index | Days in Month | Business Days |
|--------------|-------|---------------|---------------|
| January 2023 | 1     | 31            | 22            |
| February 2023| 2     | 28            | 20            |
| March 2023   | 3     | 31            | 23            |
| April 2023   | 4     | 30            | 20            |
| May 2023     | 5     | 31            | 23            |
| June 2023    | 6     | 30            | 22            |
| July 2023    | 7     | 31            | 21            |
| August 2023  | 8     | 31            | 23            |
| September 2023| 9    | 30            | 21            |
| October 2023 | 10    | 31            | 22            |
| November 2023| 11    | 30            | 22            |
| December 2023| 12    | 31            | 21            |

: Adding a time index and other day related features

To get the final drop of juice out of the date column, we can also add Fourier series features. A Fourier series feature in time series forecasting is a component that captures seasonal patterns using sine and cosine functions to model periodic cycles in the data. In a nutshell they are just recurring peaks and valleys that can occur at various date grains like monthly or daily. These features can help capture more complex seasonality in your data. The chart below shows some standard Fourier series at the monthly and quarterly grain. 

![](./chart1.png)

### Lag Features

Time series forecasting is all about learning about the past to forecast the future. In order to learn about the past we have to create lags on our data. Often what we're trying to forecast today is correlated to what happened in the past. This is a concept known as autocorrelation. For our monthly forecast example, a 3 month lag may be highly correlated to sales with a 0 month lag (or sales today). Consumer sentiment can also be correlated with sales, but this time a lag of 6 might have higher correlation, since there is most likely a long delay between customer purchase patters and our company's product. Lags can be created for any amount, depending on your domain knowledge of the business and results from more exploratory data analysis (deep dive for a different day). 

| Date         | Sales ($) | Consumer Sentiment | Sales 3-Month Lag | Sentiment 6-Month Lag |
|--------------|-----------|--------------------|-------------------|-----------------------|
| January 2023 | 100,000   | 68                 |                   |                       |
| February 2023| 110,000   | 67                 |                   |                       |
| March 2023   | 120,000   | 65                 |                   |                       |
| April 2023   | 115,000   | 70                 | 100,000           |                       |
| May 2023     | 130,000   | 72                 | 110,000           |                       |
| June 2023    | 125,000   | 73                 | 120,000           |                       |
| July 2023    | 135,000   | 74                 | 115,000           | 68                    |
| August 2023  | 140,000   | 75                 | 130,000           | 67                    |
| September 2023 | 130,000 | 70                 | 125,000           | 65                    |
| October 2023 | 145,000   | 72                 | 135,000           | 70                    |
| November 2023| 150,000   | 71                 | 140,000           | 72                    |
| December 2023| 160,000   | 75                 | 130,000           | 73                    |

: Adding lag features

Last thing I'll say here is that you can also create leading features, especially for features that you know with 100% certainty ahead of time. For example, customers knowing of a new product launch in the future will definitely change how they purchase similar products you sell for the period leading up to the launch. Someone may hold off on buying a new iPhone until the latest one gets released in a few months. Same goes for cars and many other products.  

### Rolling Window Features

### Polynomial Features

### Reversal

Too many features can increase model train time and lead to overfitting. 

### finnts

### Final Thoughts