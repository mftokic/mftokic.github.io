---
title: "Univariate Models: ARIMA"
description: "Understanding how an arima model works"
author: "Mike Tokic"
date: "2025-04-03"
categories: [machine-learning, time-series]
image: "image.png"
draft: True
---

*This post is part of the [univariate models chapter](https://mftokic.github.io/posts/2025-03-24-ts-fundamentals-univariate-models/) within a larger learning series around time series forecasting fundamentals. [Check out the main learning path](https://mftokic.github.io/posts/2024-09-25-ts-fundamentals/) to see other posts in the series.* 

*The example monthly data used in this series [can be found here.](https://github.com/mftokic/mftokic.github.io/blob/main/posts/2024-10-02-ts-fundamentals-whats-a-time-series/example_ts_data.csv) You can also find the [python code used in this post here.](https://github.com/mftokic/mftokic.github.io/blob/main/notebooks/2025-03-21-ts-fundamentals-data-stationary.ipynb)*

### Let's Break ARIMA Down

Arima stands for autoregressive integrated moving average. That sounds like a mouthful so we'll take it step by step on how each component works. 

- **AR (AutoRegressive)**: The current value depends on past values of itself
- **I (Integrated)**: Difference the data to make it [stationary](https://mftokic.github.io/posts/2025-03-21-ts-fundamentals-data-cleaning-stationary/)
- **MA (Moving Average)**: The current value depends on past prediction errors

### Putting it All Together 

Here's what happens when an arima model is trained. 

1. Integrated: Difference the data d times to remove trend or seasonality, making the series stationary. This is necessary so that the AR and MA components can model the structure reliably.
2. Autoregressive: Model the differenced series as a linear combination of its own lagged values. This captures momentum or persistence in the time series aka how past values influence future ones.
3. Moving Average: Model the current value as a function of past forecast errors. These errors are not directly observed, so the model estimates them recursively during training, learning how random shocks in the past influence the present.
4. Training: The AR and MA parts are fitted together by maximizing the likelihood of the observed data, given the model structure. This involves recursively computing residuals and using numerical optimization to find the best parameters.

To create future forecasts, we'd take the trained model and do the following. 

1. Autoregressive: Predict differenced values using past observations (or predictions) via the AR terms.
2. Moving Average: Refine predictions using known or assumed residual errors from past forecasts.
3. Recursive Forecasting: Forecast multiple steps ahead by reusing your own predictions as inputs.
4. Integrated (Last): Reverse the differencing operation to return the forecast to the original scale.

Seasonal arima. Similar to non-seasonal arima, all AR and MA components are seasonal calculations. 

### Reversal 

Forecasts flattening out over time. 

### Final Thoughts

[Forecasting Principles and Practice](https://otexts.com/fpp3/arima.html) by Rob Hyndman 